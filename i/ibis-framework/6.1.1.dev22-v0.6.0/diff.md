# Comparing `tmp/ibis_framework-6.1.1.dev22.tar.gz` & `tmp/ibis-framework-v0.6.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "ibis_framework-6.1.1.dev22.tar", max compression
+gzip compressed data, was "dist/ibis-framework-v0.6.0.tar", last modified: Mon Nov 30 09:48:46 2015, max compression
```

## Comparing `ibis_framework-6.1.1.dev22.tar` & `ibis-framework-v0.6.0.tar`

### file list

```diff
@@ -1,1412 +1,204 @@
--rw-r--r--   0        0        0    11358 2023-08-06 00:03:22.103467 ibis_framework-6.1.1.dev22/LICENSE.txt
-drwxr-xr-x   0        0        0        0 2023-08-06 00:03:22.103467 ibis_framework-6.1.1.dev22/LICENSES/
--rw-r--r--   0        0        0    11663 2023-08-06 00:03:22.103467 ibis_framework-6.1.1.dev22/README.md
--rw-r--r--   0        0        0     4331 2023-08-06 00:03:51.903498 ibis_framework-6.1.1.dev22/ibis/__init__.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/__init__.py
--rw-r--r--   0        0        0    35803 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/__init__.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/df/__init__.py
--rw-r--r--   0        0        0     7621 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/df/scope.py
--rw-r--r--   0        0        0    10095 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/df/timecontext.py
--rw-r--r--   0        0        0    11373 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/__init__.py
--rw-r--r--   0        0        0    28818 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/alchemy/__init__.py
--rw-r--r--   0        0        0     9234 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/alchemy/datatypes.py
--rw-r--r--   0        0        0      232 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/alchemy/geospatial.py
--rw-r--r--   0        0        0    13848 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/alchemy/query_builder.py
--rw-r--r--   0        0        0    24188 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/alchemy/registry.py
--rw-r--r--   0        0        0     4806 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/alchemy/translator.py
--rw-r--r--   0        0        0      559 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/compiler/__init__.py
--rw-r--r--   0        0        0     3177 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/compiler/base.py
--rw-r--r--   0        0        0    19794 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/compiler/query_builder.py
--rw-r--r--   0        0        0    10890 2023-08-06 00:03:22.139467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/compiler/select_builder.py
--rw-r--r--   0        0        0    11060 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/compiler/translator.py
--rw-r--r--   0        0        0    12380 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/ddl.py
--rw-r--r--   0        0        0      883 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/registry/__init__.py
--rw-r--r--   0        0        0     1083 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/registry/aggregate.py
--rw-r--r--   0        0        0     2746 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/registry/binary_infix.py
--rw-r--r--   0        0        0     1705 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/registry/case.py
--rw-r--r--   0        0        0     5007 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/registry/geospatial.py
--rw-r--r--   0        0        0     1741 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/registry/helpers.py
--rw-r--r--   0        0        0     2705 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/registry/identifiers.py
--rw-r--r--   0        0        0     2370 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/registry/literal.py
--rw-r--r--   0        0        0    13017 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/registry/main.py
--rw-r--r--   0        0        0     3043 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/registry/string.py
--rw-r--r--   0        0        0     3095 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/registry/timestamp.py
--rw-r--r--   0        0        0     5716 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/base/sql/registry/window.py
--rw-r--r--   0        0        0    25278 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/__init__.py
--rw-r--r--   0        0        0     6548 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/client.py
--rw-r--r--   0        0        0     5213 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/compiler.py
--rw-r--r--   0        0        0     1580 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/custom_udfs.py
--rw-r--r--   0        0        0     5464 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/datatypes.py
--rw-r--r--   0        0        0      189 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/operations.py
--rw-r--r--   0        0        0    30603 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/registry.py
--rw-r--r--   0        0        0      819 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/rewrites.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/__init__.py
--rw-r--r--   0        0        0    11999 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/conftest.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/system/__init__.py
--rw-r--r--   0        0        0     3206 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/system/conftest.py
--rw-r--r--   0        0        0      160 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/system/snapshots/test_client/test_cross_project_query/out.sql
--rw-r--r--   0        0        0      309 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/system/snapshots/test_client/test_subquery_scalar_params/out.sql
--rw-r--r--   0        0        0    13732 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/system/test_client.py
--rw-r--r--   0        0        0     6609 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/system/test_connect.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/system/udf/__init__.py
--rw-r--r--   0        0        0      389 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/system/udf/snapshots/test_udf_execute/test_udf_with_struct/out.sql
--rw-r--r--   0        0        0     4217 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/system/udf/test_udf_execute.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/__init__.py
--rw-r--r--   0        0        0      155 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_approx/filter-approx_median/out.sql
--rw-r--r--   0        0        0      153 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_approx/filter-approx_nunique/out.sql
--rw-r--r--   0        0        0      111 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_approx/no_filter-approx_median/out.sql
--rw-r--r--   0        0        0      109 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_approx/no_filter-approx_nunique/out.sql
--rw-r--r--   0        0        0       66 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_binary/out.sql
--rw-r--r--   0        0        0      130 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bit/filter-bit_and/out.sql
--rw-r--r--   0        0        0      128 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bit/filter-bit_or/out.sql
--rw-r--r--   0        0        0      130 2023-08-06 00:03:22.143467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bit/filter-bit_xor/out.sql
--rw-r--r--   0        0        0       76 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bit/no_filter-bit_and/out.sql
--rw-r--r--   0        0        0       74 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bit/no_filter-bit_or/out.sql
--rw-r--r--   0        0        0       76 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bit/no_filter-bit_xor/out.sql
--rw-r--r--   0        0        0       87 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bool_reducers/mean/out.sql
--rw-r--r--   0        0        0       86 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bool_reducers/sum/out.sql
--rw-r--r--   0        0        0      174 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bool_reducers_where_conj/out.sql
--rw-r--r--   0        0        0      131 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bool_reducers_where_simple/out.sql
--rw-r--r--   0        0        0      178 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_bucket/out.sql
--rw-r--r--   0        0        0      100 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_cast_float_to_int/out.sql
--rw-r--r--   0        0        0       44 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_compile_toplevel/out.sql
--rw-r--r--   0        0        0      117 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_cov/pop/out.sql
--rw-r--r--   0        0        0      118 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_cov/sample/out.sql
--rw-r--r--   0        0        0      105 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/date/index.sql
--rw-r--r--   0        0        0      106 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/date/name.sql
--rw-r--r--   0        0        0      134 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/datetime/index.sql
--rw-r--r--   0        0        0      135 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/datetime/name.sql
--rw-r--r--   0        0        0      105 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/string_date/index.sql
--rw-r--r--   0        0        0      106 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/string_date/name.sql
--rw-r--r--   0        0        0      134 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/string_timestamp/index.sql
--rw-r--r--   0        0        0      135 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/string_timestamp/name.sql
--rw-r--r--   0        0        0      134 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/timestamp/index.sql
--rw-r--r--   0        0        0      135 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/timestamp/name.sql
--rw-r--r--   0        0        0      105 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/timestamp_date/index.sql
--rw-r--r--   0        0        0      106 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_day_of_week/timestamp_date/name.sql
--rw-r--r--   0        0        0      119 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_divide_by_zero/floordiv/out.sql
--rw-r--r--   0        0        0       92 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_divide_by_zero/truediv/out.sql
--rw-r--r--   0        0        0       39 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_extract_temporal_from_timestamp/date/out.sql
--rw-r--r--   0        0        0       39 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_extract_temporal_from_timestamp/time/out.sql
--rw-r--r--   0        0        0       54 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_azimuth/out.sql
--rw-r--r--   0        0        0       61 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/contains/out.sql
--rw-r--r--   0        0        0       62 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/covered_by/out.sql
--rw-r--r--   0        0        0       59 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/covers/out.sql
--rw-r--r--   0        0        0       65 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/d_within/out.sql
--rw-r--r--   0        0        0       63 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/difference/out.sql
--rw-r--r--   0        0        0       61 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/disjoint/out.sql
--rw-r--r--   0        0        0       61 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/distance/out.sql
--rw-r--r--   0        0        0       59 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/geo_equals/out.sql
--rw-r--r--   0        0        0       65 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/intersection/out.sql
--rw-r--r--   0        0        0       63 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/intersects/out.sql
--rw-r--r--   0        0        0       64 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/max_distance/out.sql
--rw-r--r--   0        0        0       60 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/touches/out.sql
--rw-r--r--   0        0        0       58 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/union/out.sql
--rw-r--r--   0        0        0       59 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_binary/within/out.sql
--rw-r--r--   0        0        0       56 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_minmax/x_max/out.sql
--rw-r--r--   0        0        0       56 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_minmax/x_min/out.sql
--rw-r--r--   0        0        0       56 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_minmax/y_max/out.sql
--rw-r--r--   0        0        0       56 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_minmax/y_min/out.sql
--rw-r--r--   0        0        0       58 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_point/out.sql
--rw-r--r--   0        0        0       53 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_simplify/out.sql
--rw-r--r--   0        0        0       44 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/aread/out.sql
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/as_binary/out.sql
--rw-r--r--   0        0        0       46 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/as_text/out.sql
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/buffer/out.sql
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/centroid/out.sql
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/end_point/out.sql
--rw-r--r--   0        0        0       52 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/geometry_type/out.sql
--rw-r--r--   0        0        0       46 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/length/out.sql
--rw-r--r--   0        0        0       49 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/npoints/out.sql
--rw-r--r--   0        0        0       49 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/perimeter/out.sql
--rw-r--r--   0        0        0       49 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/point_n/out.sql
--rw-r--r--   0        0        0       50 2023-08-06 00:03:22.147467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary/start_point/out.sql
--rw-r--r--   0        0        0       49 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_unary_union/out.sql
--rw-r--r--   0        0        0       39 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_xy/x/out.sql
--rw-r--r--   0        0        0       39 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_geospatial_xy/y/out.sql
--rw-r--r--   0        0        0       82 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hash/binary/out.sql
--rw-r--r--   0        0        0       64 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hash/string/out.sql
--rw-r--r--   0        0        0       44 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/md5-test-binary/out.sql
--rw-r--r--   0        0        0       27 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/md5-test-string/out.sql
--rw-r--r--   0        0        0       45 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/sha1-test-binary/out.sql
--rw-r--r--   0        0        0       28 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/sha1-test-string/out.sql
--rw-r--r--   0        0        0       47 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/sha256-test-binary/out.sql
--rw-r--r--   0        0        0       30 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/sha256-test-string/out.sql
--rw-r--r--   0        0        0       47 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/sha512-test-binary/out.sql
--rw-r--r--   0        0        0       30 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_hashbytes/sha512-test-string/out.sql
--rw-r--r--   0        0        0      146 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_identical_to/out.sql
--rw-r--r--   0        0        0       44 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_integer_to_timestamp/ms/out.sql
--rw-r--r--   0        0        0       76 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_integer_to_timestamp/ns/out.sql
--rw-r--r--   0        0        0       44 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_integer_to_timestamp/s/out.sql
--rw-r--r--   0        0        0       43 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_integer_to_timestamp/us/out.sql
--rw-r--r--   0        0        0       66 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_timestamp_or_time/datetime/out.sql
--rw-r--r--   0        0        0       50 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_timestamp_or_time/string_time/out.sql
--rw-r--r--   0        0        0       66 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_timestamp_or_time/string_timestamp/out.sql
--rw-r--r--   0        0        0       50 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_timestamp_or_time/time/out.sql
--rw-r--r--   0        0        0       66 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_timestamp_or_time/timestamp/out.sql
--rw-r--r--   0        0        0       85 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_year/date/out.sql
--rw-r--r--   0        0        0      114 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_year/datetime/out.sql
--rw-r--r--   0        0        0       85 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_year/string_date/out.sql
--rw-r--r--   0        0        0      114 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_year/string_timestamp/out.sql
--rw-r--r--   0        0        0      114 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_year/timestamp/out.sql
--rw-r--r--   0        0        0       85 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_literal_year/timestamp_date/out.sql
--rw-r--r--   0        0        0       44 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_now/out.sql
--rw-r--r--   0        0        0      387 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_projection_fusion_only_peeks_at_immediate_parent/out.sql
--rw-r--r--   0        0        0      182 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_range_window_function/prec_foll/out.sql
--rw-r--r--   0        0        0      203 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_range_window_function/prec_prec/out.sql
--rw-r--r--   0        0        0       97 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_set_operation/difference/out.sql
--rw-r--r--   0        0        0      100 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_set_operation/intersect/out.sql
--rw-r--r--   0        0        0       91 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_set_operation/union_all/out.sql
--rw-r--r--   0        0        0       96 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_set_operation/union_distinct/out.sql
--rw-r--r--   0        0        0      117 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_substring/out.sql
--rw-r--r--   0        0        0       49 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/day-date/out.sql
--rw-r--r--   0        0        0       54 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/day-timestamp/out.sql
--rw-r--r--   0        0        0       50 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/hour-time/out.sql
--rw-r--r--   0        0        0       55 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/hour-timestamp/out.sql
--rw-r--r--   0        0        0       57 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/micros-time/out.sql
--rw-r--r--   0        0        0       62 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/micros-timestamp/out.sql
--rw-r--r--   0        0        0       57 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/millis-time/out.sql
--rw-r--r--   0        0        0       62 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/millis-timestamp/out.sql
--rw-r--r--   0        0        0       52 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/minute-time/out.sql
--rw-r--r--   0        0        0       57 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/minute-timestamp/out.sql
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/month-date/out.sql
--rw-r--r--   0        0        0       56 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/month-timestamp/out.sql
--rw-r--r--   0        0        0       53 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/quarter-date/out.sql
--rw-r--r--   0        0        0       58 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/quarter-timestamp/out.sql
--rw-r--r--   0        0        0       52 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/second-time/out.sql
--rw-r--r--   0        0        0       57 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/second-timestamp/out.sql
--rw-r--r--   0        0        0       58 2023-08-06 00:03:22.151467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/week-date/out.sql
--rw-r--r--   0        0        0       63 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/week-timestamp/out.sql
--rw-r--r--   0        0        0       50 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/year-date/out.sql
--rw-r--r--   0        0        0       55 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_temporal_truncate/year-timestamp/out.sql
--rw-r--r--   0        0        0      123 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_to_timestamp_no_timezone/out.sql
--rw-r--r--   0        0        0      155 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_to_timestamp_timezone/out.sql
--rw-r--r--   0        0        0      229 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/days/out.sql
--rw-r--r--   0        0        0      174 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/five/out.sql
--rw-r--r--   0        0        0      229 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/hours/out.sql
--rw-r--r--   0        0        0      225 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/micros/out.sql
--rw-r--r--   0        0        0      229 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/minutes/out.sql
--rw-r--r--   0        0        0      227 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/seconds/out.sql
--rw-r--r--   0        0        0      264 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/two_days/out.sql
--rw-r--r--   0        0        0      233 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/week/out.sql
--rw-r--r--   0        0        0      237 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_trailing_range_window/years/out.sql
--rw-r--r--   0        0        0      373 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_union/False/out.sql
--rw-r--r--   0        0        0      378 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_union/True/out.sql
--rw-r--r--   0        0        0      807 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_union_cte/False-False/out.sql
--rw-r--r--   0        0        0      812 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_union_cte/False-True/out.sql
--rw-r--r--   0        0        0      812 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_union_cte/True-False/out.sql
--rw-r--r--   0        0        0      817 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_union_cte/True-True/out.sql
--rw-r--r--   0        0        0      183 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_window_function/current_foll/out.sql
--rw-r--r--   0        0        0      183 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_window_function/prec_current/out.sql
--rw-r--r--   0        0        0      183 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_window_function/prec_prec/out.sql
--rw-r--r--   0        0        0       93 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_window_unbounded/following/out.sql
--rw-r--r--   0        0        0       93 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/snapshots/test_compiler/test_window_unbounded/preceding/out.sql
--rw-r--r--   0        0        0      955 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/test_client.py
--rw-r--r--   0        0        0    18715 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/test_compiler.py
--rw-r--r--   0        0        0     3195 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/test_datatypes.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/__init__.py
--rw-r--r--   0        0        0       76 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_assign/out.js
--rw-r--r--   0        0        0       42 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_binary_operators/add/out.js
--rw-r--r--   0        0        0       42 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_binary_operators/div/out.js
--rw-r--r--   0        0        0       42 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_binary_operators/mul/out.js
--rw-r--r--   0        0        0       42 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_binary_operators/sub/out.js
--rw-r--r--   0        0        0      178 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_class/out.js
--rw-r--r--   0        0        0      125 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_class_with_properties/out.js
--rw-r--r--   0        0        0      139 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_continue/out.js
--rw-r--r--   0        0        0      154 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_delete/out.js
--rw-r--r--   0        0        0       64 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_dict/out.js
--rw-r--r--   0        0        0       61 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_floordiv/out.js
--rw-r--r--   0        0        0       40 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_function_def/out.js
--rw-r--r--   0        0        0      249 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_lambda_with_splat/out.js
--rw-r--r--   0        0        0       44 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_len_rewrite/out.js
--rw-r--r--   0        0        0      138 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_list_comp/out.js
--rw-r--r--   0        0        0       80 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_logical_not/out.js
--rw-r--r--   0        0        0       56 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_missing_vararg/out.js
--rw-r--r--   0        0        0       58 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_pow/out.js
--rw-r--r--   0        0        0      323 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_scope_with_while/out.js
--rw-r--r--   0        0        0       85 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_set_to_object/out.js
--rw-r--r--   0        0        0       78 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_setitem/out.js
--rw-r--r--   0        0        0      134 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_splat/out.js
--rw-r--r--   0        0        0       49 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_str/out.js
--rw-r--r--   0        0        0      106 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_true_false_none/out.js
--rw-r--r--   0        0        0       59 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_tuple/out.js
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_unary_minus/out.js
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_unary_plus/out.js
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.155467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_varargs/out.js
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_variable_declaration/out.js
--rw-r--r--   0        0        0       40 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_yield/out.js
--rw-r--r--   0        0        0       40 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_core/test_yield_from/out.js
--rw-r--r--   0        0        0      456 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_usage/test_multiple_calls_redefinition/out.sql
--rw-r--r--   0        0        0      224 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_usage/test_udf_determinism/False/out.sql
--rw-r--r--   0        0        0      206 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_usage/test_udf_determinism/None/out.sql
--rw-r--r--   0        0        0      220 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_usage/test_udf_determinism/True/out.sql
--rw-r--r--   0        0        0      142 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/snapshots/test_usage/test_udf_sql/out.sql
--rw-r--r--   0        0        0     6533 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/test_core.py
--rw-r--r--   0        0        0     2002 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/test_find.py
--rw-r--r--   0        0        0     2931 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/tests/unit/udf/test_usage.py
--rw-r--r--   0        0        0    13275 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/udf/__init__.py
--rw-r--r--   0        0        0    17119 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/udf/core.py
--rw-r--r--   0        0        0     1598 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/udf/find.py
--rw-r--r--   0        0        0     1255 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/bigquery/udf/rewrite.py
--rw-r--r--   0        0        0    22026 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/__init__.py
--rw-r--r--   0        0        0      363 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/compiler/__init__.py
--rw-r--r--   0        0        0     2621 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/compiler/core.py
--rw-r--r--   0        0        0     6719 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/compiler/relations.py
--rw-r--r--   0        0        0    42189 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/compiler/values.py
--rw-r--r--   0        0        0     7266 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/datatypes.py
--rw-r--r--   0        0        0     2113 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/identifiers.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/__init__.py
--rw-r--r--   0        0        0     3987 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/conftest.py
--rw-r--r--   0        0        0       36 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_reduction_where/count/out.sql
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_reduction_where/max/out.sql
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_reduction_where/mean/out.sql
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_reduction_where/min/out.sql
--rw-r--r--   0        0        0       41 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_reduction_where/std/out.sql
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_reduction_where/sum/out.sql
--rw-r--r--   0        0        0       38 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_reduction_where/var/out.sql
--rw-r--r--   0        0        0       40 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_std_var_pop/std/out.sql
--rw-r--r--   0        0        0       37 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_aggregations/test_std_var_pop/var/out.sql
--rw-r--r--   0        0        0       10 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_double_col/float/out.sql
--rw-r--r--   0        0        0       37 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_double_col/float32/out.sql
--rw-r--r--   0        0        0       27 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_double_col/float64/out.sql
--rw-r--r--   0        0        0       35 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_double_col/int16/out.sql
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_double_col/int8/out.sql
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_string_col/date/out.sql
--rw-r--r--   0        0        0       35 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_string_col/int16/out.sql
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_string_col/int8/out.sql
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_string_col/mapstring_int64/out.sql
--rw-r--r--   0        0        0       26 2023-08-06 00:03:22.159467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_string_col/string/out.sql
--rw-r--r--   0        0        0       64 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_string_col/structa_string_b_int64/out.sql
--rw-r--r--   0        0        0       43 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_cast_string_col/timestamp/out.sql
--rw-r--r--   0        0        0      142 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_column_regexp_extract/out.sql
--rw-r--r--   0        0        0       45 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_column_regexp_replace/out.sql
--rw-r--r--   0        0        0       21 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_greatest_least/out1.sql
--rw-r--r--   0        0        0       29 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_greatest_least/out2.sql
--rw-r--r--   0        0        0       18 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_greatest_least/out3.sql
--rw-r--r--   0        0        0       26 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_greatest_least/out4.sql
--rw-r--r--   0        0        0      105 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_group_concat/comma_none/out.sql
--rw-r--r--   0        0        0      137 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_group_concat/comma_zero/out.sql
--rw-r--r--   0        0        0      105 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_group_concat/minus_none/out.sql
--rw-r--r--   0        0        0       21 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_hash/out.sql
--rw-r--r--   0        0        0       10 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/bigint_col/out.sql
--rw-r--r--   0        0        0        8 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/bool_col/out.sql
--rw-r--r--   0        0        0       15 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/date_string_col/out.sql
--rw-r--r--   0        0        0       10 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/double_col/out.sql
--rw-r--r--   0        0        0        9 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/float_col/out.sql
--rw-r--r--   0        0        0        2 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/id/out.sql
--rw-r--r--   0        0        0        5 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/index/out.sql
--rw-r--r--   0        0        0        7 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/int_col/out.sql
--rw-r--r--   0        0        0        5 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/month/out.sql
--rw-r--r--   0        0        0       12 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/smallint_col/out.sql
--rw-r--r--   0        0        0       10 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/string_col/out.sql
--rw-r--r--   0        0        0       13 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/timestamp_col/out.sql
--rw-r--r--   0        0        0       11 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/tinyint_col/out.sql
--rw-r--r--   0        0        0        4 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_noop_cast/year/out.sql
--rw-r--r--   0        0        0       27 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_string_column_find/out1.sql
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_string_column_find/out2.sql
--rw-r--r--   0        0        0       40 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_string_column_find_in_set/out.sql
--rw-r--r--   0        0        0       22 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_string_column_like/out1.sql
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_string_column_like/out2.sql
--rw-r--r--   0        0        0       91 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_string_column_substring/out1.sql
--rw-r--r--   0        0        0       97 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_string_column_substring/out2.sql
--rw-r--r--   0        0        0       36 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_cast/out1.sql
--rw-r--r--   0        0        0       30 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_cast/out2.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_from_integer/out.sql
--rw-r--r--   0        0        0       50 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_truncate/d/out.sql
--rw-r--r--   0        0        0       57 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_truncate/h/out.sql
--rw-r--r--   0        0        0       59 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_truncate/m/out.sql
--rw-r--r--   0        0        0       59 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_truncate/minute/out.sql
--rw-r--r--   0        0        0       52 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_truncate/w/out.sql
--rw-r--r--   0        0        0       57 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_timestamp_truncate/y/out.sql
--rw-r--r--   0        0        0       15 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/abs/out.sql
--rw-r--r--   0        0        0       16 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/ceil/out.sql
--rw-r--r--   0        0        0       15 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/exp/out.sql
--rw-r--r--   0        0        0       15 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/log/out.sql
--rw-r--r--   0        0        0       17 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/log10/out.sql
--rw-r--r--   0        0        0       16 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/log2/out.sql
--rw-r--r--   0        0        0       17 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/round/out.sql
--rw-r--r--   0        0        0       20 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/round_0/out.sql
--rw-r--r--   0        0        0       20 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/round_2/out.sql
--rw-r--r--   0        0        0       41 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/sign/out.sql
--rw-r--r--   0        0        0       16 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_functions/test_translate_math_functions/sqrt/out.sql
--rw-r--r--   0        0        0       10 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_literals/test_string_literals/nested_quote/out.sql
--rw-r--r--   0        0        0       13 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_literals/test_string_literals/nested_token/out.sql
--rw-r--r--   0        0        0        8 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_literals/test_string_literals/simple/out.sql
--rw-r--r--   0        0        0      133 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_array_join_in_subquery/out.sql
--rw-r--r--   0        0        0      220 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_complex_array_expr_projection/out.sql
--rw-r--r--   0        0        0      116 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_count_name/out.sql
--rw-r--r--   0        0        0       94 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_isin_notin_in_select/out1.sql
--rw-r--r--   0        0        0       98 2023-08-06 00:03:22.163467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_isin_notin_in_select/out2.sql
--rw-r--r--   0        0        0      149 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_isnull_case_expr_rewrite_failure/out.sql
--rw-r--r--   0        0        0      126 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_join_self_reference/out.sql
--rw-r--r--   0        0        0      129 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_named_from_filter_groupby/out1.sql
--rw-r--r--   0        0        0      129 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_named_from_filter_groupby/out2.sql
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_physical_table_reference_translate/out.sql
--rw-r--r--   0        0        0       14 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_scalar_exprs_no_table_refs/add/out.sql
--rw-r--r--   0        0        0       14 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_scalar_exprs_no_table_refs/now/out.sql
--rw-r--r--   0        0        0       54 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_self_reference_simple/out.sql
--rw-r--r--   0        0        0      118 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-awardID-any_inner_join/out.sql
--rw-r--r--   0        0        0      123 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-awardID-any_left_join/out.sql
--rw-r--r--   0        0        0      120 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-awardID-inner_join/out.sql
--rw-r--r--   0        0        0      125 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-awardID-left_join/out.sql
--rw-r--r--   0        0        0      119 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-playerID-any_inner_join/out.sql
--rw-r--r--   0        0        0      124 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-playerID-any_left_join/out.sql
--rw-r--r--   0        0        0      121 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-playerID-inner_join/out.sql
--rw-r--r--   0        0        0      126 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_joins/playerID-playerID-left_join/out.sql
--rw-r--r--   0        0        0      113 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_simple_scalar_aggregates/out.sql
--rw-r--r--   0        0        0      198 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_table_column_unbox/out.sql
--rw-r--r--   0        0        0      280 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_timestamp_extract_field/out.sql
--rw-r--r--   0        0        0      123 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_where_simple_comparisons/out.sql
--rw-r--r--   0        0        0       38 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_where_use_if/out.sql
--rw-r--r--   0        0        0      110 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_where_with_between/out.sql
--rw-r--r--   0        0        0       97 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/snapshots/test_select/test_where_with_timestamp/out.sql
--rw-r--r--   0        0        0     5063 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/test_aggregations.py
--rw-r--r--   0        0        0     7613 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/test_client.py
--rw-r--r--   0        0        0    14143 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/test_functions.py
--rw-r--r--   0        0        0     2431 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/test_literals.py
--rw-r--r--   0        0        0     8285 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/test_operators.py
--rw-r--r--   0        0        0    11568 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/test_select.py
--rw-r--r--   0        0        0     7488 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/tests/test_types.py
--rw-r--r--   0        0        0    22217 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/conftest.py
--rw-r--r--   0        0        0     4184 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/dask/__init__.py
--rw-r--r--   0        0        0     7769 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/dask/aggcontext.py
--rw-r--r--   0        0        0    15338 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/dask/core.py
--rw-r--r--   0        0        0      803 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/dask/dispatch.py
--rw-r--r--   0        0        0      950 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/__init__.py
--rw-r--r--   0        0        0     5484 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/aggregations.py
--rw-r--r--   0        0        0     1789 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/arrays.py
--rw-r--r--   0        0        0      687 2023-08-06 00:03:22.167467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/decimal.py
--rw-r--r--   0        0        0    19083 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/generic.py
--rw-r--r--   0        0        0     1481 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/indexing.py
--rw-r--r--   0        0        0     3416 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/join.py
--rw-r--r--   0        0        0     3631 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/maps.py
--rw-r--r--   0        0        0     4902 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/numeric.py
--rw-r--r--   0        0        0     7246 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/reductions.py
--rw-r--r--   0        0        0     8794 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/selection.py
--rw-r--r--   0        0        0    11528 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/strings.py
--rw-r--r--   0        0        0     1093 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/structs.py
--rw-r--r--   0        0        0     6384 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/temporal.py
--rw-r--r--   0        0        0    11857 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/util.py
--rw-r--r--   0        0        0    14439 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/execution/window.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/__init__.py
--rw-r--r--   0        0        0     2815 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/conftest.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/execution/__init__.py
--rw-r--r--   0        0        0     8482 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/execution/conftest.py
--rw-r--r--   0        0        0     6269 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/execution/test_arrays.py
--rw-r--r--   0        0        0     4813 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/execution/test_cast.py
--rw-r--r--   0        0        0     7101 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/execution/test_functions.py
--rw-r--r--   0        0        0    17385 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/execution/test_join.py
--rw-r--r--   0        0        0     3161 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/execution/test_maps.py
--rw-r--r--   0        0        0    30180 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/execution/test_operations.py
--rw-r--r--   0        0        0     4158 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/execution/test_strings.py
--rw-r--r--   0        0        0     2790 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/execution/test_structs.py
--rw-r--r--   0        0        0     7285 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/execution/test_temporal.py
--rw-r--r--   0        0        0    10722 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/execution/test_timecontext.py
--rw-r--r--   0        0        0      782 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/execution/test_util.py
--rw-r--r--   0        0        0    21535 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/execution/test_window.py
--rw-r--r--   0        0        0     3481 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/test_client.py
--rw-r--r--   0        0        0     3181 2023-08-06 00:03:22.171467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/test_core.py
--rw-r--r--   0        0        0     2693 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/test_dispatcher.py
--rw-r--r--   0        0        0    15031 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/dask/tests/test_udf.py
--rw-r--r--   0        0        0     5528 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/dask/trace.py
--rw-r--r--   0        0        0    11617 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/dask/udf.py
--rw-r--r--   0        0        0    13039 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/datafusion/__init__.py
--rw-r--r--   0        0        0    21713 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/datafusion/compiler.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/datafusion/tests/__init__.py
--rw-r--r--   0        0        0     1184 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/datafusion/tests/conftest.py
--rw-r--r--   0        0        0     1214 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/datafusion/tests/test_register.py
--rw-r--r--   0        0        0      663 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/datafusion/tests/test_select.py
--rw-r--r--   0        0        0     1366 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/datafusion/tests/test_udf.py
--rw-r--r--   0        0        0     4217 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/druid/__init__.py
--rw-r--r--   0        0        0      849 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/druid/compiler.py
--rw-r--r--   0        0        0     2748 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/druid/datatypes.py
--rw-r--r--   0        0        0     1710 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/druid/registry.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/druid/tests/__init__.py
--rw-r--r--   0        0        0     4413 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/druid/tests/conftest.py
--rw-r--r--   0        0        0    38831 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/duckdb/__init__.py
--rw-r--r--   0        0        0     1724 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/duckdb/compiler.py
--rw-r--r--   0        0        0     4946 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/duckdb/datatypes.py
--rw-r--r--   0        0        0    16217 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/duckdb/registry.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/duckdb/tests/__init__.py
--rw-r--r--   0        0        0     1462 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/duckdb/tests/conftest.py
--rw-r--r--   0        0        0       66 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/duckdb/tests/snapshots/test_datatypes/test_cast_uints/uint16/out.sql
--rw-r--r--   0        0        0       65 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/duckdb/tests/snapshots/test_datatypes/test_cast_uints/uint32/out.sql
--rw-r--r--   0        0        0       64 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/duckdb/tests/snapshots/test_datatypes/test_cast_uints/uint64/out.sql
--rw-r--r--   0        0        0       64 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/duckdb/tests/snapshots/test_datatypes/test_cast_uints/uint8/out.sql
--rw-r--r--   0        0        0     5167 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/duckdb/tests/test_datatypes.py
--rw-r--r--   0        0        0    10834 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/duckdb/tests/test_register.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/flink/__init__.py
--rw-r--r--   0        0        0      155 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/flink/compiler/__init__.py
--rw-r--r--   0        0        0     1966 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/flink/compiler/core.py
--rw-r--r--   0        0        0     7073 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/flink/registry.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/__init__.py
--rw-r--r--   0        0        0      820 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/conftest.py
--rw-r--r--   0        0        0      290 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_join/test_join_then_filter_no_column_overlap/out.sql
--rw-r--r--   0        0        0      266 2023-08-06 00:03:22.175467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_join/test_mutate_then_join_no_column_overlap/out.sql
--rw-r--r--   0        0        0      573 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_join/test_mutating_join/inner/out.sql
--rw-r--r--   0        0        0      620 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_join/test_mutating_join/left/out.sql
--rw-r--r--   0        0        0      620 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_join/test_mutating_join/outer/out.sql
--rw-r--r--   0        0        0      621 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_join/test_mutating_join/right/out.sql
--rw-r--r--   0        0        0       31 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_literals/test_literal_timestamp_or_time/datetime/out.sql
--rw-r--r--   0        0        0       38 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_literals/test_literal_timestamp_or_time/datetime_with_microseconds/out.sql
--rw-r--r--   0        0        0       15 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_literals/test_literal_timestamp_or_time/string_time/out.sql
--rw-r--r--   0        0        0       31 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_literals/test_literal_timestamp_or_time/string_timestamp/out.sql
--rw-r--r--   0        0        0       15 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_literals/test_literal_timestamp_or_time/time/out.sql
--rw-r--r--   0        0        0       31 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_literals/test_literal_timestamp_or_time/timestamp/out.sql
--rw-r--r--   0        0        0      238 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_complex_filtered_agg/out.sql
--rw-r--r--   0        0        0      231 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_complex_groupby_aggregation/out.sql
--rw-r--r--   0        0        0      166 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_complex_projections/out.sql
--rw-r--r--   0        0        0       75 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_count_star/out.sql
--rw-r--r--   0        0        0       54 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_extract_fields/day/out.sql
--rw-r--r--   0        0        0       54 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_extract_fields/day_of_year/out.sql
--rw-r--r--   0        0        0       55 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_extract_fields/hour/out.sql
--rw-r--r--   0        0        0       57 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_extract_fields/minute/out.sql
--rw-r--r--   0        0        0       56 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_extract_fields/month/out.sql
--rw-r--r--   0        0        0       58 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_extract_fields/quarter/out.sql
--rw-r--r--   0        0        0       57 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_extract_fields/second/out.sql
--rw-r--r--   0        0        0       55 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_extract_fields/week_of_year/out.sql
--rw-r--r--   0        0        0       55 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_extract_fields/year/out.sql
--rw-r--r--   0        0        0       95 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_filter/out.sql
--rw-r--r--   0        0        0       91 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_having/out.sql
--rw-r--r--   0        0        0      123 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_simple_filtered_agg/out.sql
--rw-r--r--   0        0        0       44 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_sum/out.sql
--rw-r--r--   0        0        0       71 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_timestamp_from_unix/timestamp_ms/out.sql
--rw-r--r--   0        0        0       71 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_timestamp_from_unix/timestamp_s/out.sql
--rw-r--r--   0        0        0      174 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_translator/test_translate_value_counts/out.sql
--rw-r--r--   0        0        0      155 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_window/test_range_window/out.sql
--rw-r--r--   0        0        0      115 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/snapshots/test_window/test_rows_window/out.sql
--rw-r--r--   0        0        0     1370 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/test_join.py
--rw-r--r--   0        0        0     2198 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/test_literals.py
--rw-r--r--   0        0        0     3430 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/test_translator.py
--rw-r--r--   0        0        0     2800 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/tests/test_window.py
--rw-r--r--   0        0        0      341 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/translator.py
--rw-r--r--   0        0        0    10529 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/flink/utils.py
--rw-r--r--   0        0        0    42864 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/impala/__init__.py
--rw-r--r--   0        0        0    15445 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/impala/client.py
--rw-r--r--   0        0        0      803 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/impala/compat.py
--rw-r--r--   0        0        0     1750 2023-08-06 00:03:22.179467 ibis_framework-6.1.1.dev22/ibis/backends/impala/compiler.py
--rw-r--r--   0        0        0     9296 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/ddl.py
--rw-r--r--   0        0        0     8623 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/metadata.py
--rw-r--r--   0        0        0     3295 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/pandas_interop.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/__init__.py
--rw-r--r--   0        0        0    12605 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/conftest.py
--rw-r--r--   0        0        0      212 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/mocks.py
--rw-r--r--   0        0        0       50 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/first/out.sql
--rw-r--r--   0        0        0       20 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/lag_arg/out.sql
--rw-r--r--   0        0        0       17 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/lag_default/out.sql
--rw-r--r--   0        0        0       23 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/lag_explicit_default/out.sql
--rw-r--r--   0        0        0       49 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/last/out.sql
--rw-r--r--   0        0        0       21 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/lead_arg/out.sql
--rw-r--r--   0        0        0       18 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/lead_default/out.sql
--rw-r--r--   0        0        0       24 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/lead_explicit_default/out.sql
--rw-r--r--   0        0        0        8 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/ntile/out.sql
--rw-r--r--   0        0        0       14 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_analytic_functions/test_analytic_exprs/percent_rank/out.sql
--rw-r--r--   0        0        0      521 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_assign_labels/out.sql
--rw-r--r--   0        0        0      159 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/close_extreme_false/out.sql
--rw-r--r--   0        0        0      159 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/close_extreme_false_closed_right/out.sql
--rw-r--r--   0        0        0      205 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/close_extreme_false_include_under_include_over/out.sql
--rw-r--r--   0        0        0      160 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/closed_right/out.sql
--rw-r--r--   0        0        0      182 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/closed_right_close_extreme_false_include_under/out.sql
--rw-r--r--   0        0        0       84 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/closed_right_include_over_include_under/out.sql
--rw-r--r--   0        0        0      160 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/default/out.sql
--rw-r--r--   0        0        0       84 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/include_over_include_under0/out.sql
--rw-r--r--   0        0        0       97 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/include_over_include_under1/out.sql
--rw-r--r--   0        0        0      100 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/include_over_include_under2/out.sql
--rw-r--r--   0        0        0      182 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/include_under/out.sql
--rw-r--r--   0        0        0      205 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_bucket_histogram/test_bucket_to_case/include_under_include_over/out.sql
--rw-r--r--   0        0        0       28 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_case_exprs/test_decimal_fillna_cast_arg/fillna_l_extendedprice/out.sql
--rw-r--r--   0        0        0       30 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_case_exprs/test_decimal_fillna_cast_arg/fillna_l_extendedprice_double/out.sql
--rw-r--r--   0        0        0       23 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_case_exprs/test_decimal_fillna_cast_arg/fillna_l_quantity/out.sql
--rw-r--r--   0        0        0       99 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_case_exprs/test_identical_to/out.sql
--rw-r--r--   0        0        0       20 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_case_exprs/test_identical_to_special_case/out.sql
--rw-r--r--   0        0        0       21 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_case_exprs/test_isnull_1_0/out1.sql
--rw-r--r--   0        0        0       26 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_case_exprs/test_isnull_1_0/out2.sql
--rw-r--r--   0        0        0       42 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_case_exprs/test_nullif_ifnull/nullif_boolean/out.sql
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_case_exprs/test_nullif_ifnull/nullif_input/out.sql
--rw-r--r--   0        0        0       43 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_case_exprs/test_nullif_ifnull/nullif_negate_boolean/out.sql
--rw-r--r--   0        0        0       92 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_case_exprs/test_search_case/out.sql
--rw-r--r--   0        0        0       77 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_case_exprs/test_simple_case/out.sql
--rw-r--r--   0        0        0       21 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_case_exprs/test_where_use_if/out.sql
--rw-r--r--   0        0        0       33 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_coalesce_greater_least/test_varargs_functions/coalesce_columns/out.sql
--rw-r--r--   0        0        0       29 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_coalesce_greater_least/test_varargs_functions/coalesce_scalar/out.sql
--rw-r--r--   0        0        0       33 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_coalesce_greater_least/test_varargs_functions/greatest_columns/out.sql
--rw-r--r--   0        0        0       29 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_coalesce_greater_least/test_varargs_functions/greatest_scalar/out.sql
--rw-r--r--   0        0        0       30 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_coalesce_greater_least/test_varargs_functions/least_columns/out.sql
--rw-r--r--   0        0        0       26 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_coalesce_greater_least/test_varargs_functions/least_scalar/out.sql
--rw-r--r--   0        0        0       50 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_add_partition/out.sql
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_add_partition_string_key/out.sql
--rw-r--r--   0        0        0       80 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_add_partition_with_props/out.sql
--rw-r--r--   0        0        0       80 2023-08-06 00:03:22.183467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_partition_properties/out1.sql
--rw-r--r--   0        0        0       66 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_partition_properties/out2.sql
--rw-r--r--   0        0        0       93 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_partition_properties/out3.sql
--rw-r--r--   0        0        0       82 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_partition_properties/out4.sql
--rw-r--r--   0        0        0       80 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_table_properties/out1.sql
--rw-r--r--   0        0        0       66 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_table_properties/out2.sql
--rw-r--r--   0        0        0       93 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_table_properties/out3.sql
--rw-r--r--   0        0        0       82 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_alter_table_properties/out4.sql
--rw-r--r--   0        0        0       94 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_avro_other_formats/out.sql
--rw-r--r--   0        0        0       47 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_cache_table_pool_name/out1.sql
--rw-r--r--   0        0        0       47 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_cache_table_pool_name/out2.sql
--rw-r--r--   0        0        0      118 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_external_table_as/out.sql
--rw-r--r--   0        0        0      491 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_external_table_avro/out.sql
--rw-r--r--   0        0        0      241 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_table_delimited/out.sql
--rw-r--r--   0        0        0      128 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_table_like_parquet/out.sql
--rw-r--r--   0        0        0      120 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_table_parquet/out.sql
--rw-r--r--   0        0        0      106 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_table_parquet_like_other/out.sql
--rw-r--r--   0        0        0      140 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_table_parquet_with_schema/out.sql
--rw-r--r--   0        0        0      126 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_create_table_with_location_compile/out.sql
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_drop_partition/out.sql
--rw-r--r--   0        0        0       22 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_drop_table_compile/out1.sql
--rw-r--r--   0        0        0       32 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_drop_table_compile/out2.sql
--rw-r--r--   0        0        0      102 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_load_data_partitioned/out1.sql
--rw-r--r--   0        0        0      112 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_load_data_partitioned/out2.sql
--rw-r--r--   0        0        0       71 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_load_data_unpartitioned/out1.sql
--rw-r--r--   0        0        0       81 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_load_data_unpartitioned/out2.sql
--rw-r--r--   0        0        0      123 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_no_overwrite/out.sql
--rw-r--r--   0        0        0       84 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_select_basics/out1.sql
--rw-r--r--   0        0        0       89 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_ddl_compilation/test_select_basics/out2.sql
--rw-r--r--   0        0        0      282 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_exprs/test_filter_with_analytic/out.sql
--rw-r--r--   0        0        0      103 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_exprs/test_named_from_filter_group_by/abc.sql
--rw-r--r--   0        0        0      103 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_exprs/test_named_from_filter_group_by/foo.sql
--rw-r--r--   0        0        0      123 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_exprs/test_nunique_where/out.sql
--rw-r--r--   0        0        0      100 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_exprs/test_where_with_timestamp/out.sql
--rw-r--r--   0        0        0       28 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_in_not_in/test_field_in_literals/isin/out.sql
--rw-r--r--   0        0        0       32 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_in_not_in/test_field_in_literals/notin/out.sql
--rw-r--r--   0        0        0       61 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_in_not_in/test_isin_notin_in_select/isin/out.sql
--rw-r--r--   0        0        0       65 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_in_not_in/test_isin_notin_in_select/notin/out.sql
--rw-r--r--   0        0        0       20 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_in_not_in/test_literal_in_fields/isin/out.sql
--rw-r--r--   0        0        0       24 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_in_not_in/test_literal_in_fields/notin/out.sql
--rw-r--r--   0        0        0      163 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_group_by_with_window_preserves_range/out.sql
--rw-r--r--   0        0        0       69 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_is_parens/isnull/out.sql
--rw-r--r--   0        0        0       77 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_is_parens/notnull/out.sql
--rw-r--r--   0        0        0      105 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_is_parens_identical_to/out.sql
--rw-r--r--   0        0        0      692 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_join_aliasing/out.sql
--rw-r--r--   0        0        0      945 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_join_key_name/out.sql
--rw-r--r--   0        0        0      637 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_join_key_name2/out.sql
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_join_no_predicates_for_impala/cross_join/out.sql
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_join_no_predicates_for_impala/inner_join/out.sql
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_join_no_predicates_for_impala/left_join/out.sql
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_join_no_predicates_for_impala/outer_join/out.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_join_with_nested_or_condition/out.sql
--rw-r--r--   0        0        0      176 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_join_with_nested_xor_condition/out.sql
--rw-r--r--   0        0        0      111 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_limit_cte_extract/out.sql
--rw-r--r--   0        0        0       76 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_logically_negate_complex_boolean_expr/out.sql
--rw-r--r--   0        0        0      161 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_multiple_filters/out.sql
--rw-r--r--   0        0        0      199 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_multiple_filters2/out.sql
--rw-r--r--   0        0        0      250 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_nested_join_base/out.sql
--rw-r--r--   0        0        0      661 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_nested_join_multiple_ctes/out.sql
--rw-r--r--   0        0        0      495 2023-08-06 00:03:22.187467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_nested_joins_single_cte/out.sql
--rw-r--r--   0        0        0       71 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_sql/test_relabel_projection/out.sql
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_find/out.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/ascii_str/out.sql
--rw-r--r--   0        0        0       21 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/capitalize/out.sql
--rw-r--r--   0        0        0       31 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/extract_host/out.sql
--rw-r--r--   0        0        0       29 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/find/out.sql
--rw-r--r--   0        0        0       36 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/find_in_set_multiple/out.sql
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/find_in_set_single/out.sql
--rw-r--r--   0        0        0       32 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/find_with_offset/out.sql
--rw-r--r--   0        0        0       20 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/length/out.sql
--rw-r--r--   0        0        0       24 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/like/out.sql
--rw-r--r--   0        0        0       52 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/like_multiple/out.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/lower/out.sql
--rw-r--r--   0        0        0       26 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/lpad_char/out.sql
--rw-r--r--   0        0        0       27 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/lpad_default/out.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/lstrip/out.sql
--rw-r--r--   0        0        0       40 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/re_extract/out.sql
--rw-r--r--   0        0        0       44 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/re_replace/out.sql
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/re_search/out.sql
--rw-r--r--   0        0        0       23 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/repeat/out.sql
--rw-r--r--   0        0        0       21 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/reverse/out.sql
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/rlike/out.sql
--rw-r--r--   0        0        0       26 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/rpad_char/out.sql
--rw-r--r--   0        0        0       27 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/rpad_default/out.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/rstrip/out.sql
--rw-r--r--   0        0        0       18 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/strip/out.sql
--rw-r--r--   0        0        0       25 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/strright/out.sql
--rw-r--r--   0        0        0       30 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/substr_0_3/out.sql
--rw-r--r--   0        0        0       27 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/substr_2/out.sql
--rw-r--r--   0        0        0       33 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/translate/out.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_builtins/upper/out.sql
--rw-r--r--   0        0        0       24 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_string_builtins/test_string_join/out.sql
--rw-r--r--   0        0        0      171 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_udf/test_create_uda/False/out.sql
--rw-r--r--   0        0        0      196 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_udf/test_create_uda/True/out.sql
--rw-r--r--   0        0        0       99 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_udf/test_create_udf/out.sql
--rw-r--r--   0        0        0      115 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_udf/test_create_udf_type_conversions/out.sql
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_udf/test_delete_udf_aggregate/out.sql
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_udf/test_delete_udf_db/out.sql
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_udf/test_delete_udf_if_exists/out.sql
--rw-r--r--   0        0        0       41 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_udf/test_delete_udf_simple/out.sql
--rw-r--r--   0        0        0       32 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_udf/test_list_udafs/out.sql
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_udf/test_list_udafs_like/out.sql
--rw-r--r--   0        0        0       22 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_udf/test_list_udf/out.sql
--rw-r--r--   0        0        0       38 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_udf/test_list_udfs_like/out.sql
--rw-r--r--   0        0        0       73 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_udf/test_sql_generation/out.sql
--rw-r--r--   0        0        0       52 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_udf/test_sql_generation_from_infoclass/out.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_hash/out.sql
--rw-r--r--   0        0        0       20 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/log_with_base/out.sql
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/round_expr/out.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/round_no_args/out.sql
--rw-r--r--   0        0        0       22 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/round_two/out.sql
--rw-r--r--   0        0        0       22 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/round_zero/out.sql
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/sign_double/out.sql
--rw-r--r--   0        0        0       17 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/sign_float/out.sql
--rw-r--r--   0        0        0       36 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric/sign_tinyint/out.sql
--rw-r--r--   0        0        0       17 2023-08-06 00:03:22.191467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-abs/out.sql
--rw-r--r--   0        0        0       25 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-approx_median/out.sql
--rw-r--r--   0        0        0       17 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-approx_nunique/out.sql
--rw-r--r--   0        0        0       18 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-ceil/out.sql
--rw-r--r--   0        0        0       17 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-exp/out.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-floor/out.sql
--rw-r--r--   0        0        0       16 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-ln/out.sql
--rw-r--r--   0        0        0       16 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-log/out.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-log10/out.sql
--rw-r--r--   0        0        0       18 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-log2/out.sql
--rw-r--r--   0        0        0       24 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-nullifzero/out.sql
--rw-r--r--   0        0        0       18 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-sqrt/out.sql
--rw-r--r--   0        0        0       24 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/double_col-zeroifnull/out.sql
--rw-r--r--   0        0        0       14 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-abs/out.sql
--rw-r--r--   0        0        0       22 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-approx_median/out.sql
--rw-r--r--   0        0        0       14 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-approx_nunique/out.sql
--rw-r--r--   0        0        0       15 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-ceil/out.sql
--rw-r--r--   0        0        0       14 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-exp/out.sql
--rw-r--r--   0        0        0       16 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-floor/out.sql
--rw-r--r--   0        0        0       13 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-ln/out.sql
--rw-r--r--   0        0        0       13 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-log/out.sql
--rw-r--r--   0        0        0       16 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-log10/out.sql
--rw-r--r--   0        0        0       15 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-log2/out.sql
--rw-r--r--   0        0        0       21 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-nullifzero/out.sql
--rw-r--r--   0        0        0       15 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-sqrt/out.sql
--rw-r--r--   0        0        0       21 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_numeric_unary_builtins/int_col-zeroifnull/out.sql
--rw-r--r--   0        0        0       46 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/avg/out.sql
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/count/out.sql
--rw-r--r--   0        0        0       46 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/max/out.sql
--rw-r--r--   0        0        0       46 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/min/out.sql
--rw-r--r--   0        0        0       53 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/stddev_pop/out.sql
--rw-r--r--   0        0        0       54 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/stddev_samp/out.sql
--rw-r--r--   0        0        0       46 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/sum/out.sql
--rw-r--r--   0        0        0       50 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/var_pop/out.sql
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_unary_builtins/test_reduction_where/var_samp/out.sql
--rw-r--r--   0        0        0       12 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_any_all/all/out.sql
--rw-r--r--   0        0        0       12 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_any_all/any/out.sql
--rw-r--r--   0        0        0       22 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_any_all/not_all/out.sql
--rw-r--r--   0        0        0       22 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_any_all/not_any/out.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_between/out.sql
--rw-r--r--   0        0        0        9 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/add/out.sql
--rw-r--r--   0        0        0       17 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/and/out.sql
--rw-r--r--   0        0        0        9 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/div/out.sql
--rw-r--r--   0        0        0        9 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/eq/out.sql
--rw-r--r--   0        0        0       10 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/ge/out.sql
--rw-r--r--   0        0        0        9 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/gt/out.sql
--rw-r--r--   0        0        0       10 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/le/out.sql
--rw-r--r--   0        0        0        9 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/lt/out.sql
--rw-r--r--   0        0        0        9 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/mul/out.sql
--rw-r--r--   0        0        0       10 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/ne/out.sql
--rw-r--r--   0        0        0       16 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/or/out.sql
--rw-r--r--   0        0        0       13 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/pow/out.sql
--rw-r--r--   0        0        0        9 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/sub/out.sql
--rw-r--r--   0        0        0       46 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_operators/xor/out.sql
--rw-r--r--   0        0        0       13 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_parenthesization/function_call/out.sql
--rw-r--r--   0        0        0       20 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_parenthesization/negation/out.sql
--rw-r--r--   0        0        0       17 2023-08-06 00:03:22.195467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_binary_infix_parenthesization/parens_left/out.sql
--rw-r--r--   0        0        0       21 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_casts/a-int16/out.sql
--rw-r--r--   0        0        0       16 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_casts/a-int32/out.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_casts/a-int64/out.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_casts/a-string/out.sql
--rw-r--r--   0        0        0       20 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_casts/d-int8/out.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_casts/g-double/out.sql
--rw-r--r--   0        0        0       22 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_casts/g-timestamp/out.sql
--rw-r--r--   0        0        0       29 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_column_ref_table_aliases/out.sql
--rw-r--r--   0        0        0       15 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_correlated_predicate_subquery/out.sql
--rw-r--r--   0        0        0       27 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_decimal_casts/column/out.sql
--rw-r--r--   0        0        0       35 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_decimal_casts/literal/out.sql
--rw-r--r--   0        0        0       17 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_isnull_notnull/compound_isnull/out.sql
--rw-r--r--   0        0        0       11 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_isnull_notnull/isnull/out.sql
--rw-r--r--   0        0        0       15 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_isnull_notnull/notnull/out.sql
--rw-r--r--   0        0        0       13 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_literals/embedded_double_quote/out.sql
--rw-r--r--   0        0        0       10 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_literals/embedded_single_quote/out.sql
--rw-r--r--   0        0        0        5 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_literals/false/out.sql
--rw-r--r--   0        0        0        3 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_literals/float/out.sql
--rw-r--r--   0        0        0        1 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_literals/int/out.sql
--rw-r--r--   0        0        0        8 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_literals/simple/out.sql
--rw-r--r--   0        0        0        4 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_literals/true/out.sql
--rw-r--r--   0        0        0       14 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_misc_conditionals/out.sql
--rw-r--r--   0        0        0       30 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_named_expressions/cast/out.sql
--rw-r--r--   0        0        0       27 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_named_expressions/compound_expr/out.sql
--rw-r--r--   0        0        0       20 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_named_expressions/spaces/out.sql
--rw-r--r--   0        0        0        4 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_negate/a/out.sql
--rw-r--r--   0        0        0        4 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_negate/f/out.sql
--rw-r--r--   0        0        0        7 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_negate/h/out.sql
--rw-r--r--   0        0        0      136 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_sql_extract/out.sql
--rw-r--r--   0        0        0       30 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_day_of_week/full_name/out.sql
--rw-r--r--   0        0        0       45 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_day_of_week/index/out.sql
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/days/out1.sql
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/days/out2.sql
--rw-r--r--   0        0        0       49 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/hours/out1.sql
--rw-r--r--   0        0        0       49 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/hours/out2.sql
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/minutes/out1.sql
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/minutes/out2.sql
--rw-r--r--   0        0        0       50 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/months/out1.sql
--rw-r--r--   0        0        0       50 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/months/out2.sql
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/seconds/out1.sql
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/seconds/out2.sql
--rw-r--r--   0        0        0       49 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/weeks/out1.sql
--rw-r--r--   0        0        0       49 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/weeks/out2.sql
--rw-r--r--   0        0        0       49 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/years/out1.sql
--rw-r--r--   0        0        0       49 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_deltas/years/out2.sql
--rw-r--r--   0        0        0       19 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/day/out.sql
--rw-r--r--   0        0        0       20 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/hour/out.sql
--rw-r--r--   0        0        0       27 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/microsecond/out.sql
--rw-r--r--   0        0        0       27 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/millisecond/out.sql
--rw-r--r--   0        0        0       22 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/minute/out.sql
--rw-r--r--   0        0        0       21 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/month/out.sql
--rw-r--r--   0        0        0       22 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/second/out.sql
--rw-r--r--   0        0        0       20 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_extract_field/year/out.sql
--rw-r--r--   0        0        0       60 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_from_integer/default/out.sql
--rw-r--r--   0        0        0       87 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_from_integer/ms/out.sql
--rw-r--r--   0        0        0       90 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_from_integer/us/out.sql
--rw-r--r--   0        0        0       21 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_literals/pd_timestamp/out.sql
--rw-r--r--   0        0        0       21 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_literals/pydatetime/out.sql
--rw-r--r--   0        0        0       21 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_literals/timestamp_function/out.sql
--rw-r--r--   0        0        0        5 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_value_exprs/test_timestamp_now/out.sql
--rw-r--r--   0        0        0      444 2023-08-06 00:03:22.199467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_add_default_order_by/out.sql
--rw-r--r--   0        0        0       74 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_aggregate_in_projection/out.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/max/out1.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/max/out2.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/mean/out1.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/mean/out2.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/min/out1.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/min/out2.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/sum/out1.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_cumulative_functions/sum/out2.sql
--rw-r--r--   0        0        0      113 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_multiple_windows/out.sql
--rw-r--r--   0        0        0      114 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_nested_analytic_function/out.sql
--rw-r--r--   0        0        0       93 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_order_by_desc/out1.sql
--rw-r--r--   0        0        0      174 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_order_by_desc/out2.sql
--rw-r--r--   0        0        0      154 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_propagate_nested_windows/out.sql
--rw-r--r--   0        0        0      149 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_rank_functions/out.sql
--rw-r--r--   0        0        0       86 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_row_number_does_not_require_order_by/out1.sql
--rw-r--r--   0        0        0      113 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_row_number_does_not_require_order_by/out2.sql
--rw-r--r--   0        0        0       90 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_row_number_properly_composes_with_arithmetic/out.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/cumulative/out.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/foll_0/out.sql
--rw-r--r--   0        0        0      115 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/foll_10_5/out.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/foll_2/out.sql
--rw-r--r--   0        0        0      114 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/foll_2_prec_0/out.sql
--rw-r--r--   0        0        0      115 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/foll_5_10/out.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/prec_0/out.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/prec_5/out.sql
--rw-r--r--   0        0        0      114 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/prec_5_foll_0/out.sql
--rw-r--r--   0        0        0      114 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/prec_5_foll_2/out.sql
--rw-r--r--   0        0        0      115 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/snapshots/test_window/test_window_frame_specs/trailing_10/out.sql
--rw-r--r--   0        0        0     1215 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_analytic_functions.py
--rw-r--r--   0        0        0     2930 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_bucket_histogram.py
--rw-r--r--   0        0        0     2885 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_case_exprs.py
--rw-r--r--   0        0        0     8492 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_client.py
--rw-r--r--   0        0        0     1076 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_coalesce_greater_least.py
--rw-r--r--   0        0        0    10484 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_ddl.py
--rw-r--r--   0        0        0     9191 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_ddl_compilation.py
--rw-r--r--   0        0        0    19410 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_exprs.py
--rw-r--r--   0        0        0     1204 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_in_not_in.py
--rw-r--r--   0        0        0     3857 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_metadata.py
--rw-r--r--   0        0        0     5680 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_pandas_interop.py
--rw-r--r--   0        0        0     2863 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_parquet_ddl.py
--rw-r--r--   0        0        0     7396 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_partition.py
--rw-r--r--   0        0        0     1704 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_patched.py
--rw-r--r--   0        0        0     9528 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_sql.py
--rw-r--r--   0        0        0     2586 2023-08-06 00:03:22.203467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_string_builtins.py
--rw-r--r--   0        0        0    17381 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_udf.py
--rw-r--r--   0        0        0     3336 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_unary_builtins.py
--rw-r--r--   0        0        0     7982 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_value_exprs.py
--rw-r--r--   0        0        0     5089 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_window.py
--rw-r--r--   0        0        0     8886 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/impala/udf.py
--rw-r--r--   0        0        0     6284 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/mssql/__init__.py
--rw-r--r--   0        0        0     1107 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/mssql/compiler.py
--rw-r--r--   0        0        0     4580 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/mssql/datatypes.py
--rw-r--r--   0        0        0     7414 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/mssql/registry.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/mssql/tests/__init__.py
--rw-r--r--   0        0        0     2415 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/mssql/tests/conftest.py
--rw-r--r--   0        0        0     2719 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/mssql/tests/test_client.py
--rw-r--r--   0        0        0     5681 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/mysql/__init__.py
--rw-r--r--   0        0        0      786 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/mysql/compiler.py
--rw-r--r--   0        0        0     7347 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/mysql/datatypes.py
--rw-r--r--   0        0        0     8341 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/mysql/registry.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/mysql/tests/__init__.py
--rw-r--r--   0        0        0     4130 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/mysql/tests/conftest.py
--rw-r--r--   0        0        0     5197 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/mysql/tests/test_client.py
--rw-r--r--   0        0        0     7122 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/oracle/__init__.py
--rw-r--r--   0        0        0     2466 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/oracle/datatypes.py
--rw-r--r--   0        0        0     3086 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/oracle/registry.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/oracle/tests/__init__.py
--rw-r--r--   0        0        0     4981 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/oracle/tests/conftest.py
--rw-r--r--   0        0        0      217 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/oracle/tests/test_client.py
--rw-r--r--   0        0        0     9281 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/__init__.py
--rw-r--r--   0        0        0    21417 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/aggcontext.py
--rw-r--r--   0        0        0    19215 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/core.py
--rw-r--r--   0        0        0     2948 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/dispatch.py
--rw-r--r--   0        0        0     4543 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/dispatcher.py
--rw-r--r--   0        0        0      772 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/execution/__init__.py
--rw-r--r--   0        0        0     4711 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/execution/arrays.py
--rw-r--r--   0        0        0     2153 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/execution/constants.py
--rw-r--r--   0        0        0     4231 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/execution/decimal.py
--rw-r--r--   0        0        0    50710 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/execution/generic.py
--rw-r--r--   0        0        0     5572 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/execution/join.py
--rw-r--r--   0        0        0     6386 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/execution/maps.py
--rw-r--r--   0        0        0    10805 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/execution/selection.py
--rw-r--r--   0        0        0    17914 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/execution/strings.py
--rw-r--r--   0        0        0     1411 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/execution/structs.py
--rw-r--r--   0        0        0    11129 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/execution/temporal.py
--rw-r--r--   0        0        0     2916 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/execution/timecontext.py
--rw-r--r--   0        0        0     4178 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/execution/util.py
--rw-r--r--   0        0        0    17781 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/execution/window.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/__init__.py
--rw-r--r--   0        0        0     1169 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/conftest.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/execution/__init__.py
--rw-r--r--   0        0        0     7963 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/execution/conftest.py
--rw-r--r--   0        0        0     6233 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/execution/test_arrays.py
--rw-r--r--   0        0        0     5306 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/execution/test_cast.py
--rw-r--r--   0        0        0     8920 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/execution/test_functions.py
--rw-r--r--   0        0        0    19191 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/execution/test_join.py
--rw-r--r--   0        0        0     2813 2023-08-06 00:03:22.207467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/execution/test_maps.py
--rw-r--r--   0        0        0    26026 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/execution/test_operations.py
--rw-r--r--   0        0        0     5205 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/execution/test_strings.py
--rw-r--r--   0        0        0     2407 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/execution/test_structs.py
--rw-r--r--   0        0        0     5979 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/execution/test_temporal.py
--rw-r--r--   0        0        0    14391 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/execution/test_timecontext.py
--rw-r--r--   0        0        0    22511 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/execution/test_window.py
--rw-r--r--   0        0        0     4020 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/test_aggcontext.py
--rw-r--r--   0        0        0     2326 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/test_client.py
--rw-r--r--   0        0        0     2880 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/test_core.py
--rw-r--r--   0        0        0     2700 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/test_dispatcher.py
--rw-r--r--   0        0        0    12094 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/tests/test_udf.py
--rw-r--r--   0        0        0     5883 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/trace.py
--rw-r--r--   0        0        0     6263 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pandas/udf.py
--rw-r--r--   0        0        0    16135 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/polars/__init__.py
--rw-r--r--   0        0        0    31851 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/polars/compiler.py
--rw-r--r--   0        0        0     3092 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/polars/datatypes.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/polars/tests/__init__.py
--rw-r--r--   0        0        0     1726 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/polars/tests/conftest.py
--rw-r--r--   0        0        0     2226 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/polars/tests/test_datatypes.py
--rw-r--r--   0        0        0     1424 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/polars/tests/test_udf.py
--rw-r--r--   0        0        0     9789 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/__init__.py
--rw-r--r--   0        0        0     1055 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/compiler.py
--rw-r--r--   0        0        0     5069 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/datatypes.py
--rw-r--r--   0        0        0    26647 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/registry.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/tests/__init__.py
--rw-r--r--   0        0        0     3903 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/tests/conftest.py
--rw-r--r--   0        0        0       47 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/tests/snapshots/test_client/test_compile_toplevel/out.sql
--rw-r--r--   0        0        0      814 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/tests/snapshots/test_functions/test_union_cte/False/out.sql
--rw-r--r--   0        0        0      806 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/tests/snapshots/test_functions/test_union_cte/True/out.sql
--rw-r--r--   0        0        0     7177 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/tests/test_client.py
--rw-r--r--   0        0        0    42207 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/tests/test_functions.py
--rw-r--r--   0        0        0    19767 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/tests/test_geospatial.py
--rw-r--r--   0        0        0      415 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/tests/test_json.py
--rw-r--r--   0        0        0     9844 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/tests/test_postgis.py
--rw-r--r--   0        0        0      488 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/tests/test_string.py
--rw-r--r--   0        0        0     5372 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/tests/test_udf.py
--rw-r--r--   0        0        0     5817 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/postgres/udf.py
--rw-r--r--   0        0        0    23682 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/__init__.py
--rw-r--r--   0        0        0     4711 2023-08-06 00:03:22.211467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/client.py
--rw-r--r--   0        0        0    59676 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/compiler.py
--rw-r--r--   0        0        0     3907 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/datatypes.py
--rw-r--r--   0        0        0     5199 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/ddl.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/tests/__init__.py
--rw-r--r--   0        0        0    10798 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/tests/conftest.py
--rw-r--r--   0        0        0      676 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/tests/test_aggregation.py
--rw-r--r--   0        0        0     5132 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/tests/test_array.py
--rw-r--r--   0        0        0     7767 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/tests/test_basic.py
--rw-r--r--   0        0        0     5617 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/tests/test_ddl.py
--rw-r--r--   0        0        0      845 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/tests/test_null.py
--rw-r--r--   0        0        0     4000 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/tests/test_timecontext.py
--rw-r--r--   0        0        0     2401 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/tests/test_window.py
--rw-r--r--   0        0        0    13841 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/tests/test_window_context_adjustment.py
--rw-r--r--   0        0        0     2647 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/pyspark/timecontext.py
--rw-r--r--   0        0        0    21320 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/snowflake/__init__.py
--rw-r--r--   0        0        0      293 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/snowflake/converter.py
--rw-r--r--   0        0        0     2874 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/snowflake/datatypes.py
--rw-r--r--   0        0        0    16564 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/snowflake/registry.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/snowflake/tests/__init__.py
--rw-r--r--   0        0        0     3164 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/snowflake/tests/conftest.py
--rw-r--r--   0        0        0     4388 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/snowflake/tests/test_client.py
--rw-r--r--   0        0        0     9470 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/sqlite/__init__.py
--rw-r--r--   0        0        0     1179 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/sqlite/compiler.py
--rw-r--r--   0        0        0     3139 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/sqlite/datatypes.py
--rw-r--r--   0        0        0    15246 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/sqlite/registry.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/sqlite/tests/__init__.py
--rw-r--r--   0        0        0     2957 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/sqlite/tests/conftest.py
--rw-r--r--   0        0        0       42 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/sqlite/tests/snapshots/test_client/test_compile_toplevel/out.sql
--rw-r--r--   0        0        0      616 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/sqlite/tests/snapshots/test_functions/test_count_on_order_by/out.sql
--rw-r--r--   0        0        0     2687 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/sqlite/tests/test_client.py
--rw-r--r--   0        0        0    21028 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/sqlite/tests/test_functions.py
--rw-r--r--   0        0        0     2730 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/sqlite/tests/test_types.py
--rw-r--r--   0        0        0    10074 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/sqlite/udf.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/tests/__init__.py
--rw-r--r--   0        0        0     9375 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/tests/base.py
--rw-r--r--   0        0        0     2637 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/tests/data.py
--rw-r--r--   0        0        0      511 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/bigquery/out.sql
--rw-r--r--   0        0        0     1294 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/clickhouse/out.sql
--rw-r--r--   0        0        0      497 2023-08-06 00:03:22.215467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/duckdb/out.sql
--rw-r--r--   0        0        0      513 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/impala/out.sql
--rw-r--r--   0        0        0      523 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/mssql/out.sql
--rw-r--r--   0        0        0      535 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/mysql/out.sql
--rw-r--r--   0        0        0      540 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/oracle/out.sql
--rw-r--r--   0        0        0      497 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/postgres/out.sql
--rw-r--r--   0        0        0      543 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/snowflake/out.sql
--rw-r--r--   0        0        0      523 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/sqlite/out.sql
--rw-r--r--   0        0        0      497 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_generic/test_many_subqueries/trino/out.sql
--rw-r--r--   0        0        0      267 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/bigquery/out.sql
--rw-r--r--   0        0        0      588 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/clickhouse/out.sql
--rw-r--r--   0        0        0      281 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/duckdb/out.sql
--rw-r--r--   0        0        0      269 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/impala/out.sql
--rw-r--r--   0        0        0      287 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/mssql/out.sql
--rw-r--r--   0        0        0      302 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/mysql/out.sql
--rw-r--r--   0        0        0      283 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/oracle/out.sql
--rw-r--r--   0        0        0      264 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/postgres/out.sql
--rw-r--r--   0        0        0      288 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/snowflake/out.sql
--rw-r--r--   0        0        0      287 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/sqlite/out.sql
--rw-r--r--   0        0        0      264 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_cte_refs_in_topo_order/trino/out.sql
--rw-r--r--   0        0        0      387 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/bigquery/out.sql
--rw-r--r--   0        0        0      671 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/clickhouse/out.sql
--rw-r--r--   0        0        0      379 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/duckdb/out.sql
--rw-r--r--   0        0        0      389 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/impala/out.sql
--rw-r--r--   0        0        0      701 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/mssql/out.sql
--rw-r--r--   0        0        0      379 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/mysql/out.sql
--rw-r--r--   0        0        0      680 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/oracle/out.sql
--rw-r--r--   0        0        0      379 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/postgres/out.sql
--rw-r--r--   0        0        0      389 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/snowflake/out.sql
--rw-r--r--   0        0        0      379 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/sqlite/out.sql
--rw-r--r--   0        0        0      379 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_group_by_has_index/trino/out.sql
--rw-r--r--   0        0        0      179 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_isin_bug/bigquery/out.sql
--rw-r--r--   0        0        0      171 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_isin_bug/clickhouse/out.sql
--rw-r--r--   0        0        0      196 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_isin_bug/duckdb/out.sql
--rw-r--r--   0        0        0      184 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_isin_bug/impala/out.sql
--rw-r--r--   0        0        0      179 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_isin_bug/mssql/out.sql
--rw-r--r--   0        0        0      179 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_isin_bug/mysql/out.sql
--rw-r--r--   0        0        0      184 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_isin_bug/oracle/out.sql
--rw-r--r--   0        0        0      179 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_isin_bug/postgres/out.sql
--rw-r--r--   0        0        0      193 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_isin_bug/snowflake/out.sql
--rw-r--r--   0        0        0      179 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_isin_bug/sqlite/out.sql
--rw-r--r--   0        0        0      179 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_isin_bug/trino/out.sql
--rw-r--r--   0        0        0     6238 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_union_aliasing/clickhouse/out.sql
--rw-r--r--   0        0        0     2823 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_union_aliasing/duckdb/out.sql
--rw-r--r--   0        0        0     2882 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_union_aliasing/postgres/out.sql
--rw-r--r--   0        0        0     3166 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/snapshots/test_sql/test_union_aliasing/trino/out.sql
--rw-r--r--   0        0        0    49423 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_aggregation.py
--rw-r--r--   0        0        0     4234 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_api.py
--rw-r--r--   0        0        0    22336 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_array.py
--rw-r--r--   0        0        0      934 2023-08-06 00:03:22.219467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_binary.py
--rw-r--r--   0        0        0    42139 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_client.py
--rw-r--r--   0        0        0     1143 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_column.py
--rw-r--r--   0        0        0     4319 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_dataframe_interchange.py
--rw-r--r--   0        0        0     8525 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_dot_sql.py
--rw-r--r--   0        0        0     1039 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_examples.py
--rw-r--r--   0        0        0    14127 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_export.py
--rw-r--r--   0        0        0    38124 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_generic.py
--rw-r--r--   0        0        0    10209 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_join.py
--rw-r--r--   0        0        0     2612 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_json.py
--rw-r--r--   0        0        0     8101 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_map.py
--rw-r--r--   0        0        0     3533 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_network.py
--rw-r--r--   0        0        0    51415 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_numeric.py
--rw-r--r--   0        0        0     7335 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_param.py
--rw-r--r--   0        0        0    11787 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_register.py
--rw-r--r--   0        0        0     6944 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_set_ops.py
--rw-r--r--   0        0        0     6357 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_sql.py
--rw-r--r--   0        0        0    36553 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_string.py
--rw-r--r--   0        0        0     2783 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_struct.py
--rw-r--r--   0        0        0    77753 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_temporal.py
--rw-r--r--   0        0        0     3622 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_timecontext.py
--rw-r--r--   0        0        0     4639 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_udf.py
--rw-r--r--   0        0        0     1930 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_uuid.py
--rw-r--r--   0        0        0    24187 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_vectorized_udf.py
--rw-r--r--   0        0        0    33789 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/tests/test_window.py
--rw-r--r--   0        0        0     9645 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/trino/__init__.py
--rw-r--r--   0        0        0     1371 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/trino/compiler.py
--rw-r--r--   0        0        0     6980 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/trino/datatypes.py
--rw-r--r--   0        0        0    16717 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/trino/registry.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.223467 ibis_framework-6.1.1.dev22/ibis/backends/trino/tests/__init__.py
--rw-r--r--   0        0        0     4913 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/backends/trino/tests/conftest.py
--rw-r--r--   0        0        0      860 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/backends/trino/tests/test_client.py
--rw-r--r--   0        0        0     2221 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/backends/trino/tests/test_datatypes.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/__init__.py
--rw-r--r--   0        0        0    15799 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/annotations.py
--rw-r--r--   0        0        0     4587 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/caching.py
--rw-r--r--   0        0        0     7266 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/collections.py
--rw-r--r--   0        0        0     2931 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/dispatch.py
--rw-r--r--   0        0        0    24977 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/egraph.py
--rw-r--r--   0        0        0     4220 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/exceptions.py
--rw-r--r--   0        0        0     6401 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/graph.py
--rw-r--r--   0        0        0     8044 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/grounds.py
--rw-r--r--   0        0        0     1019 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/parsing.py
--rw-r--r--   0        0        0    34175 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/patterns.py
--rw-r--r--   0        0        0     6111 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/temporal.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/tests/__init__.py
--rw-r--r--   0        0        0      150 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/tests/conftest.py
--rw-r--r--   0        0        0    11424 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/tests/test_annotations.py
--rw-r--r--   0        0        0     7125 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/tests/test_collections.py
--rw-r--r--   0        0        0     2443 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/tests/test_dispatch.py
--rw-r--r--   0        0        0    12854 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/tests/test_egraph.py
--rw-r--r--   0        0        0     4277 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/tests/test_graph.py
--rw-r--r--   0        0        0    26483 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/tests/test_grounds.py
--rw-r--r--   0        0        0     1023 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/tests/test_grounds_py310.py
--rw-r--r--   0        0        0    23681 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/tests/test_patterns.py
--rw-r--r--   0        0        0     7372 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/tests/test_temporal.py
--rw-r--r--   0        0        0     2294 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/tests/test_typing.py
--rw-r--r--   0        0        0     8820 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/tests/test_validators.py
--rw-r--r--   0        0        0     4431 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/typing.py
--rw-r--r--   0        0        0    17321 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/common/validators.py
--rw-r--r--   0        0        0     5973 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/config.py
--rw-r--r--   0        0        0     1105 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/conftest.py
--rw-r--r--   0        0        0      758 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/examples/CITATIONS.md
--rw-r--r--   0        0        0     4624 2023-08-06 00:03:22.227467 ibis_framework-6.1.1.dev22/ibis/examples/__init__.py
--rw-r--r--   0        0        0    44481 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/examples/metadata.json
--rw-r--r--   0        0        0    38707 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/examples/registry.txt
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/examples/tests/__init__.py
--rw-r--r--   0        0        0     3022 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/examples/tests/test_examples.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/__init__.py
--rw-r--r--   0        0        0    29394 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/analysis.py
--rw-r--r--   0        0        0    43627 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/api.py
--rw-r--r--   0        0        0     9884 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/builders.py
--rw-r--r--   0        0        0      523 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/datatypes/__init__.py
--rw-r--r--   0        0        0     6555 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/datatypes/cast.py
--rw-r--r--   0        0        0    28719 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/datatypes/core.py
--rw-r--r--   0        0        0     5279 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/datatypes/parse.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/datatypes/tests/__init__.py
--rw-r--r--   0        0        0     1665 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/datatypes/tests/test_cast.py
--rw-r--r--   0        0        0    15037 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/datatypes/tests/test_core.py
--rw-r--r--   0        0        0     6815 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/datatypes/tests/test_parse.py
--rw-r--r--   0        0        0    11753 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/datatypes/tests/test_value.py
--rw-r--r--   0        0        0     9769 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/datatypes/value.py
--rw-r--r--   0        0        0    11135 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/decompile.py
--rw-r--r--   0        0        0     7904 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/deferred.py
--rw-r--r--   0        0        0    20328 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/format.py
--rw-r--r--   0        0        0     1121 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/operations/__init__.py
--rw-r--r--   0        0        0     3233 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/operations/analytic.py
--rw-r--r--   0        0        0     4077 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/operations/arrays.py
--rw-r--r--   0        0        0     3048 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/operations/core.py
--rw-r--r--   0        0        0     7776 2023-08-06 00:03:22.231467 ibis_framework-6.1.1.dev22/ibis/expr/operations/generic.py
--rw-r--r--   0        0        0    10458 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/geospatial.py
--rw-r--r--   0        0        0     1480 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/histograms.py
--rw-r--r--   0        0        0      615 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/json.py
--rw-r--r--   0        0        0     6519 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/logical.py
--rw-r--r--   0        0        0     1764 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/maps.py
--rw-r--r--   0        0        0     6504 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/numeric.py
--rw-r--r--   0        0        0     8034 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/reductions.py
--rw-r--r--   0        0        0    20225 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/relations.py
--rw-r--r--   0        0        0      539 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/sortkeys.py
--rw-r--r--   0        0        0     5157 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/strings.py
--rw-r--r--   0        0        0      980 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/structs.py
--rw-r--r--   0        0        0     6310 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/temporal.py
--rw-r--r--   0        0        0     4911 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/udf.py
--rw-r--r--   0        0        0     1096 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/vectorized.py
--rw-r--r--   0        0        0     3338 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/operations/window.py
--rw-r--r--   0        0        0    14746 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/rules.py
--rw-r--r--   0        0        0     7499 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/schema.py
--rw-r--r--   0        0        0      261 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/selectors.py
--rw-r--r--   0        0        0    10213 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/sql.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/tests/__init__.py
--rw-r--r--   0        0        0     3288 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/tests/test_api.py
--rw-r--r--   0        0        0     3960 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/tests/test_deferred.py
--rw-r--r--   0        0        0     8256 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/tests/test_rules.py
--rw-r--r--   0        0        0    12446 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/tests/test_schema.py
--rw-r--r--   0        0        0      861 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/types/__init__.py
--rw-r--r--   0        0        0    38704 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/types/arrays.py
--rw-r--r--   0        0        0      819 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/types/binary.py
--rw-r--r--   0        0        0      271 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/types/collections.py
--rw-r--r--   0        0        0    17828 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/types/core.py
--rw-r--r--   0        0        0     5402 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/types/dataframe_interchange.py
--rw-r--r--   0        0        0    53553 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/types/generic.py
--rw-r--r--   0        0        0    20863 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/types/geospatial.py
--rw-r--r--   0        0        0     9717 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/types/groupby.py
--rw-r--r--   0        0        0      500 2023-08-06 00:03:22.235467 ibis_framework-6.1.1.dev22/ibis/expr/types/inet.py
--rw-r--r--   0        0        0     5142 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/expr/types/json.py
--rw-r--r--   0        0        0    18817 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/expr/types/logical.py
--rw-r--r--   0        0        0    16606 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/expr/types/maps.py
--rw-r--r--   0        0        0    38464 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/expr/types/numeric.py
--rw-r--r--   0        0        0    11977 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/expr/types/pretty.py
--rw-r--r--   0        0        0   199196 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/expr/types/relations.py
--rw-r--r--   0        0        0       95 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/expr/types/ruff.toml
--rw-r--r--   0        0        0    51230 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/expr/types/strings.py
--rw-r--r--   0        0        0    14220 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/expr/types/structs.py
--rw-r--r--   0        0        0    18509 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/expr/types/temporal.py
--rw-r--r--   0        0        0      146 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/expr/types/typing.py
--rw-r--r--   0        0        0      312 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/expr/types/uuid.py
--rw-r--r--   0        0        0     5953 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/expr/visualize.py
--rw-r--r--   0        0        0     4965 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/formats/__init__.py
--rw-r--r--   0        0        0     3534 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/formats/numpy.py
--rw-r--r--   0        0        0     7938 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/formats/pandas.py
--rw-r--r--   0        0        0     8951 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/formats/pyarrow.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/formats/tests/__init__.py
--rw-r--r--   0        0        0     6926 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/formats/tests/test_dask.py
--rw-r--r--   0        0        0     4223 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/formats/tests/test_numpy.py
--rw-r--r--   0        0        0    13869 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/formats/tests/test_pandas.py
--rw-r--r--   0        0        0     5905 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/formats/tests/test_pyarrow.py
--rw-r--r--   0        0        0      235 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/interactive.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/legacy/__init__.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.239467 ibis_framework-6.1.1.dev22/ibis/legacy/udf/__init__.py
--rw-r--r--   0        0        0     2281 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/legacy/udf/validate.py
--rw-r--r--   0        0        0    11839 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/legacy/udf/vectorized.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/py.typed
--rw-r--r--   0        0        0    22911 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/selectors.py
--rw-r--r--   0        0        0     2241 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/streamlit/__init__.py
--rw-r--r--   0        0        0      609 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/__init__.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/benchmarks/__init__.py
--rw-r--r--   0        0        0    19802 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/benchmarks/test_benchmarks.py
--rw-r--r--   0        0        0      829 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/conftest.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/__init__.py
--rw-r--r--   0        0        0     2827 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/conftest.py
--rw-r--r--   0        0        0    15733 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/mocks.py
--rw-r--r--   0        0        0       66 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/snapshots/test_interactive/test_default_limit/out.sql
--rw-r--r--   0        0        0       66 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/snapshots/test_interactive/test_disable_query_limit/out.sql
--rw-r--r--   0        0        0       66 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/snapshots/test_interactive/test_respect_set_limit/out.sql
--rw-r--r--   0        0        0     9672 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_analysis.py
--rw-r--r--   0        0        0     2767 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_analytics.py
--rw-r--r--   0        0        0     2744 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_case.py
--rw-r--r--   0        0        0     4082 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_decimal.py
--rw-r--r--   0        0        0     1891 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_decompile.py
--rw-r--r--   0        0        0     9697 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_format.py
--rw-r--r--   0        0        0     3304 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_interactive.py
--rw-r--r--   0        0        0     4836 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_literal.py
--rw-r--r--   0        0        0     7276 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_operations.py
--rw-r--r--   0        0        0      789 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_operations_py310.py
--rw-r--r--   0        0        0     1599 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_pipe.py
--rw-r--r--   0        0        0     4857 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_pretty_repr.py
--rw-r--r--   0        0        0    12619 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_selectors.py
--rw-r--r--   0        0        0     1305 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_set_operations.py
--rw-r--r--   0        0        0     3047 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_sql.py
--rw-r--r--   0        0        0     5893 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_sql_builtins.py
--rw-r--r--   0        0        0     3549 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_string.py
--rw-r--r--   0        0        0     2389 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_struct.py
--rw-r--r--   0        0        0    52745 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_table.py
--rw-r--r--   0        0        0    23563 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_temporal.py
--rw-r--r--   0        0        0     5753 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_timestamp.py
--rw-r--r--   0        0        0     2296 2023-08-06 00:03:22.243467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_udf.py
--rw-r--r--   0        0        0    47849 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_value_exprs.py
--rw-r--r--   0        0        0     4572 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_visualize.py
--rw-r--r--   0        0        0    15487 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_window_frames.py
--rw-r--r--   0        0        0     1514 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/expr/test_window_functions.py
--rw-r--r--   0        0        0        0 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/__init__.py
--rw-r--r--   0        0        0     4844 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/conftest.py
--rw-r--r--   0        0        0      207 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_agg_and_non_agg_filter/out.sql
--rw-r--r--   0        0        0      214 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_agg_filter/out.sql
--rw-r--r--   0        0        0      214 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_agg_filter_with_alias/out.sql
--rw-r--r--   0        0        0      582 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_column_distinct/decompiled.py
--rw-r--r--   0        0        0       59 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_column_distinct/out.sql
--rw-r--r--   0        0        0      128 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_column_expr_default_name/decompiled.py
--rw-r--r--   0        0        0       66 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_column_expr_default_name/out.sql
--rw-r--r--   0        0        0      142 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_column_expr_retains_name/decompiled.py
--rw-r--r--   0        0        0       54 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_column_expr_retains_name/out.sql
--rw-r--r--   0        0        0      699 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_count_distinct/decompiled.py
--rw-r--r--   0        0        0      130 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_count_distinct/out.sql
--rw-r--r--   0        0        0     1049 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_difference_project_column/decompiled.py
--rw-r--r--   0        0        0      450 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_difference_project_column/out.sql
--rw-r--r--   0        0        0      201 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_having_from_filter/decompiled.py
--rw-r--r--   0        0        0       98 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_having_from_filter/out.sql
--rw-r--r--   0        0        0      139 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_having_size/out.sql
--rw-r--r--   0        0        0     1056 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_intersect_project_column/decompiled.py
--rw-r--r--   0        0        0      453 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_intersect_project_column/out.sql
--rw-r--r--   0        0        0      736 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_multiple_count_distinct/decompiled.py
--rw-r--r--   0        0        0      166 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_multiple_count_distinct/out.sql
--rw-r--r--   0        0        0      187 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_pushdown_with_or/out.sql
--rw-r--r--   0        0        0      169 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_simple_agg_filter/out.sql
--rw-r--r--   0        0        0      502 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_subquery_where_location/decompiled.py
--rw-r--r--   0        0        0      288 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_subquery_where_location/out.sql
--rw-r--r--   0        0        0     1020 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_table_difference/decompiled.py
--rw-r--r--   0        0        0      386 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_table_difference/out.sql
--rw-r--r--   0        0        0      619 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_table_distinct/decompiled.py
--rw-r--r--   0        0        0       73 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_table_distinct/out.sql
--rw-r--r--   0        0        0      427 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_table_drop_with_filter/decompiled.py
--rw-r--r--   0        0        0      335 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_table_drop_with_filter/out.sql
--rw-r--r--   0        0        0     1027 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_table_intersect/decompiled.py
--rw-r--r--   0        0        0      389 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_table_intersect/out.sql
--rw-r--r--   0        0        0      995 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_union/decompiled.py
--rw-r--r--   0        0        0      385 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_union/out.sql
--rw-r--r--   0        0        0      174 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_union_order_by/decompiled.py
--rw-r--r--   0        0        0      160 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_union_order_by/out.sql
--rw-r--r--   0        0        0     1000 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_union_project_column/decompiled.py
--rw-r--r--   0        0        0      453 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_compiler/test_union_project_column/out.sql
--rw-r--r--   0        0        0      536 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_aggregate_count_joined/decompiled.py
--rw-r--r--   0        0        0      180 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_aggregate_count_joined/out.sql
--rw-r--r--   0        0        0       91 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_aggregate_having/explicit.sql
--rw-r--r--   0        0        0       89 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_aggregate_having/inline.sql
--rw-r--r--   0        0        0      172 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_aggregate_projection_alias_bug/out.sql
--rw-r--r--   0        0        0      175 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_aggregate_projection_subquery/agg_filtered.sql
--rw-r--r--   0        0        0      164 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_aggregate_projection_subquery/agg_filtered2.sql
--rw-r--r--   0        0        0      100 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_aggregate_projection_subquery/filtered.sql
--rw-r--r--   0        0        0       71 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_aggregate_projection_subquery/proj.sql
--rw-r--r--   0        0        0      324 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_anti_join/decompiled.py
--rw-r--r--   0        0        0       84 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_anti_join/out.sql
--rw-r--r--   0        0        0      197 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_bool_bool/decompiled.py
--rw-r--r--   0        0        0       73 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_bool_bool/out.sql
--rw-r--r--   0        0        0      359 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_bug_duplicated_where/out.sql
--rw-r--r--   0        0        0      567 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_bug_project_multiple_times/out.sql
--rw-r--r--   0        0        0      832 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_case_in_projection/decompiled.py
--rw-r--r--   0        0        0      255 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_case_in_projection/out.sql
--rw-r--r--   0        0        0      823 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_complex_union/result.sql
--rw-r--r--   0        0        0      274 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_double_nested_subquery_no_aliases/out.sql
--rw-r--r--   0        0        0      217 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_endswith/decompiled.py
--rw-r--r--   0        0        0       65 2023-08-06 00:03:22.247467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_endswith/out.sql
--rw-r--r--   0        0        0      180 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_filter_inside_exists/out.sql
--rw-r--r--   0        0        0      226 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_filter_predicates/out.sql
--rw-r--r--   0        0        0      344 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_filter_self_join_analysis_bug/result.sql
--rw-r--r--   0        0        0      126 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_filter_subquery_derived_reduction/expr3.sql
--rw-r--r--   0        0        0      132 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_filter_subquery_derived_reduction/expr4.sql
--rw-r--r--   0        0        0      300 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_fuse_projections/decompiled.py
--rw-r--r--   0        0        0      104 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_fuse_projections/project.sql
--rw-r--r--   0        0        0      121 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_fuse_projections/project_filter.sql
--rw-r--r--   0        0        0       64 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_identifier_quoting/out.sql
--rw-r--r--   0        0        0       79 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_incorrect_predicate_pushdown/result.sql
--rw-r--r--   0        0        0       70 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_incorrect_predicate_pushdown_with_literal/result.sql
--rw-r--r--   0        0        0      690 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_join_between_joins/decompiled.py
--rw-r--r--   0        0        0      314 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_join_between_joins/out.sql
--rw-r--r--   0        0        0      409 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_join_filtered_tables_no_pushdown/out.sql
--rw-r--r--   0        0        0      848 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_join_just_materialized/decompiled.py
--rw-r--r--   0        0        0      172 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_join_just_materialized/out.sql
--rw-r--r--   0        0        0      286 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_join_projection_subquery_bug/out.sql
--rw-r--r--   0        0        0      131 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_join_with_limited_table/out.sql
--rw-r--r--   0        0        0      109 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_limit_cte_extract/out.sql
--rw-r--r--   0        0        0     2105 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_limit_with_self_join/decompiled.py
--rw-r--r--   0        0        0      998 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_limit_with_self_join/out.sql
--rw-r--r--   0        0        0      323 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_loj_subquery_filter_handling/out.sql
--rw-r--r--   0        0        0      735 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_multiple_joins/decompiled.py
--rw-r--r--   0        0        0      291 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_multiple_joins/out.sql
--rw-r--r--   0        0        0      595 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_multiple_limits/decompiled.py
--rw-r--r--   0        0        0       48 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_multiple_limits/out.sql
--rw-r--r--   0        0        0       72 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_nameless_table/decompiled.py
--rw-r--r--   0        0        0       23 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_nameless_table/out.sql
--rw-r--r--   0        0        0      698 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_order_by_on_limit_yield_subquery/decompiled.py
--rw-r--r--   0        0        0      151 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_order_by_on_limit_yield_subquery/out.sql
--rw-r--r--   0        0        0       88 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_projection_filter_fuse/out.sql
--rw-r--r--   0        0        0       89 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_scalar_subquery_different_table/out.sql
--rw-r--r--   0        0        0      215 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/agg_explicit_column/decompiled.py
--rw-r--r--   0        0        0       67 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/agg_explicit_column/out.sql
--rw-r--r--   0        0        0      237 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/agg_string_columns/decompiled.py
--rw-r--r--   0        0        0       83 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/agg_string_columns/out.sql
--rw-r--r--   0        0        0      146 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/aggregate_table_count_metric/decompiled.py
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/aggregate_table_count_metric/out.sql
--rw-r--r--   0        0        0      183 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/filter_then_limit/decompiled.py
--rw-r--r--   0        0        0       51 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/filter_then_limit/out.sql
--rw-r--r--   0        0        0      148 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/limit_simple/decompiled.py
--rw-r--r--   0        0        0       34 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/limit_simple/out.sql
--rw-r--r--   0        0        0      183 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/limit_then_filter/decompiled.py
--rw-r--r--   0        0        0       81 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/limit_then_filter/out.sql
--rw-r--r--   0        0        0      151 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/limit_with_offset/decompiled.py
--rw-r--r--   0        0        0       43 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/limit_with_offset/out.sql
--rw-r--r--   0        0        0      195 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/mixed_columns_ascending/decompiled.py
--rw-r--r--   0        0        0       58 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/mixed_columns_ascending/out.sql
--rw-r--r--   0        0        0      145 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/self_reference_simple/decompiled.py
--rw-r--r--   0        0        0       25 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/self_reference_simple/out.sql
--rw-r--r--   0        0        0      177 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/single_column/decompiled.py
--rw-r--r--   0        0        0       45 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/single_column/out.sql
--rw-r--r--   0        0        0      328 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/test_physical_table_reference_translate/decompiled.py
--rw-r--r--   0        0        0       28 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_select_sql/test_physical_table_reference_translate/out.sql
--rw-r--r--   0        0        0      233 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_self_join_subquery_distinct_equal/out.sql
--rw-r--r--   0        0        0      324 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_semi_join/decompiled.py
--rw-r--r--   0        0        0       84 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_semi_join/out.sql
--rw-r--r--   0        0        0      363 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_simple_joins/decompiled.py
--rw-r--r--   0        0        0       80 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_simple_joins/inner.sql
--rw-r--r--   0        0        0      121 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_simple_joins/inner_two_preds.sql
--rw-r--r--   0        0        0       85 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_simple_joins/left.sql
--rw-r--r--   0        0        0       85 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_simple_joins/outer.sql
--rw-r--r--   0        0        0      148 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_sort_then_group_by_propagates_keys/result1.sql
--rw-r--r--   0        0        0      148 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_sort_then_group_by_propagates_keys/result2.sql
--rw-r--r--   0        0        0      219 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_startswith/decompiled.py
--rw-r--r--   0        0        0       65 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_startswith/out.sql
--rw-r--r--   0        0        0      530 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_subquery_factor_correlated_subquery/out.sql
--rw-r--r--   0        0        0       94 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_subquery_in_filter_predicate/expr.sql
--rw-r--r--   0        0        0      122 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_subquery_in_filter_predicate/expr2.sql
--rw-r--r--   0        0        0      620 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_subquery_in_union/decompiled.py
--rw-r--r--   0        0        0      531 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_subquery_in_union/out.sql
--rw-r--r--   0        0        0      394 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_subquery_used_for_self_join/out.sql
--rw-r--r--   0        0        0      428 2023-08-06 00:03:22.251467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_topk_analysis_bug/out.sql
--rw-r--r--   0        0        0      234 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_topk_operation/e1.sql
--rw-r--r--   0        0        0      244 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_topk_operation/e2.sql
--rw-r--r--   0        0        0      476 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_topk_predicate_pushdown_bug/out.sql
--rw-r--r--   0        0        0      157 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_topk_to_aggregate/out.sql
--rw-r--r--   0        0        0      698 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_tpch_self_join_failure/out.sql
--rw-r--r--   0        0        0      752 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_where_analyze_scalar_op/decompiled.py
--rw-r--r--   0        0        0      243 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_where_analyze_scalar_op/out.sql
--rw-r--r--   0        0        0      406 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_where_no_pushdown_possible/decompiled.py
--rw-r--r--   0        0        0      164 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_where_no_pushdown_possible/out.sql
--rw-r--r--   0        0        0      440 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_where_with_between/decompiled.py
--rw-r--r--   0        0        0       82 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_where_with_between/out.sql
--rw-r--r--   0        0        0      416 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_where_with_join/decompiled.py
--rw-r--r--   0        0        0      156 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_select_sql/test_where_with_join/out.sql
--rw-r--r--   0        0        0       94 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_aggregate/having_count/out.sql
--rw-r--r--   0        0        0       94 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_aggregate/having_sum/out.sql
--rw-r--r--   0        0        0       70 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_aggregate/single/out.sql
--rw-r--r--   0        0        0       88 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_aggregate/two/out.sql
--rw-r--r--   0        0        0       77 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_between/out.sql
--rw-r--r--   0        0        0       86 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_boolean_conjunction/and/out.sql
--rw-r--r--   0        0        0       85 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_boolean_conjunction/or/out.sql
--rw-r--r--   0        0        0      168 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_coalesce/out.sql
--rw-r--r--   0        0        0       64 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_comparisons/eq/out.sql
--rw-r--r--   0        0        0       65 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_comparisons/ge/out.sql
--rw-r--r--   0        0        0       64 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_comparisons/gt/out.sql
--rw-r--r--   0        0        0       65 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_comparisons/le/out.sql
--rw-r--r--   0        0        0       64 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_comparisons/lt/out.sql
--rw-r--r--   0        0        0       65 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_comparisons/ne/out.sql
--rw-r--r--   0        0        0      167 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_cte_factor_distinct_but_equal/out.sql
--rw-r--r--   0        0        0       77 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_distinct/count_distinct/out.sql
--rw-r--r--   0        0        0      107 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_distinct/group_by_count_distinct/out.sql
--rw-r--r--   0        0        0       76 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_distinct/projection_distinct/out.sql
--rw-r--r--   0        0        0       62 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_distinct/single_column_projection_distinct/out.sql
--rw-r--r--   0        0        0      252 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_distinct/table_distinct/out.sql
--rw-r--r--   0        0        0      161 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_exists/e1.sql
--rw-r--r--   0        0        0      181 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_exists/e2.sql
--rw-r--r--   0        0        0      183 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_filter_group_by_agg_with_same_name/out.sql
--rw-r--r--   0        0        0      631 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_gh_1045/out.sql
--rw-r--r--   0        0        0       68 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_isnull_notnull/isnull/out.sql
--rw-r--r--   0        0        0       72 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_isnull_notnull/notnull/out.sql
--rw-r--r--   0        0        0      390 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_join_just_materialized/out.sql
--rw-r--r--   0        0        0      200 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_joins/inner/out.sql
--rw-r--r--   0        0        0      153 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_joins/inner_select/out.sql
--rw-r--r--   0        0        0      211 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_joins/left/out.sql
--rw-r--r--   0        0        0      164 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_joins/left_select/out.sql
--rw-r--r--   0        0        0      211 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_joins/outer/out.sql
--rw-r--r--   0        0        0      164 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_joins/outer_select/out.sql
--rw-r--r--   0        0        0       73 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_limit/expr_fn0/out.sql
--rw-r--r--   0        0        0       82 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_limit/expr_fn1/out.sql
--rw-r--r--   0        0        0       90 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_limit_filter/out.sql
--rw-r--r--   0        0        0      197 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_limit_subquery/out.sql
--rw-r--r--   0        0        0      489 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_lower_projection_sort_key/decompiled.py
--rw-r--r--   0        0        0      459 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_lower_projection_sort_key/out.sql
--rw-r--r--   0        0        0      252 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_multi_join/out.sql
--rw-r--r--   0        0        0      173 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_mutate_filter_join_no_cross_join/out.sql
--rw-r--r--   0        0        0       64 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_named_expr/out.sql
--rw-r--r--   0        0        0       65 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_negate/out.sql
--rw-r--r--   0        0        0      588 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_no_cart_join/out.sql
--rw-r--r--   0        0        0      237 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_no_cross_join/out.sql
--rw-r--r--   0        0        0      187 2023-08-06 00:03:22.255467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_not_exists/out.sql
--rw-r--r--   0        0        0       92 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_order_by/column/out.sql
--rw-r--r--   0        0        0       96 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_order_by/random/out.sql
--rw-r--r--   0        0        0       85 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_order_by_deprecated/ascending/out.sql
--rw-r--r--   0        0        0      105 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_order_by_deprecated/mixed/out.sql
--rw-r--r--   0        0        0      151 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_order_by_expr/out.sql
--rw-r--r--   0        0        0      176 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_searched_case/out.sql
--rw-r--r--   0        0        0      399 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_self_reference_in_not_exists/anti.sql
--rw-r--r--   0        0        0      373 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_self_reference_in_not_exists/semi.sql
--rw-r--r--   0        0        0      108 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_self_reference_join/out.sql
--rw-r--r--   0        0        0      108 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_simple_case/out.sql
--rw-r--r--   0        0        0      192 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_sort_aggregation_translation_failure/out.sql
--rw-r--r--   0        0        0      203 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_subquery_aliased/out.sql
--rw-r--r--   0        0        0      840 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_tpc_h11/out.sql
--rw-r--r--   0        0        0      344 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_tpc_h17/out.sql
--rw-r--r--   0        0        0      183 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_where_correlated_subquery/out.sql
--rw-r--r--   0        0        0      542 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_where_correlated_subquery_with_join/out.sql
--rw-r--r--   0        0        0      200 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_where_simple_comparisons/decompiled.py
--rw-r--r--   0        0        0      101 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_where_simple_comparisons/out.sql
--rw-r--r--   0        0        0      130 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/snapshots/test_sqlalchemy/test_where_uncorrelated_subquery/out.sql
--rw-r--r--   0        0        0     2181 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/test_ast_builder.py
--rw-r--r--   0        0        0     7623 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/test_compiler.py
--rw-r--r--   0        0        0    26172 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/test_select_sql.py
--rw-r--r--   0        0        0    18843 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/sql/test_sqlalchemy.py
--rw-r--r--   0        0        0     5528 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/strategies.py
--rw-r--r--   0        0        0     1865 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/test_api.py
--rw-r--r--   0        0        0      343 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/test_config.py
--rw-r--r--   0        0        0     4234 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/test_strategies.py
--rw-r--r--   0        0        0     1487 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/test_util.py
--rw-r--r--   0        0        0      226 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/test_version.py
--rw-r--r--   0        0        0     1256 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/tests/util.py
--rw-r--r--   0        0        0    12831 2023-08-06 00:03:22.259467 ibis_framework-6.1.1.dev22/ibis/util.py
--rw-r--r--   0        0        0    16848 2023-08-06 00:03:51.899498 ibis_framework-6.1.1.dev22/pyproject.toml
--rw-r--r--   0        0        0    91438 1970-01-01 00:00:00.000000 ibis_framework-6.1.1.dev22/setup.py
--rw-r--r--   0        0        0    16927 1970-01-01 00:00:00.000000 ibis_framework-6.1.1.dev22/PKG-INFO
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/docs/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      925 2015-07-24 17:45:25.000000 ibis-framework-v0.6.0/docs/build-notebooks.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/docs/sphinxext/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-07-24 17:45:25.000000 ibis-framework-v0.6.0/docs/sphinxext/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/docs/sphinxext/ipython_sphinxext/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-10-07 03:41:44.000000 ibis-framework-v0.6.0/docs/sphinxext/ipython_sphinxext/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      116 2015-10-07 03:41:44.000000 ibis-framework-v0.6.0/docs/sphinxext/ipython_sphinxext/LICENSE
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    37645 2015-10-07 03:41:44.000000 ibis-framework-v0.6.0/docs/sphinxext/ipython_sphinxext/ipython_directive.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     4181 2015-10-07 03:41:44.000000 ibis-framework-v0.6.0/docs/sphinxext/ipython_sphinxext/ipython_console_highlighting.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)       74 2015-06-10 22:32:58.000000 ibis-framework-v0.6.0/docs/README
+-rw-r--r--   0 wesm      (1000) wesm      (1000)       26 2015-06-10 22:32:58.000000 ibis-framework-v0.6.0/docs/requirements-docs.txt
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/docs/source/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     8595 2015-10-07 03:41:44.000000 ibis-framework-v0.6.0/docs/source/conf.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      236 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/docs/source/tutorial.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    23073 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/docs/source/impala.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1722 2015-07-24 17:45:25.000000 ibis-framework-v0.6.0/docs/source/developer.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     2578 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/docs/source/index.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3190 2015-09-02 07:50:13.000000 ibis-framework-v0.6.0/docs/source/configuration.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    10447 2015-11-30 09:40:30.000000 ibis-framework-v0.6.0/docs/source/release.rst
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/docs/source/_templates/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      487 2015-07-24 17:45:25.000000 ibis-framework-v0.6.0/docs/source/_templates/layout.html
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    33258 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/docs/source/sql.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    11507 2015-07-24 17:45:25.000000 ibis-framework-v0.6.0/docs/source/legal.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      102 2015-07-24 17:45:25.000000 ibis-framework-v0.6.0/docs/source/type-system.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     4044 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/docs/source/getting-started.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     8770 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/docs/source/api.rst
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     6464 2015-06-10 22:32:58.000000 ibis-framework-v0.6.0/docs/make.bat
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     7377 2015-07-24 17:45:25.000000 ibis-framework-v0.6.0/docs/Makefile
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1750 2015-11-05 21:11:43.000000 ibis-framework-v0.6.0/README.md
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis_framework.egg-info/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        5 2015-11-30 09:48:45.000000 ibis-framework-v0.6.0/ibis_framework.egg-info/top_level.txt
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      841 2015-11-30 09:48:45.000000 ibis-framework-v0.6.0/ibis_framework.egg-info/PKG-INFO
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        1 2015-11-30 09:48:45.000000 ibis-framework-v0.6.0/ibis_framework.egg-info/dependency_links.txt
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     4409 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis_framework.egg-info/SOURCES.txt
+-rw-r--r--   0 wesm      (1000) wesm      (1000)       46 2015-11-30 09:48:45.000000 ibis-framework-v0.6.0/ibis_framework.egg-info/pbr.json
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      119 2015-11-30 09:48:45.000000 ibis-framework-v0.6.0/ibis_framework.egg-info/requires.txt
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    62474 2015-10-08 04:18:58.000000 ibis-framework-v0.6.0/versioneer.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1383 2015-08-25 05:04:44.000000 ibis-framework-v0.6.0/ibis/config_init.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3771 2015-10-08 04:18:58.000000 ibis-framework-v0.6.0/ibis/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     2416 2015-05-11 04:48:02.000000 ibis-framework-v0.6.0/ibis/wire.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     7815 2015-08-25 05:04:44.000000 ibis-framework-v0.6.0/ibis/tasks.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    32671 2015-01-21 05:11:59.000000 ibis-framework-v0.6.0/ibis/comms.pyx
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    31387 2015-08-24 21:56:32.000000 ibis-framework-v0.6.0/ibis/cloudpickle.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    20779 2015-07-13 05:54:08.000000 ibis-framework-v0.6.0/ibis/config.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    13243 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/client.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/spark/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/spark/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/spark/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/spark/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1619 2015-11-01 14:03:25.000000 ibis-framework-v0.6.0/ibis/compat.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/src/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     2239 2015-01-24 22:13:49.000000 ibis-framework-v0.6.0/ibis/src/ipc_support.c
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1445 2015-01-21 05:11:59.000000 ibis-framework-v0.6.0/ibis/src/ipc_support.h
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/hive/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/hive/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/hive/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/hive/tests/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-05-11 04:48:04.000000 ibis-framework-v0.6.0/ibis/sql/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    23190 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/sql/alchemy.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/redshift/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/redshift/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/redshift/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/redshift/tests/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     2768 2015-10-08 04:27:31.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/client.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1400 2015-10-08 04:27:31.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/api.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     4806 2015-10-08 03:41:51.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/compiler.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     9911 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/tests/test_functions.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      616 2015-09-01 04:13:53.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/tests/conftest.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1970 2015-09-02 00:57:42.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/tests/common.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3047 2015-10-08 04:27:31.000000 ibis-framework-v0.6.0/ibis/sql/sqlite/tests/test_client.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3701 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/sql/transforms.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/presto/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/presto/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/presto/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/presto/tests/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/postgres/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-09-01 04:13:53.000000 ibis-framework-v0.6.0/ibis/sql/postgres/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/postgres/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-09-01 04:13:53.000000 ibis-framework-v0.6.0/ibis/sql/postgres/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      616 2015-09-01 04:13:53.000000 ibis-framework-v0.6.0/ibis/sql/postgres/tests/conftest.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/vertica/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/vertica/__init__.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/vertica/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/sql/vertica/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    43932 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/sql/compiler.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/sql/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-05-11 04:48:05.000000 ibis-framework-v0.6.0/ibis/sql/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      616 2015-09-01 04:13:53.000000 ibis-framework-v0.6.0/ibis/sql/tests/conftest.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    59525 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/sql/tests/test_compiler.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    18028 2015-09-10 15:51:22.000000 ibis-framework-v0.6.0/ibis/sql/tests/test_sqlalchemy.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     5330 2015-11-22 23:42:20.000000 ibis-framework-v0.6.0/ibis/util.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      472 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/_version.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/impala/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/impala/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     9517 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/impala/udf.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     2672 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/impala/identifiers.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     8436 2015-11-22 23:42:20.000000 ibis-framework-v0.6.0/ibis/impala/metadata.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    62350 2015-11-30 09:40:30.000000 ibis-framework-v0.6.0/ibis/impala/client.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3428 2015-08-20 06:26:34.000000 ibis-framework-v0.6.0/ibis/impala/madlib.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      728 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/impala/compat.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      649 2015-11-22 23:42:20.000000 ibis-framework-v0.6.0/ibis/impala/parquet.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3531 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/ibis/impala/api.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    24037 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/ibis/impala/ddl.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    37724 2015-11-29 21:58:21.000000 ibis-framework-v0.6.0/ibis/impala/compiler.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/impala/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/impala/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      616 2015-08-14 15:53:00.000000 ibis-framework-v0.6.0/ibis/impala/tests/conftest.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     8090 2015-09-03 06:45:25.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_window.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    47622 2015-11-29 21:58:21.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_exprs.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1100 2015-09-01 04:13:53.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_sql.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    36166 2015-11-30 09:40:30.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_ddl.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     4536 2015-11-22 23:42:20.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_metadata.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    18191 2015-08-27 14:46:57.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_udf.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1787 2015-09-09 04:43:12.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_madlib.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     6556 2015-11-22 23:43:12.000000 ibis-framework-v0.6.0/ibis/impala/tests/common.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     9610 2015-11-22 23:42:20.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_pandas_interop.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    10737 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_client.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     8670 2015-11-30 09:43:53.000000 ibis-framework-v0.6.0/ibis/impala/tests/test_partition.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     6233 2015-11-22 23:42:20.000000 ibis-framework-v0.6.0/ibis/impala/pandas_interop.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     9745 2015-08-24 21:56:32.000000 ibis-framework-v0.6.0/ibis/server.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     4129 2015-01-21 05:11:59.000000 ibis-framework-v0.6.0/ibis/comms.pxd
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/expr/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-05-11 04:48:06.000000 ibis-framework-v0.6.0/ibis/expr/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     8784 2015-09-16 18:48:19.000000 ibis-framework-v0.6.0/ibis/expr/format.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     7295 2015-06-29 06:28:49.000000 ibis-framework-v0.6.0/ibis/expr/temporal.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    26447 2015-09-16 18:48:19.000000 ibis-framework-v0.6.0/ibis/expr/analysis.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    11262 2015-11-05 19:34:31.000000 ibis-framework-v0.6.0/ibis/expr/datatypes.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    27030 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/expr/types.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    51662 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/expr/operations.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     7032 2015-09-09 04:43:12.000000 ibis-framework-v0.6.0/ibis/expr/groupby.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     7688 2015-09-03 06:45:25.000000 ibis-framework-v0.6.0/ibis/expr/window.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    49858 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/ibis/expr/api.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    19387 2015-08-24 21:56:32.000000 ibis-framework-v0.6.0/ibis/expr/rules.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     5516 2015-08-11 20:25:43.000000 ibis-framework-v0.6.0/ibis/expr/analytics.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/expr/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)        0 2015-05-11 04:48:08.000000 ibis-framework-v0.6.0/ibis/expr/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      616 2015-09-01 04:13:53.000000 ibis-framework-v0.6.0/ibis/expr/tests/conftest.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     2858 2015-09-02 23:21:36.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_interactive.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3386 2015-08-05 16:09:03.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_case.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     5034 2015-09-03 06:45:25.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_window_functions.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     6266 2015-09-16 18:48:19.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_format.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     9216 2015-09-16 18:48:19.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_analysis.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3313 2015-06-29 06:28:49.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_decimal.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     6270 2015-10-08 04:09:39.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_sql_builtins.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    20407 2015-11-30 09:38:55.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_value_exprs.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     5955 2015-08-24 21:56:32.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_temporal.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    13673 2015-09-09 04:43:12.000000 ibis-framework-v0.6.0/ibis/expr/tests/mocks.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3282 2015-10-07 04:53:40.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_string.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1850 2015-06-29 06:28:49.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_pipe.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3299 2015-08-11 20:30:38.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_analytics.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     3624 2015-10-08 05:36:22.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_timestamp.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    38376 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/expr/tests/test_table.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      997 2015-07-13 05:54:08.000000 ibis-framework-v0.6.0/ibis/common.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/ibis/tests/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      573 2015-06-29 06:28:49.000000 ibis-framework-v0.6.0/ibis/tests/__init__.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     2688 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/ibis/tests/conftest.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    11505 2015-06-29 06:28:49.000000 ibis-framework-v0.6.0/ibis/tests/test_comms.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    10953 2015-08-24 21:56:32.000000 ibis-framework-v0.6.0/ibis/tests/test_tasks.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     6059 2015-11-22 23:42:20.000000 ibis-framework-v0.6.0/ibis/tests/test_server.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      956 2015-08-25 05:04:44.000000 ibis-framework-v0.6.0/ibis/tests/util.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    16773 2015-09-02 07:50:13.000000 ibis-framework-v0.6.0/ibis/tests/test_filesystems.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     9550 2015-08-25 05:04:44.000000 ibis-framework-v0.6.0/ibis/filesystems.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      841 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/PKG-INFO
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/scripts/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      753 2015-01-21 05:11:59.000000 ibis-framework-v0.6.0/scripts/semaphore_perf.py
+-rwxr-xr-x   0 wesm      (1000) wesm      (1000)     4139 2015-10-08 03:41:51.000000 ibis-framework-v0.6.0/scripts/run_jenkins.sh
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1268 2015-09-10 05:09:43.000000 ibis-framework-v0.6.0/scripts/airline.py
+-rwxr-xr-x   0 wesm      (1000) wesm      (1000)    17250 2015-09-10 05:09:43.000000 ibis-framework-v0.6.0/scripts/test_data_admin.py
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      212 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/setup.cfg
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/conda-recipes/
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/conda-recipes/hdfs/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      776 2015-09-11 17:09:40.000000 ibis-framework-v0.6.0/conda-recipes/hdfs/meta.yaml
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      219 2015-08-07 16:53:33.000000 ibis-framework-v0.6.0/conda-recipes/hdfs/build.sh
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      236 2015-08-07 16:53:33.000000 ibis-framework-v0.6.0/conda-recipes/hdfs/bld.bat
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/conda-recipes/impyla/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1117 2015-11-01 14:03:25.000000 ibis-framework-v0.6.0/conda-recipes/impyla/meta.yaml
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      219 2015-08-07 16:53:33.000000 ibis-framework-v0.6.0/conda-recipes/impyla/build.sh
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      236 2015-08-07 16:53:33.000000 ibis-framework-v0.6.0/conda-recipes/impyla/bld.bat
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/conda-recipes/ibis-framework/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      971 2015-11-10 22:41:53.000000 ibis-framework-v0.6.0/conda-recipes/ibis-framework/meta.yaml
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      294 2015-11-01 14:03:25.000000 ibis-framework-v0.6.0/conda-recipes/ibis-framework/build.sh
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      314 2015-11-01 14:03:25.000000 ibis-framework-v0.6.0/conda-recipes/ibis-framework/bld.bat
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/conda-recipes/thrift/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      498 2015-08-07 16:53:33.000000 ibis-framework-v0.6.0/conda-recipes/thrift/meta.yaml
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      219 2015-08-07 16:53:33.000000 ibis-framework-v0.6.0/conda-recipes/thrift/build.sh
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      236 2015-08-07 16:53:33.000000 ibis-framework-v0.6.0/conda-recipes/thrift/bld.bat
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/conda-recipes/thrift_sasl/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      554 2015-09-11 17:09:40.000000 ibis-framework-v0.6.0/conda-recipes/thrift_sasl/meta.yaml
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      219 2015-09-11 17:09:40.000000 ibis-framework-v0.6.0/conda-recipes/thrift_sasl/build.sh
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      236 2015-09-11 17:09:40.000000 ibis-framework-v0.6.0/conda-recipes/thrift_sasl/bld.bat
+-rw-r--r--   0 wesm      (1000) wesm      (1000)      561 2015-10-08 04:18:58.000000 ibis-framework-v0.6.0/MANIFEST.in
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     4200 2015-11-01 14:02:52.000000 ibis-framework-v0.6.0/setup.py
+drwxr-xr-x   0 wesm      (1000) wesm      (1000)        0 2015-11-30 09:48:46.000000 ibis-framework-v0.6.0/LICENSES/
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1061 2015-06-11 06:57:54.000000 ibis-framework-v0.6.0/LICENSES/hdfscli.txt
+-rw-r--r--   0 wesm      (1000) wesm      (1000)     1838 2015-06-04 20:35:47.000000 ibis-framework-v0.6.0/LICENSES/pandas.txt
+-rw-r--r--   0 wesm      (1000) wesm      (1000)       84 2015-11-10 22:41:53.000000 ibis-framework-v0.6.0/requirements.txt
+-rw-r--r--   0 wesm      (1000) wesm      (1000)    11358 2015-03-11 21:51:00.000000 ibis-framework-v0.6.0/LICENSE.txt
```

### filetype from file(1)

```diff
@@ -1 +1 @@
-POSIX tar archive
+POSIX tar archive (GNU)
```

### Comparing `ibis_framework-6.1.1.dev22/LICENSE.txt` & `ibis-framework-v0.6.0/LICENSE.txt`

 * *Files identical despite different names*

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/base/sql/__init__.py` & `ibis-framework-v0.6.0/ibis/client.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,365 +1,471 @@
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-import abc
-import contextlib
-import os
-from functools import lru_cache
-from typing import TYPE_CHECKING, Any, Iterable, Mapping
+from ibis.compat import zip as czip
+from ibis.config import options
+import ibis.expr.types as ir
+import ibis.expr.operations as ops
+import ibis.sql.compiler as comp
+import ibis.common as com
+import ibis.util as util
 
-import toolz
 
-import ibis.common.exceptions as exc
-import ibis.expr.analysis as an
-import ibis.expr.operations as ops
-import ibis.expr.schema as sch
-import ibis.expr.types as ir
-from ibis import util
-from ibis.backends.base import BaseBackend
-from ibis.backends.base.sql.compiler import Compiler
+class Client(object):
 
-if TYPE_CHECKING:
-    import pandas as pd
-    import pyarrow as pa
+    pass
 
-__all__ = ['BaseSQLBackend']
 
+class Query(object):
 
-class BaseSQLBackend(BaseBackend):
-    """Base backend class for backends that compile to SQL."""
+    """
+    Abstraction for DDL query execution to enable both synchronous and
+    asynchronous queries, progress, cancellation and more (for backends
+    supporting such functionality).
+    """
 
-    compiler = Compiler
+    def __init__(self, client, ddl):
+        self.client = client
 
-    @property
-    def _sqlglot_dialect(self) -> str:
-        return self.name
+        if isinstance(ddl, comp.DDL):
+            self.compiled_ddl = ddl.compile()
+        else:
+            self.compiled_ddl = ddl
+
+        self.result_wrapper = getattr(ddl, 'result_handler', None)
+
+    def execute(self):
+        # synchronous by default
+        with self.client._execute(self.compiled_ddl, results=True) as cur:
+            result = self._fetch(cur)
+
+        return self._wrap_result(result)
+
+    def _wrap_result(self, result):
+        if self.result_wrapper is not None:
+            result = self.result_wrapper(result)
+        return result
+
+    def _fetch(self, cursor):
+        import pandas as pd
+        rows = cursor.fetchall()
+        # TODO(wesm): please evaluate/reimpl to optimize for perf/memory
+        dtypes = [self._db_type_to_dtype(x[1]) for x in cursor.description]
+        names = [x[0] for x in cursor.description]
+        cols = {}
+        for (col, name, dtype) in czip(czip(*rows), names, dtypes):
+            try:
+                cols[name] = pd.Series(col, dtype=dtype)
+            except TypeError:
+                # coercing to specified dtype failed, e.g. NULL vals in int col
+                cols[name] = pd.Series(col)
+        return pd.DataFrame(cols, columns=names)
+
+    def _db_type_to_dtype(self, db_type):
+        raise NotImplementedError
+
+
+class AsyncQuery(Query):
+
+    """
+    Abstract asynchronous query
+    """
 
-    def _from_url(self, url: str, **kwargs: Any) -> BaseBackend:
-        """Connect to a backend using a URL `url`.
+    def execute(self):
+        raise NotImplementedError
+
+    def is_finished(self):
+        raise NotImplementedError
+
+    def cancel(self):
+        raise NotImplementedError
+
+    def get_result(self):
+        raise NotImplementedError
+
+
+class SQLClient(Client):
+
+    sync_query = Query
+    async_query = Query
+
+    def table(self, name, database=None):
+        """
+        Create a table expression that references a particular table in the
+        database
 
         Parameters
         ----------
-        url
-            URL with which to connect to a backend.
-        kwargs
-            Additional keyword arguments passed to the `connect` method.
+        name : string
+        database : string, optional
 
         Returns
         -------
-        BaseBackend
-            A backend instance
+        table : TableExpr
         """
-        import sqlalchemy as sa
+        qualified_name = self._fully_qualified_name(name, database)
+        schema = self._get_table_schema(qualified_name)
+        node = ops.DatabaseTable(qualified_name, schema, self)
+        return self._table_expr_klass(node)
 
-        url = sa.engine.make_url(url)
-        new_kwargs = kwargs.copy()
-        kwargs = {}
-
-        for name in ("host", "port", "database", "password"):
-            if value := (
-                getattr(url, name, None)
-                or os.environ.get(f"{self.name.upper()}_{name.upper()}")
-            ):
-                kwargs[name] = value
-        if username := url.username:
-            kwargs["user"] = username
-
-        kwargs.update(url.query)
-        new_kwargs = toolz.merge(kwargs, new_kwargs)
-        self._convert_kwargs(new_kwargs)
-        return self.connect(**new_kwargs)
+    @property
+    def _table_expr_klass(self):
+        return ir.TableExpr
 
-    def table(self, name: str, database: str | None = None) -> ir.Table:
-        """Construct a table expression.
+    @property
+    def current_database(self):
+        return self.con.database
+
+    def database(self, name=None):
+        """
+        Create a Database object for a given database name that can be used for
+        exploring and manipulating the objects (tables, functions, views, etc.)
+        inside
 
         Parameters
         ----------
-        name
-            Table name
-        database
-            Database name
+        name : string
+          Name of database
 
         Returns
         -------
-        Table
-            Table expression
+        database : Database
         """
-        if database is not None and not isinstance(database, str):
-            raise exc.IbisTypeError(
-                f"`database` must be a string; got {type(database)}"
-            )
-        qualified_name = self._fully_qualified_name(name, database)
-        schema = self.get_schema(qualified_name)
-        node = ops.DatabaseTable(name, schema, self, namespace=database)
-        return node.to_expr()
+        # TODO: validate existence of database
+        if name is None:
+            name = self.current_database
+        return self.database_class(name, self)
 
     def _fully_qualified_name(self, name, database):
         # XXX
         return name
 
-    def sql(
-        self, query: str, schema: sch.Schema | None = None, dialect: str | None = None
-    ) -> ir.Table:
-        """Convert a SQL query to an Ibis table expression.
+    def _execute(self, query, results=False):
+        cur = self.con.execute(query)
+        if results:
+            return cur
+        else:
+            cur.release()
+
+    def sql(self, query):
+        """
+        Convert a SQL query to an Ibis table expression
 
         Parameters
         ----------
-        query
-            SQL string
-        schema
-            The expected schema for this query. If not provided, will be
-            inferred automatically if possible.
-        dialect
-            Optional string indicating the dialect of `query`. The default
-            value of `None` will use the backend's native dialect.
 
         Returns
         -------
-        Table
-            Table expression
+        table : TableExpr
         """
-        query = self._transpile_sql(query, dialect=dialect)
-        if schema is None:
-            schema = self._get_schema_using_query(query)
-        else:
-            schema = sch.schema(schema)
-        return ops.SQLQueryResult(query, schema, self).to_expr()
-
-    def _get_schema_using_query(self, query):
-        raise NotImplementedError(f"Backend {self.name} does not support .sql()")
+        # Get the schema by adding a LIMIT 0 on to the end of the query. If
+        # there is already a limit in the query, we find and remove it
+        limited_query = """\
+SELECT *
+FROM (
+{0}
+) t0
+LIMIT 0""".format(query)
+        schema = self._get_schema_using_query(limited_query)
 
-    def raw_sql(self, query: str):
-        """Execute a query string.
+        node = ops.SQLQueryResult(query, schema, self)
+        return ir.TableExpr(node)
 
-        !!! warning "The returned cursor object must be **manually** released if results are returned."
-
-        Parameters
-        ----------
-        query
-            DDL or DML statement
+    def raw_sql(self, query, results=False):
         """
-        return self.con.execute(query)
-
-    @contextlib.contextmanager
-    def _safe_raw_sql(self, *args, **kwargs):
-        yield self.raw_sql(*args, **kwargs)
-
-    def _cursor_batches(
-        self,
-        expr: ir.Expr,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        chunk_size: int = 1_000_000,
-    ) -> Iterable[list]:
-        self._run_pre_execute_hooks(expr)
-        query_ast = self.compiler.to_ast_ensure_limit(expr, limit, params=params)
-        sql = query_ast.compile()
-
-        with self._safe_raw_sql(sql) as cursor:
-            while batch := cursor.fetchmany(chunk_size):
-                yield batch
-
-    @util.experimental
-    def to_pyarrow_batches(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        chunk_size: int = 1_000_000,
-        **_: Any,
-    ) -> pa.ipc.RecordBatchReader:
-        """Execute expression and return an iterator of pyarrow record batches.
-
-        This method is eager and will execute the associated expression
-        immediately.
+        Execute a given query string. Could have unexpected results if the
+        query modifies the behavior of the session in a way unknown to Ibis; be
+        careful.
 
         Parameters
         ----------
-        expr
-            Ibis expression to export to pyarrow
-        limit
-            An integer to effect a specific row limit. A value of `None` means
-            "no limit". The default is in `ibis/config.py`.
-        params
-            Mapping of scalar parameter expressions to value.
-        chunk_size
-            Maximum number of rows in each returned record batch.
+        query : string
+          SQL or DDL statement
+        results : boolean, default False
+          Pass True if the query as a result set
 
         Returns
         -------
-        RecordBatchReader
-            Collection of pyarrow `RecordBatch`s.
+        cur : ImpalaCursor if results=True, None otherwise
+          You must call cur.release() after you are finished using the cursor.
         """
-        pa = self._import_pyarrow()
-
-        schema = expr.as_table().schema()
-        array_type = schema.as_struct().to_pyarrow()
-        arrays = (
-            pa.array(map(tuple, batch), type=array_type)
-            for batch in self._cursor_batches(
-                expr, params=params, limit=limit, chunk_size=chunk_size
-            )
-        )
-        batches = map(pa.RecordBatch.from_struct_array, arrays)
-
-        return pa.ipc.RecordBatchReader.from_batches(schema.to_pyarrow(), batches)
-
-    def _register_udfs(self, expr: ir.Expr) -> None:
-        """Return an iterator of DDL strings, once for each UDFs contained within `expr`."""
-        if self.supports_python_udfs:
-            raise NotImplementedError(self.name)
-
-    def _define_udf_translation_rules(self, expr: ir.Expr) -> None:
-        if self.supports_python_udfs:
-            raise NotImplementedError(self.name)
-
-    def execute(
-        self,
-        expr: ir.Expr,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: str = 'default',
-        **kwargs: Any,
-    ):
-        """Compile and execute an Ibis expression.
+        return self._execute(query, results=results)
 
+    def execute(self, expr, params=None, limit='default', async=False):
+        """
         Compile and execute Ibis expression using this backend client
         interface, returning results in-memory in the appropriate object type
 
         Parameters
         ----------
-        expr
-            Ibis expression
-        limit
-            For expressions yielding result sets; retrieve at most this number
-            of values/rows. Overrides any limit already set on the expression.
-        params
-            Named unbound parameters
-        kwargs
-            Backend specific arguments. For example, the clickhouse backend
-            uses this to receive `external_tables` as a dictionary of pandas
-            DataFrames.
+        expr : Expr
+        limit : int, default None
+          For expressions yielding result yets; retrieve at most this number of
+          values/rows. Overrides any limit already set on the expression.
+        params : not yet implemented
+        async : boolean, default False
 
         Returns
         -------
-        DataFrame | Series | Scalar
-            * `Table`: pandas.DataFrame
-            * `Column`: pandas.Series
-            * `Scalar`: Python scalar value
-        """
-        # TODO Reconsider having `kwargs` here. It's needed to support
-        # `external_tables` in clickhouse, but better to deprecate that
-        # feature than all this magic.
-        # we don't want to pass `timecontext` to `raw_sql`
-        self._run_pre_execute_hooks(expr)
-
-        kwargs.pop('timecontext', None)
-        query_ast = self.compiler.to_ast_ensure_limit(expr, limit, params=params)
-        sql = query_ast.compile()
-        self._log(sql)
-
-        schema = expr.as_table().schema()
-
-        with self._safe_raw_sql(sql, **kwargs) as cursor:
-            result = self.fetch_from_cursor(cursor, schema)
-
-        return expr.__pandas_result__(result)
-
-    def _register_in_memory_table(self, _: ops.InMemoryTable) -> None:
-        raise NotImplementedError(self.name)
-
-    def _register_in_memory_tables(self, expr: ir.Expr) -> None:
-        if self.compiler.cheap_in_memory_tables:
-            for memtable in an.find_memtables(expr.op()):
-                self._register_in_memory_table(memtable)
-
-    @abc.abstractmethod
-    def fetch_from_cursor(self, cursor, schema):
-        """Fetch data from cursor."""
-
-    def _log(self, sql: str) -> None:
-        """Log the SQL, usually to the standard output.
-
-        This method can be implemented by subclasses. The logging
-        happens when `ibis.options.verbose` is `True`.
-        """
-        util.log(sql)
-
-    def compile(
-        self,
-        expr: ir.Expr,
-        limit: str | None = None,
-        params: Mapping[ir.Expr, Any] | None = None,
-        timecontext: tuple[pd.Timestamp, pd.Timestamp] | None = None,
-    ) -> Any:
-        """Compile an Ibis expression.
+        output : input type dependent
+          Table expressions: pandas.DataFrame
+          Array expressions: pandas.Series
+          Scalar expressions: Python scalar value
+        """
+        ast = self._build_ast_ensure_limit(expr, limit)
 
-        Parameters
-        ----------
-        expr
-            Ibis expression
-        limit
-            For expressions yielding result sets; retrieve at most this number
-            of values/rows. Overrides any limit already set on the expression.
-        params
-            Named unbound parameters
-        timecontext
-            Additional information about data source time boundaries
+        if len(ast.queries) > 1:
+            raise NotImplementedError
+        else:
+            return self._execute_query(ast.queries[0], async=async)
+
+    def _execute_query(self, ddl, async=False):
+        klass = self.async_query if async else self.sync_query
+        return klass(self, ddl).execute()
+
+    def compile(self, expr, params=None, limit=None):
+        """
+        Translate expression to one or more queries according to backend target
 
         Returns
         -------
-        Any
-            The output of compilation. The type of this value depends on the
-            backend.
-        """
-        self._define_udf_translation_rules(expr)
-        return self.compiler.to_ast_ensure_limit(expr, limit, params=params).compile()
-
-    def _to_sql(self, expr: ir.Expr, **kwargs) -> str:
-        return str(self.compile(expr, **kwargs))
-
-    def explain(
-        self,
-        expr: ir.Expr | str,
-        params: Mapping[ir.Expr, Any] | None = None,
-    ) -> str:
-        """Explain an expression.
+        output : single query or list of queries
+        """
+        ast = self._build_ast_ensure_limit(expr, limit)
+        queries = [query.compile() for query in ast.queries]
+        return queries[0] if len(queries) == 1 else queries
+
+    def _build_ast_ensure_limit(self, expr, limit):
+        ast = self._build_ast(expr)
+        # note: limit can still be None at this point, if the global
+        # default_limit is None
+        for query in reversed(ast.queries):
+            if (isinstance(query, comp.Select) and
+                    not isinstance(expr, ir.ScalarExpr) and
+                    query.table_set is not None):
+                if query.limit is None:
+                    if limit == 'default':
+                        query_limit = options.sql.default_limit
+                    else:
+                        query_limit = limit
+                    if query_limit:
+                        query.limit = {
+                            'n': query_limit,
+                            'offset': 0
+                        }
+                elif limit is not None and limit != 'default':
+                    query.limit = {'n': limit,
+                                   'offset': query.limit['offset']}
+        return ast
 
-        Return the query plan associated with the indicated expression or SQL
-        query.
+    def explain(self, expr):
+        """
+        Query for and return the query plan associated with the indicated
+        expression or SQL query.
 
         Returns
         -------
-        str
-            Query plan
+        plan : string
         """
         if isinstance(expr, ir.Expr):
-            context = self.compiler.make_context(params=params)
-            query_ast = self.compiler.to_ast(expr, context)
-            if len(query_ast.queries) > 1:
+            ast = self._build_ast(expr)
+            if len(ast.queries) > 1:
                 raise Exception('Multi-query expression')
 
-            query = query_ast.queries[0].compile()
+            query = ast.queries[0].compile()
         else:
             query = expr
 
-        statement = f'EXPLAIN {query}'
+        statement = 'EXPLAIN {0}'.format(query)
 
-        with self._safe_raw_sql(statement) as cur:
+        with self._execute(statement, results=True) as cur:
             result = self._get_list(cur)
 
-        return '\n'.join(['Query:', util.indent(query, 2), '', *result])
+        return 'Query:\n{0}\n\n{1}'.format(util.indent(query, 2),
+                                           '\n'.join(result))
+
+    def _build_ast(self, expr):
+        # Implement in clients
+        raise NotImplementedError
+
+
+class QueryPipeline(object):
+    """
+    Execute a series of queries, possibly asynchronously, and capture any
+    result sets generated
+
+    Note: No query pipelines have yet been implemented
+    """
+    pass
+
+
+def execute(expr, limit='default', async=False):
+    backend = find_backend(expr)
+    return backend.execute(expr, limit=limit, async=async)
+
+
+def compile(expr, limit=None):
+    backend = find_backend(expr)
+    return backend.compile(expr, limit=limit)
+
+
+def find_backend(expr):
+    backends = []
+
+    def walk(expr):
+        node = expr.op()
+        for arg in node.flat_args():
+            if isinstance(arg, Client):
+                backends.append(arg)
+            elif isinstance(arg, ir.Expr):
+                walk(arg)
+
+    walk(expr)
+    backends = util.unique_by_key(backends, id)
+
+    if len(backends) > 1:
+        raise ValueError('Multiple backends found')
+    elif len(backends) == 0:
+        default = options.default_backend
+        if default is None:
+            raise com.IbisError('Expression depends on no backends, '
+                                'and found no default')
+        return default
+
+    return backends[0]
+
+
+class Database(object):
+
+    def __init__(self, name, client):
+        self.name = name
+        self.client = client
+
+    def __repr__(self):
+        return "{0}('{1}')".format('Database', self.name)
+
+    def __dir__(self):
+        attrs = dir(type(self))
+        unqualified_tables = [self._unqualify(x) for x in self.tables]
+        return list(sorted(set(attrs + unqualified_tables)))
+
+    def __contains__(self, key):
+        return key in self.tables
+
+    @property
+    def tables(self):
+        return self.list_tables()
+
+    def __getitem__(self, key):
+        return self.table(key)
+
+    def __getattr__(self, key):
+        special_attrs = ['_ipython_display_', 'trait_names',
+                         '_getAttributeNames']
+
+        try:
+            return object.__getattribute__(self, key)
+        except AttributeError:
+            if key in special_attrs:
+                raise
+            return self.table(key)
+
+    def _qualify(self, value):
+        return value
+
+    def _unqualify(self, value):
+        return value
+
+    def drop(self, force=False):
+        """
+        Drop the database
+
+        Parameters
+        ----------
+        drop : boolean, default False
+          Drop any objects if they exist, and do not fail if the databaes does
+          not exist
+        """
+        self.client.drop_database(self.name, force=force)
+
+    def namespace(self, ns):
+        """
+        Creates a derived Database instance for collections of objects having a
+        common prefix. For example, for tables fooa, foob, and fooc, creating
+        the "foo" namespace would enable you to reference those objects as a,
+        b, and c, respectively.
+
+        Returns
+        -------
+        ns : DatabaseNamespace
+        """
+        return DatabaseNamespace(self, ns)
+
+    def table(self, name):
+        """
+        Return a table expression referencing a table in this database
+
+        Returns
+        -------
+        table : TableExpr
+        """
+        qualified_name = self._qualify(name)
+        return self.client.table(qualified_name, self.name)
+
+    def list_tables(self, like=None):
+        return self.client.list_tables(like=self._qualify_like(like),
+                                       database=self.name)
+
+    def _qualify_like(self, like):
+        return like
+
+
+class DatabaseNamespace(Database):
+
+    def __init__(self, parent, namespace):
+        self.parent = parent
+        self.namespace = namespace
+
+    def __repr__(self):
+        return ("{0}(database={1!r}, namespace={2!r})"
+                .format('DatabaseNamespace', self.name, self.namespace))
+
+    @property
+    def client(self):
+        return self.parent.client
+
+    @property
+    def name(self):
+        return self.parent.name
+
+    def _qualify(self, value):
+        return self.namespace + value
+
+    def _unqualify(self, value):
+        return value.replace(self.namespace, '', 1)
+
+    def _qualify_like(self, like):
+        if like:
+            return self.namespace + like
+        else:
+            return '{0}*'.format(self.namespace)
+
+
+class DatabaseEntity(object):
+    pass
+
+
+class View(DatabaseEntity):
 
-    @classmethod
-    @lru_cache
-    def _get_operations(cls):
-        translator = cls.compiler.translator_class
-        return translator._registry.keys() | translator._rewrites.keys()
-
-    @classmethod
-    def has_operation(cls, operation: type[ops.Value]) -> bool:
-        return operation in cls._get_operations()
-
-    def _create_temp_view(self, view, definition):
-        raise NotImplementedError(
-            f"The {self.name} backend does not implement temporary view creation"
-        )
+    def drop(self):
+        pass
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/base/sql/alchemy/__init__.py` & `ibis-framework-v0.6.0/ibis/comms.pyx`

 * *Files 24% similar despite different names*

```diff
@@ -1,870 +1,1152 @@
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# This module handles IPC coordination (using system semaphores) between Ibis
+# workers and the Impala host, as well as low-level serialization and
+# deserialization concerns via shared memory (memory maps).
+
+# cython: embedsignature = True
+# cython: boundscheck = False
+# cython: wraparound = False
+
+from libc.errno cimport *
+from libc.stdlib cimport free, malloc, realloc
+from libc.string cimport memcpy, memcmp
+
+from comms cimport *
+
+from numpy cimport ndarray
+cimport numpy as cnp
+
+cimport cpython as cp
+from cython.operator cimport (dereference as deref,
+                              preincrement as preinc,
+                              predecrement as predec)
+cimport cython
+
+import numpy as np
+import os
+
+cnp.import_array()
+
+cdef double NaN = <double> np.NaN
+
+cdef fused signed_int:
+    int8_t
+    int16_t
+    int32_t
+    int64_t
+
+
+cdef class IPCLock:
+    """
+    Prototype class to deal with low-overhead control handoff between two
+    processes on the same machine using system semaphores. By using two
+    semaphores, we can implement a low-latency "my turn, your turn" approach
+    that should not be prone to race conditions. Local testing suggests a
+    handoff cycle could take less than 50 microseconds (in ideal system load
+    conditions).
+
+    Has a flag is_slave so that we can masquerade as the master (or slave) for
+    testing purposes. When creating a non-slave, the "other" process lock
+    starts off in unlocked state.
+    """
+
+    def __cinit__(self, semaphore_id=None, int lock_timeout_ms=1,
+                  bint is_slave=1):
+        self.timeout_ms = lock_timeout_ms
+        self.is_slave = is_slave
+
+        if not is_slave:
+            self.create()
+            self.release()
+        else:
+            # This must not be None
+            self.semaphore_id = semaphore_id
 
-import abc
-import atexit
-import contextlib
-import getpass
-import warnings
-from operator import methodcaller
-from typing import TYPE_CHECKING, Any, Iterable, Mapping
-
-import sqlalchemy as sa
-from sqlalchemy.ext.compiler import compiles
-from sqlalchemy.sql import quoted_name
-from sqlalchemy.sql.expression import ClauseElement, Executable
-
-import ibis
-import ibis.common.exceptions as com
-import ibis.expr.datatypes as dt
-import ibis.expr.operations as ops
-import ibis.expr.schema as sch
-import ibis.expr.types as ir
-from ibis import util
-from ibis.backends.base import CanCreateSchema
-from ibis.backends.base.sql import BaseSQLBackend
-from ibis.backends.base.sql.alchemy.geospatial import geospatial_supported
-from ibis.backends.base.sql.alchemy.query_builder import AlchemyCompiler
-from ibis.backends.base.sql.alchemy.registry import (
-    fixed_arity,
-    get_sqla_table,
-    reduction,
-    sqlalchemy_operation_registry,
-    sqlalchemy_window_functions_registry,
-    unary,
-    varargs,
-    variance_reduction,
-)
-from ibis.backends.base.sql.alchemy.translator import (
-    AlchemyContext,
-    AlchemyExprTranslator,
-)
-from ibis.formats.pandas import PandasData
-
-if TYPE_CHECKING:
-    import pandas as pd
-    import pyarrow as pa
-
-
-__all__ = (
-    'BaseAlchemyBackend',
-    'AlchemyExprTranslator',
-    'AlchemyContext',
-    'AlchemyCompiler',
-    'sqlalchemy_operation_registry',
-    'sqlalchemy_window_functions_registry',
-    'reduction',
-    'variance_reduction',
-    'fixed_arity',
-    'unary',
-    'infix_op',
-    'get_sqla_table',
-    'schema_from_table',
-    'varargs',
-)
-
-
-class CreateTableAs(Executable, ClauseElement):
-    inherit_cache = True
-
-    def __init__(
-        self,
-        name,
-        query,
-        temp: bool = False,
-        overwrite: bool = False,
-        quote: bool | None = None,
-    ):
-        self.name = name
-        self.query = query
-        self.temp = temp
-        self.overwrite = overwrite
-        self.quote = quote
-
-
-@compiles(CreateTableAs)
-def _create_table_as(element, compiler, **kw):
-    stmt = "CREATE "
-
-    if element.overwrite:
-        stmt += "OR REPLACE "
-
-    if element.temp:
-        stmt += "TEMPORARY "
-
-    name = compiler.preparer.quote(quoted_name(element.name, quote=element.quote))
-    return stmt + f"TABLE {name} AS {compiler.process(element.query, **kw)}"
-
-
-class AlchemyCanCreateSchema(CanCreateSchema):
-    def list_schemas(self, like: str | None = None) -> list[str]:
-        return self._filter_with_like(self.inspector.get_schema_names(), like)
-
-
-class BaseAlchemyBackend(BaseSQLBackend):
-    """Backend class for backends that compile to SQLAlchemy expressions."""
-
-    compiler = AlchemyCompiler
-    supports_temporary_tables = True
-    _temporary_prefix = "TEMPORARY"
-
-    def _scalar_query(self, query):
-        method = "exec_driver_sql" if isinstance(query, str) else "execute"
-        with self.begin() as con:
-            return getattr(con, method)(query).scalar()
-
-    def _compile_type(self, dtype) -> str:
-        dialect = self.con.dialect
-        return sa.types.to_instance(
-            self.compiler.translator_class.get_sqla_type(dtype)
-        ).compile(dialect=dialect)
-
-    def _build_alchemy_url(self, url, host, port, user, password, database, driver):
-        if url is not None:
-            return sa.engine.url.make_url(url)
-
-        user = user or getpass.getuser()
-        return sa.engine.url.URL.create(
-            driver,
-            host=host,
-            port=port,
-            username=user,
-            password=password,
-            database=database,
-        )
+    def __dealloc__(self):
+        # We need to deallocate the semaphore array if we created it
+        if not self.is_slave:
+            semarray_delete(self.semaphore_id)
+
+    cdef create(self):
+        # Initialize the semaphores and set in locked state.
+        cdef unsigned short init_vals[2]
+        init_vals[0] = 0
+        init_vals[1] = 0
+        self.semaphore_id = semarray_init(2, init_vals)
+
+    def __repr__(self):
+        # TODO: show lock state
+        return 'IPCLock(semaphore_id=%d)' % self.semaphore_id
+
+    def __enter__(self):
+        self.acquire()
+        return self
+
+    def __exit__(self, type, value, tb):
+        self.release()
 
-    @property
-    def _current_schema(self) -> str | None:
-        return None
-
-    def do_connect(self, con: sa.engine.Engine) -> None:
-        self.con = con
-        self._inspector = None
-        self._schemas: dict[str, sch.Schema] = {}
-        self._temp_views: set[str] = set()
-
-    @property
-    def version(self):
-        if self._inspector is None:
-            self._inspector = sa.inspect(self.con)
-        return '.'.join(map(str, self.con.dialect.server_version_info))
-
-    def list_tables(self, like=None, database=None):
-        tables = self.inspector.get_table_names(schema=database)
-        views = self.inspector.get_view_names(schema=database)
-        return self._filter_with_like(tables + views, like)
-
-    def list_databases(self, like=None):
-        """List databases in the current server."""
-        databases = self.inspector.get_schema_names()
-        return self._filter_with_like(databases, like)
-
-    @property
-    def inspector(self):
-        if self._inspector is None:
-            self._inspector = sa.inspect(self.con)
+    def acquire(self, bint block=1):
+        """
+        Returns True on success, False on failure with timeout. Otherwise
+        Exception is raised for other failures.
+        """
+        cdef:
+            timespec timeout
+            int ret
+
+        # Try to lock this process's semaphore
+        with nogil:
+            if block:
+                ret = semarray_lock(self.semaphore_id, self._our_sem_slot(),
+                                    NULL)
+            else:
+                self.set_timeout(&timeout)
+                ret = semarray_lock(self.semaphore_id,
+                                    self._our_sem_slot(), &timeout)
+
+        return self._check_semop_status(ret)
+
+    def release(self, bint block=1):
+        cdef:
+            timespec timeout
+            int ret
+
+        # Try to unlock the other process's semaphore
+        with nogil:
+            if block:
+                ret = semarray_unlock(self.semaphore_id,
+                                      self._their_sem_slot(), NULL)
+            else:
+                self.set_timeout(&timeout)
+                ret = semarray_unlock(self.semaphore_id,
+                                      self._their_sem_slot(), &timeout)
+
+        return self._check_semop_status(ret)
+
+    cdef _check_semop_status(self, int ret):
+        if ret == -1:
+            if errno == EAGAIN:
+                return False
+            raise Exception('errno was %d' % ret)
         else:
-            self._inspector.info_cache.clear()
-        return self._inspector
+            return True
 
-    def _to_sql(self, expr: ir.Expr, **kwargs) -> str:
-        # For `ibis.to_sql` calls we render with literal binds and qmark params
-        dialect_class = sa.dialects.registry.load(
-            self.compiler.translator_class._dialect_name
-        )
-        sql = self.compile(expr, **kwargs).compile(
-            dialect=dialect_class(paramstyle="qmark"),
-            compile_kwargs=dict(literal_binds=True),
-        )
-        return str(sql)
+    cdef inline int _our_sem_slot(self) nogil:
+        return 0 if self.is_slave else 1
+
+    cdef inline int _their_sem_slot(self) nogil:
+        return 1 if self.is_slave else 0
+
+    cdef void set_timeout(self, timespec* timeout) nogil:
+        timeout.tv_sec = 0
+        timeout.tv_nsec = self.timeout_ms * 1000000
+
+
+cdef class BufferLike:
+
+    def __len__(self):
+        return self.size
+
+    cdef uint8_t* get_buffer(self) nogil:
+        return NULL
+
+    def read(self, int nbytes=-1):
+        self._raise_if_closed()
+
+        if (nbytes < 0) or nbytes > (self.size - self.pos):
+            # Read no more than the remaining capacity of the buffer
+            nbytes = self.size - self.pos
+
+        result = cp.PyBytes_FromStringAndSize(<char*> self.get_buffer(),
+                                              nbytes)
+
+        self.pos += nbytes
+
+        return result
+
+    def write(self, object s):
+        """
+        Write UTF8 encoded bytes to the memory map
+        """
+        self._raise_if_closed()
+
+        if not isinstance(s, bytes):
+            s = s.encode('utf-8')
 
-    @contextlib.contextmanager
-    def _safe_raw_sql(self, *args, **kwargs):
-        with self.begin() as con:
-            yield con.execute(*args, **kwargs)
-
-    # TODO(kszucs): move to ibis.formats.pandas
-    @staticmethod
-    def _to_geodataframe(df, schema):
-        """Convert `df` to a `GeoDataFrame`.
-
-        Required libraries for geospatial support must be installed and
-        a geospatial column is present in the dataframe.
-        """
-        import geopandas as gpd
-        from geoalchemy2 import shape
-
-        geom_col = None
-        for name, dtype in schema.items():
-            if dtype.is_geospatial():
-                if not geom_col:
-                    geom_col = name
-                df[name] = df[name].map(shape.to_shape, na_action="ignore")
-        if geom_col:
-            df[geom_col] = gpd.array.GeometryArray(df[geom_col].values)
-            df = gpd.GeoDataFrame(df, geometry=geom_col)
-        return df
+        cdef char* s_bytes = cp.PyBytes_AsString(s)
+        cdef size_t length = len(s)
+        with nogil:
+            memcpy(self.get_buffer(), s_bytes, length)
 
-    def fetch_from_cursor(self, cursor, schema: sch.Schema) -> pd.DataFrame:
-        import pandas as pd
+        self.pos += length
 
+    def seek(self, int where):
+        self._raise_if_closed()
+
+        if where < 0 or where >= self.size:
+            raise IOError('Position out of bounds')
+
+        self.pos = where
+
+    def _raise_if_closed(self):
+        # No-op
+        pass
+
+
+cdef class RAMBuffer(BufferLike):
+    cdef:
+        uint8_t* buf
+
+    def __cinit__(self, size):
+        self.size = size
+        self.buf = <uint8_t*> malloc(self.size)
+        self.pos = 0
+
+    def __dealloc__(self):
+        if self.buf != NULL:
+            free(self.buf)
+
+    cdef uint8_t* get_buffer(self) nogil:
+        return self.buf + self.pos
+
+
+cdef class SharedMmap(BufferLike):
+    """
+    Simple interface to shared memory exchanged between Ibis and the parent
+    process via memory maps. It also implements the file interface so that it
+    can be treated as a file-like object by pure Python code.
+    """
+
+    def __cinit__(self, location, size, offset=0, create=False):
+        self.location = location
+        self.size = size
+        self.offset = offset
+
+        if not os.path.exists(self.location):
+            if not create:
+                # Don't create the file if it's not there already
+                raise IOError('%s does not exist' % self.location)
+            elif offset != 0:
+                raise IOError('File does not exist; nonzero offset invalid')
+
+            # Create the file and truncate to indicated size
+            self.file_handle = open(self.location, 'wb+')
+            self.file_handle.truncate(size)
+        else:
+            self.file_handle = open(self.location, 'rb+')
+
+        # Memory-map the file, raise on failure
+        cdef int fd = self.file_handle.fileno()
+        with nogil:
+            self.buf = <uint8_t*> mmap(NULL, self.size, PROT_READ | PROT_WRITE,
+                                       MAP_SHARED, fd, self.offset)
+
+        if self.buf == NULL:
+            # TODO: check various errno failure conditions, raise a more
+            # informative exception
+            raise Exception('Memory mapping failed')
+
+        self.closed = 0
+
+        # So we can treat this as file-like
+        self.pos = 0
+
+    def __dealloc__(self):
+        self.close()
+
+    cdef uint8_t* get_buffer(self) nogil:
+        return self.buf + self.pos
+
+    def __repr__(self):
+        return ('SharedMmap(%s, size=%d, offset=%d)' %
+                (self.location, self.size, self.offset))
+
+    def close(self):
+        if self.closed:
+            return
+
+        with nogil:
+            munmap(<void*> self.buf, self.size)
         try:
-            df = pd.DataFrame.from_records(
-                cursor, columns=schema.names, coerce_float=True
-            )
-        except Exception:
-            # clean up the cursor if we fail to create the DataFrame
-            #
-            # in the sqlite case failing to close the cursor results in
-            # artificially locked tables
-            cursor.close()
-            raise
-        df = PandasData.convert_table(df, schema)
-        if not df.empty and geospatial_supported:
-            return self._to_geodataframe(df, schema)
-        return df
-
-    @contextlib.contextmanager
-    def begin(self):
-        with self.con.begin() as bind:
-            yield bind
-
-    def _clean_up_tmp_table(self, tmptable: sa.Table) -> None:
-        with self.begin() as bind:
-            tmptable.drop(bind=bind)
-
-    def create_table(
-        self,
-        name: str,
-        obj: pd.DataFrame | pa.Table | ir.Table | None = None,
-        *,
-        schema: sch.Schema | None = None,
-        database: str | None = None,
-        temp: bool = False,
-        overwrite: bool = False,
-    ) -> ir.Table:
-        """Create a table.
+            self.file_handle.close()
+        except:
+            pass
 
-        Parameters
-        ----------
-        name
-            Name of the new table.
-        obj
-            An Ibis table expression or pandas table that will be used to
-            extract the schema and the data of the new table. If not provided,
-            `schema` must be given.
-        schema
-            The schema for the new table. Only one of `schema` or `obj` can be
-            provided.
-        database
-            Name of the database where the table will be created, if not the
-            default.
-        temp
-            Should the table be temporary for the session.
-        overwrite
-            Clobber existing data
+        self.closed = 1
 
-        Returns
-        -------
-        Table
-            The table that was created.
+    def tell(self):
+        self._raise_if_closed()
+        return self.pos
+
+    def flush(self):
+        cdef int ret
+
+        self._raise_if_closed()
+
+        with nogil:
+           ret = msync(<void*> self.buf, self.size, MS_SYNC)
+        if ret == -1:
+            raise IOError('msync failed')
+
+    def _raise_if_closed(self):
+        if self.closed:
+            raise IOError('File is closed')
+
+
+#----------------------------------------------------------------------
+# Read and write tables with as little copying as possible (preferably nearly
+# zero in the case of primitive types) from the binary format delivered by
+# Impala.
+#
+# TODO: is endianness ever a concern?
+# TODO: do we care about the column names? Adds complexity
+#
+# OMITTED FOR NOW
+# uint32_t*
+#   name offsets (K + 1 of them)
+# char*
+#   column names (no null terminators)
+#
+# Version 1 of binary table layout
+#
+# uint32_t
+#   M magic number
+# uint32_t
+#   K number of columns
+# uint64_t
+#   table length
+# uint8_t*
+#   T column dtype codes
+# uint32_t*
+#   O byte offsets (K + 1 offsets, so we have an exact length for each)
+# uint8_t
+#   F Column format
+# COLUMN* array of column blocks
+# INTERN_TABLE
+#   All strings refer to a master array of strings at the end of the blob
+#
+# The most general column format looks like the following:
+# uint8_t*
+#   null mask
+# void*
+#   data
+#
+# I'm putting a format code in the table layout to leave the door open for
+# delivery formats that are friendlier to the receiving data structure. For
+# example: badger format arrays (especially strings / category types) can be
+# received with less copying than either pandas/NumPy containers.
+#
+# One problem with this is that it will require copying for most practical
+# input formats (NumPy, pandas, badger, etc.) in use by users. Any string data
+# used by NumPy or pandas users will have to be copied / boxed in Python
+# objects, anyway. We also will have to adopt pandas conventions for missing
+# data in boolean and integer arrays (casting to a different NumPy dtype and
+# using a missing value marker).
+#
+# String columns: uint32_t* with references into master intern table
+# Intern table format (all UTF8 bytes, no null terminators):
+#   uint32_t K (length)
+#   uint32_t* offsets (K + 1 size)
+#   char* data
+#
+#   So offsets[0] = 0, and offsets[K] is the total size of the data array. We
+#   don't store any nulls at all.
+#
+# Timestamps and decimals will require some specialized handling.
+
+# Keep around instances of the common numeric dtypes for simplicity
+NPY_U1 = np.dtype('u1')
+NPY_U2 = np.dtype('u2')
+NPY_U4 = np.dtype('u4')
+NPY_U8 = np.dtype('u8')
+NPY_I1 = np.dtype('i1')
+NPY_I2 = np.dtype('i2')
+NPY_I4 = np.dtype('i4')
+NPY_I8 = np.dtype('i8')
+NPY_F4 = np.dtype('f4')
+NPY_F8 = np.dtype('f8')
+NPY_O = np.dtype('O')
+
+
+cdef uint32_t IMPALA_MAGIC_UINT32 = 1337959792
+
+# Only implementing the first type for now
+cdef uint8_t FORMAT_MASKED = 0
+cdef uint8_t FORMAT_PANDAS = 1
+
+# cdef uint8_t FORMAT_BADGER = 2
+
+
+# Enum-like class, for use on the Python side
+cdef class IbisType:
+    BOOLEAN = TYPE_BOOLEAN
+    TINYINT = TYPE_TINYINT
+    SMALLINT = TYPE_SMALLINT
+    INT = TYPE_INT
+    BIGINT = TYPE_BIGINT
+    FLOAT = TYPE_FLOAT
+    DOUBLE = TYPE_DOUBLE
+    STRING = TYPE_STRING
+
+    # TODO: Are these all handled as strings in Ibis-land?
+    VARCHAR = TYPE_VARCHAR
+    CHAR = TYPE_CHAR
+
+    TIMESTAMP = TYPE_TIMESTAMP
+    DECIMAL = TYPE_DECIMAL
+
+
+_ibis_to_numpy = {
+    IbisType.BOOLEAN: NPY_U1,
+    IbisType.TINYINT: NPY_I1,
+    IbisType.SMALLINT: NPY_I2,
+    IbisType.INT: NPY_I4,
+    IbisType.BIGINT: NPY_I8,
+    IbisType.FLOAT: NPY_F4,
+    IbisType.DOUBLE: NPY_F8,
+    IbisType.STRING: NPY_O,
+    IbisType.CHAR: NPY_O,
+    IbisType.VARCHAR: NPY_O,
+    IbisType.TIMESTAMP: NPY_I8,
+    IbisType.DECIMAL: NPY_F8  # HACK
+}
+
+_ibis_stride = {
+    IbisType.BOOLEAN: 1,
+    IbisType.TINYINT: 1,
+    IbisType.SMALLINT: 2,
+    IbisType.INT: 4,
+    IbisType.BIGINT: 8,
+    IbisType.FLOAT: 4,
+    IbisType.DOUBLE: 8,
+    IbisType.STRING: 1
+}
+
+cdef inline int type_to_stride(int dtype) except -1:
+    return _ibis_stride[dtype]
+
+
+cdef check_numpy_compat(ndarray arr, ibis_type):
+    ex_dtype = _ibis_to_numpy[ibis_type]
+    if arr.dtype != ex_dtype:
+        raise TypeError('Needed type to be %s, was %s' %
+                        (str(ex_dtype), str(arr.dtype)))
+
+
+_type_names = {
+    TYPE_BOOLEAN: 'boolean'
+}
+
+
+cdef class InternTableBuilder:
+    """
+    Quick and dirty so we can build our own intern tables for testing purposes
+    """
+    cdef:
+        char* data
+        uint32_t* offsets
+        uint32_t length
+        uint32_t data_cap
+        uint32_t offsets_cap
+
+        # hash table
+        int* ht
+        int hash_size
+
+    def __cinit__(self):
+        self.offsets = self.data = NULL
+
+        self.length = 0
+
+        self.offsets_cap = 1024
+        self.offsets = <uint32_t*> malloc(self.offsets_cap * 4)
+
+        if self.offsets == NULL:
+            raise MemoryError
+
+        self.data_cap = 32768
+        self.data = <char*> malloc(self.data_cap)
+
+        if self.data == NULL:
+            free(self.offsets)
+            raise MemoryError
+
+    def __dealloc__(self):
+        if self.offsets != NULL:
+            free(self.offsets)
+
+        if self.data != NULL:
+            free(self.data)
+
+    cdef inline uint32_t get(self, char* val, size_t length):
         """
-        if obj is None and schema is None:
-            raise com.IbisError("The schema or obj parameter is required")
+        Get code for string, and add to intern table if not in there already,
+        """
+        pass
 
-        import pandas as pd
-        import pyarrow as pa
+    def finalize(self):
+        """
+        Save the current state of the builder as an immutable InternTable
+        """
 
-        if isinstance(obj, (pd.DataFrame, pa.Table)):
-            obj = ibis.memtable(obj)
-
-        if database == self.current_database:
-            # avoid fully qualified name
-            database = None
-
-        if database is not None:
-            raise NotImplementedError(
-                "Creating tables from a different database is not yet implemented"
-            )
-
-        if obj is not None and schema is not None:
-            if not obj.schema().equals(ibis.schema(schema)):
-                raise com.IbisTypeError(
-                    'Expression schema is not equal to passed schema. '
-                    'Try passing the expression without the schema'
-                )
-        if schema is None:
-            schema = obj.schema()
-
-        self._schemas[self._fully_qualified_name(name, database)] = schema
-
-        if has_expr := obj is not None:
-            # this has to happen outside the `begin` block, so that in-memory
-            # tables are visible inside the transaction created by it
-            self._run_pre_execute_hooks(obj)
 
-        table = self._table_from_schema(
-            name, schema, database=database or self.current_database, temp=temp
-        )
+# Fowler-Noll-Vo hash function for strings, ported from Impala
+cdef uint64_t FNV64_PRIME = 1099511628211UL
+cdef inline uint64_t fnv_hash64(void* data, int bytes, uint64_t hash):
+    cdef uint8_t* ptr = <uint8_t*> data
+    while bytes > 0:
+        hash = (deref(ptr) ^ hash) * FNV64_PRIME
+        preinc(ptr)
+        predec(bytes)
+    return hash
+
+cdef inline uint32_t fnv_hash32(void* data, int bytes, uint32_t hash):
+    cdef uint64_t hash_u64 = hash | (<uint64_t>hash << 32)
+    hash_u64 = fnv_hash64(data, bytes, hash_u64)
+    return (hash_u64 >> 32) ^ (hash_u64 & 0xFFFFFFFF)
+
+
+cdef class InternTable:
+    """
+
+    """
+    cdef:
+        int fmt
+        uint32_t* offsets
+        uint8_t* data
+        size_t length
+
+        # hash table
+        uint32_t* ht
+
+    def __cinit__(self, format='pybytes'):
+        # TODO: the intern table might not necessarily want to produce PyBytes
+        # objects in all cases (e.g. if we have a string container that can
+        # handle raw bytes)
+        self.fmt = 0
+
+    cdef nbytes(self):
+        return self.offsets[self.length]
+
+    cdef init(self, uint32_t* offsets, uint8_t* data, size_t length):
+        self.data = data
+        self.offsets = offsets
+        self.length = length
 
-        if has_expr:
-            if self.supports_create_or_replace:
-                ctas = CreateTableAs(
-                    name,
-                    self.compile(obj),
-                    temp=temp,
-                    overwrite=overwrite,
-                    quote=self.compiler.translator_class._quote_table_names,
-                )
-                with self.begin() as bind:
-                    bind.execute(ctas)
-            else:
-                tmptable = self._table_from_schema(
-                    util.gen_name("tmp_table_insert"),
-                    schema,
-                    # some backends don't support temporary tables
-                    temp=self.supports_temporary_tables,
-                )
-                method = self._get_insert_method(obj)
-                insert = table.insert().from_select(tmptable.columns, tmptable.select())
-
-                with self.begin() as bind:
-                    # 1. write `obj` to a unique temp table
-                    tmptable.create(bind=bind)
-
-                # try/finally here so that a successfully created tmptable gets
-                # cleaned up no matter what
-                try:
-                    with self.begin() as bind:
-                        bind.execute(method(tmptable.insert()))
-
-                        # 2. recreate the existing table
-                        if overwrite:
-                            table.drop(bind=bind, checkfirst=True)
-                        table.create(bind=bind)
-
-                        # 3. insert the temp table's data into the (re)created table
-                        bind.execute(insert)
-                finally:
-                    self._clean_up_tmp_table(tmptable)
+    cdef write_buffer(self, uint8_t* buf):
+        """
+        Write the table to the passed buffer
+        """
+        cdef BufferInterface face = BufferInterface()
+        face.set_buffer(buf)
+
+        face.write_array(self.offsets, self.length + 1, 4)
+        face.write_array(self.data, self.nbytes(), 1)
+
+    cdef inline uint32_t get(self, char* val, size_t length):
+        pass
+
+
+cdef class IbisTableReader:
+    """
+
+    """
+    cdef readonly:
+        int ncolumns
+        uint64_t length
+
+    cdef:
+        uint8_t* buf
+        size_t bufsize
+
+        uint8_t* dtypes
+        uint32_t* col_offsets
+        uint8_t table_format
+
+        uint8_t* data_start
+        InternTable intern_table
+
+    def __cinit__(self, BufferLike container, format='numpy'):
+        self.buf = container.get_buffer()
+        self.bufsize = container.size
+
+        cdef BufferInterface reader = BufferInterface()
+        reader.set_buffer(self.buf)
+
+        cdef uint32_t magic = reader.read_uint32()
+        if magic != IMPALA_MAGIC_UINT32:
+            raise ValueError('Magic code at start of buffer did not match')
+
+        self.ncolumns = reader.read_uint32()
+        self.length = reader.read_uint64()
+
+        cdef uint8_t* data = self.buf + reader.pos
+
+        # TODO: decide if column names are desired
+
+        # Just the bytes there. Any reason to copy?
+        self.dtypes = data
+        data += self.ncolumns
+
+        self.col_offsets = <uint32_t*> data
+        data += 4 * (self.ncolumns + 1)
+
+        # Validate this?
+        self.table_format = data[0]
+
+        self.data_start = data + 1
+
+        # Initialize the intern table
+        self.intern_table = None
+
+    def get_column(self, i):
+        # TODO: boundscheck
+        if self.table_format == FORMAT_MASKED:
+            return self._read_masked(i)
         else:
-            with self.begin() as bind:
-                if overwrite:
-                    table.drop(bind=bind, checkfirst=True)
-                table.create(bind=bind)
-        return self.table(name, database=database)
-
-    def _get_insert_method(self, expr):
-        compiled = self.compile(expr)
-
-        # if in memory tables aren't cheap then try to pull out their data
-        # FIXME: queries that *select* from in memory tables are still broken
-        # for mysql/sqlite/postgres because the generated SQL is wrong
-        if (
-            not self.compiler.cheap_in_memory_tables
-            and self.compiler.support_values_syntax_in_select
-            and isinstance(expr.op(), ops.InMemoryTable)
-        ):
-            (from_,) = compiled.get_final_froms()
-            try:
-                (rows,) = from_._data
-            except AttributeError:
-                return methodcaller("from_select", list(expr.columns), from_)
-            else:
-                return methodcaller("values", rows)
+            raise NotImplementedError
 
-        return methodcaller("from_select", list(expr.columns), compiled)
+    cdef _read_masked(self, int i):
+        cdef MaskedColumnReader reader = MaskedColumnReader()
 
-    def _columns_from_schema(self, name: str, schema: sch.Schema) -> list[sa.Column]:
-        return [
-            sa.Column(
-                colname,
-                self.compiler.translator_class.get_sqla_type(dtype),
-                nullable=dtype.nullable,
-                quote=self.compiler.translator_class._quote_column_names,
-            )
-            for colname, dtype in zip(schema.names, schema.types)
-        ]
-
-    def _table_from_schema(
-        self,
-        name: str,
-        schema: sch.Schema,
-        temp: bool = False,
-        database: str | None = None,
-        **kwargs: Any,
-    ) -> sa.Table:
-        columns = self._columns_from_schema(name, schema)
-        return sa.Table(
-            name,
-            sa.MetaData(),
-            *columns,
-            prefixes=[self._temporary_prefix] if temp else [],
-            quote=self.compiler.translator_class._quote_table_names,
-            **kwargs,
-        )
+        reader.init(self.dtypes[i], self.length,
+                    self.data_start + self.col_offsets[i],
+                    self.intern_table)
 
-    def drop_table(
-        self, name: str, *, database: str | None = None, force: bool = False
-    ) -> None:
-        """Drop a table.
+        return reader.read()
 
-        Parameters
-        ----------
-        name
-            Table to drop
-        database
-            Database to drop table from
-        force
-            Check for existence before dropping
-        """
-        if database == self.current_database:
-            # avoid fully qualified name
-            database = None
-
-        if database is not None:
-            raise com.IbisInputError(
-                "Dropping tables from a different database is not yet implemented"
-            )
-
-        t = self._get_sqla_table(name, schema=database, autoload=False)
-        with self.begin() as bind:
-            t.drop(bind=bind, checkfirst=force)
-
-        qualified_name = self._fully_qualified_name(name, database)
-
-        with contextlib.suppress(KeyError):
-            # schemas won't be cached if created with raw_sql
-            del self._schemas[qualified_name]
-
-    def truncate_table(self, name: str, database: str | None = None) -> None:
-        t = self._get_sqla_table(name, schema=database)
-        with self.begin() as con:
-            con.execute(t.delete())
+    cdef _read_pandas(self, int i):
+        raise NotImplementedError
+
+
+cdef class IbisColumnReader:
+    pass
+
+
+
+cdef class MaskedColumnReader(IbisColumnReader):
+
+    cdef:
+        uint8_t* buf
+        uint8_t dtype
+        uint64_t length
+        InternTable intern_table
+
+    cdef init(self, uint8_t dtype, uint64_t length, uint8_t* buf,
+              InternTable table):
+        self.buf = buf
+        self.dtype = dtype
+        self.length = length
+        self.intern_table = table
+
+    def read(self):
+        cdef MaskedColumn result = MaskedColumn()
+
+        result.dtype = self.dtype
+        result.length = self.length
+        result.stride = type_to_stride(self.dtype)
+
+        result.null_mask = self.buf
+        result.data = self.buf + self.length
 
-    def schema(self, name: str) -> sch.Schema:
-        """Get an ibis schema from the current database for the table `name`.
+        return result
+
+#----------------------------------------------------------------------
+# Column data handlers and coercion to various compatible formats
+
+cdef class IbisColumn:
+
+    cdef readonly:
+        int dtype
+        uint64_t length
+        size_t stride
+
+    # N.B. all Cython cdef methods are "virtual" in the C++ sense, so it's safe
+    # to use cdef IbisColumn and you'll get the subclass methods
+    cpdef nbytes(self):
+        raise NotImplementedError
+
+    cdef write_buffer(self, uint8_t* buf):
+        raise NotImplementedError
+
+
+cdef class MaskedColumn(IbisColumn):
+    """
+    Adapter class for masked array format data with columnar layout (null mask
+    and data stored in contiguous arrays).
+    """
+    cdef:
+        # We never own this data
+        uint8_t* null_mask
+        uint8_t* data
+
+        # In case we need to hold on to references to some objects
+        list obj_refs
+
+    format_code = FORMAT_MASKED
+
+    cpdef nbytes(self):
+        # The number of bytes taken up by the table in binary-serialized form
+        return self.length * (self.stride + 1)
+
+    def __len__(self):
+        return self.length
+
+    cdef init_from_buffer(self, uint8_t* buf, int dtype,
+                          uint64_t length, size_t stride):
+        self.dtype = dtype
+        self.length = length
+        self.stride = stride
+
+        self.null_mask = buf
+        self.data = buf + length
+
+    cdef write_buffer(self, uint8_t* buf):
+        memcpy(buf, self.null_mask, self.length)
+        memcpy(buf + self.length, self.data, self.length * self.stride)
+
+    def mask(self):
+        return buffer_to_numpy_view(self.null_mask, self.length, cnp.NPY_UINT8)
+
+    def data_bytes(self):
+        return buffer_to_numpy_view(self.data, self.length * self.stride,
+                                    cnp.NPY_UINT8)
+
+    def to_numpy_for_pandas(self, copy=False):
+        """
+        Produce a new array (copy of data) containing data in a suitable
+        representation for immediate use in pandas.
 
         Parameters
         ----------
-        name
-            Table name
+        copy : bool, default False
+            Avoid copying any data if we can
 
         Returns
         -------
-        Schema
-            The ibis schema of `name`
+        arr : ndarray
         """
-        return self.database().schema(name)
+        # TODO: reduce code duplication
+        if self.dtype == TYPE_BOOLEAN:
+            return _box_pandas_bool(self.data, self.null_mask, self.length,
+                                    copy=copy)
+        elif self.dtype == TYPE_TINYINT:
+            return _box_pandas_integer(<int8_t*> self.data, self.null_mask,
+                                       self.length, NPY_I1, copy=copy)
+        elif self.dtype == TYPE_SMALLINT:
+            return _box_pandas_integer(<int16_t*> self.data, self.null_mask,
+                                       self.length, NPY_I2, copy=copy)
+        elif self.dtype == TYPE_INT:
+            return _box_pandas_integer(<int32_t*> self.data, self.null_mask,
+                                       self.length, NPY_I4, copy=copy)
+        elif self.dtype == TYPE_BIGINT:
+            return _box_pandas_integer(<int64_t*> self.data, self.null_mask,
+                                       self.length, NPY_I8, copy=copy)
+        elif self.dtype == TYPE_FLOAT:
+            return _box_pandas_floating(<float*> self.data, self.null_mask,
+                                        self.length, NPY_F4, copy=copy)
+        elif self.dtype == TYPE_DOUBLE:
+            return _box_pandas_floating(<double*> self.data, self.null_mask,
+                                        self.length, NPY_F8, copy=copy)
+        elif self.dtype == TYPE_TIMESTAMP:
+            raise NotImplementedError
+        elif self.dtype == TYPE_DECIMAL:
+            raise NotImplementedError
 
-    def _log(self, sql):
-        try:
-            query_str = str(sql)
-        except sa.exc.UnsupportedCompilationError:
-            pass
-        else:
-            util.log(query_str)
-
-    @staticmethod
-    def _new_sa_metadata():
-        return sa.MetaData()
-
-    def _get_sqla_table(
-        self, name: str, schema: str | None = None, autoload: bool = True, **_: Any
-    ) -> sa.Table:
-        meta = self._new_sa_metadata()
-        with warnings.catch_warnings():
-            warnings.filterwarnings(
-                "ignore", message="Did not recognize type", category=sa.exc.SAWarning
-            )
-            warnings.filterwarnings(
-                "ignore", message="index key", category=sa.exc.SAWarning
-            )
-            table = sa.Table(
-                name,
-                meta,
-                schema=schema,
-                autoload_with=self.con if autoload else None,
-                quote=self.compiler.translator_class._quote_table_names,
-            )
-            nulltype_cols = frozenset(
-                col.name for col in table.c if isinstance(col.type, sa.types.NullType)
-            )
-
-            if not nulltype_cols:
-                return table
-            return self._handle_failed_column_type_inference(table, nulltype_cols)
-
-    # TODO(kszucs): remove the schema parameter
-    @classmethod
-    def _schema_from_sqla_table(
-        cls,
-        table: sa.sql.TableClause,
-        schema: sch.Schema | None = None,
-    ) -> sch.Schema:
-        """Retrieve an ibis schema from a SQLAlchemy `Table`.
+    def to_masked_array(self, copy=False):
+        """
+        Create a numpy.ma.MaskedArray from the, and do not copy the data if
+        possible by default
 
         Parameters
         ----------
-        table
-            Table whose schema to infer
-        schema
-            Predefined ibis schema to pull types from
-        dialect
-            Optional sqlalchemy dialect
+        copy: bool, default False
+            Avoid copying any data if we can
 
         Returns
         -------
-        schema
-            An ibis schema corresponding to the types of the columns in `table`.
+        arr : numpy.ma.MaskedArray
         """
-        schema = schema if schema is not None else {}
-        pairs = []
-        for column in table.columns:
-            name = column.name
-            if name in schema:
-                dtype = schema[name]
+        pass
+
+    def equals(self, MaskedColumn other):
+        if (self.dtype != other.dtype or
+            self.length != other.length):
+            return False
+
+        # Compare the data ignoring data behind the null mask
+        cdef int i
+
+        for i in range(self.length):
+            if self.null_mask[i] != other.null_mask[i]:
+                return False
+
+            if self.null_mask[i]:
+                continue
+
+            if memcmp(self.data + i * self.stride,
+                      other.data + i * other.stride, self.stride) != 0:
+                return False
+
+        return True
+
+
+cdef _box_pandas_bool(uint8_t* data, uint8_t* mask, int length,
+                      copy=False):
+    cdef:
+        int i
+        bint has_null = 0
+        ndarray[object] oresult
+
+    # Is there a null?
+    with nogil:
+        for i in range(length):
+            if mask[i]:
+                has_null = 1
+                break
+
+    if has_null:
+        # Pack in object array
+        oresult = np.empty(length, dtype=object)
+        for i in range(length):
+            if mask[i]:
+                oresult[i] = None
             else:
-                dtype = cls.compiler.translator_class.get_ibis_type(
-                    column.type, nullable=column.nullable
-                )
-            pairs.append((name, dtype))
-        return sch.schema(pairs)
-
-    def _handle_failed_column_type_inference(
-        self, table: sa.Table, nulltype_cols: Iterable[str]
-    ) -> sa.Table:
-        """Handle cases where SQLAlchemy cannot infer the column types of `table`."""
-
-        self.inspector.reflect_table(table, table.columns)
-
-        dialect = self.con.dialect
-
-        quoted_name = ".".join(
-            map(
-                dialect.identifier_preparer.quote,
-                filter(None, [table.schema, table.name]),
-            )
-        )
+                if data[i]:
+                    oresult[i] = True
+                else:
+                    oresult[i] = False
+        return oresult
+    else:
+        result = buffer_to_numpy_view(data, length, cnp.NPY_BOOL)
+        if copy:
+            result = result.copy()
+        return result
+
+
+cdef _box_pandas_integer(signed_int* data, uint8_t* mask, int length,
+                         object dtype, copy=False):
+    cdef:
+        int i
+        bint has_null = 0
+        ndarray[double] fresult
+
+    # Is there a null?
+    with nogil:
+        for i in range(length):
+            if mask[i]:
+                has_null = 1
+                break
+
+    if has_null:
+        # Must pack in float64 array
+        fresult = np.empty(length, dtype=np.float64)
+        with nogil:
+            for i in range(length):
+                if mask[i]:
+                    fresult[i] = NaN
+                else:
+                    # 64-bit integers could be outside the FP representation
+                    # range, but that's always been an issue with pandas
+                    fresult[i] = data[i]
+        return fresult
+    else:
+        result = buffer_to_numpy_view(data, length, dtype.num)
+        if copy:
+            result = result.copy()
+        return result
+
+cdef _box_pandas_floating(cython.floating* data, uint8_t* mask, int length,
+                          object dtype, copy=False):
+    cdef:
+        int i
+        ndarray[cython.floating] result
+
+    result = np.empty(length, dtype=dtype)
+    with nogil:
+        for i in range(length):
+            if mask[i]:
+                result[i] = NaN
+            else:
+                result[i] = data[i]
+    return result
 
-        for colname, type in self._metadata(quoted_name):
-            if colname in nulltype_cols:
-                # replace null types discovered by sqlalchemy with non null
-                # types
-                table.append_column(
-                    sa.Column(
-                        colname,
-                        self.compiler.translator_class.get_sqla_type(type),
-                        nullable=type.nullable,
-                        quote=self.compiler.translator_class._quote_column_names,
-                    ),
-                    replace_existing=True,
-                )
-        return table
 
-    def raw_sql(self, query) -> None:
-        """Execute a query string.
+cdef _box_pandas_string(uint32_t* labels, uint8_t* mask, int length,
+                        InternTable table):
+    cdef:
+        ndarray[object] result
 
-        !!! warning "The returned cursor object must be **manually** released."
+    result = np.empty(length, dtype=object)
 
-        Parameters
-        ----------
-        query
-            DDL or DML statement
-        """
-        return self.con.connect().execute(
-            sa.text(query) if isinstance(query, str) else query
-        )
+    return result
 
-    def table(
-        self,
-        name: str,
-        database: str | None = None,
-        schema: str | None = None,
-    ) -> ir.Table:
-        """Create a table expression from a table in the database.
 
-        Parameters
-        ----------
-        name
-            Table name
-        database
-            The database the table resides in
-        schema
-            The schema inside `database` where the table resides.
+cdef buffer_to_numpy_view(void* data, int n, int ndtype):
+    cdef:
+        cnp.npy_intp shape[1]
+        cnp.ndarray result
 
-            !!! warning "`schema` refers to database organization"
+    # I believe this is a view by default
+    shape[0] = <cnp.npy_intp> n
+    result = cnp.PyArray_SimpleNewFromData(1, shape, ndtype, data)
 
-                The `schema` parameter does **not** refer to the column names
-                and types of `table`.
+    return result
 
-        Returns
-        -------
-        Table
-            Table expression
-        """
-        namespace = schema
-        if database is not None:
-            if not isinstance(database, str):
-                raise com.IbisTypeError(
-                    f"`database` must be a string; got {type(database)}"
-                )
-            if database != self.current_database:
-                return self.database(name=database).table(name=name, schema=schema)
 
-        sqla_table = self._get_sqla_table(name, schema=schema)
+def masked_from_numpy(ndarray values, ndarray mask, int ibis_type,
+                      InternTableBuilder intern_t=None):
+    # Helper function to convert masked format data represented as NumPy arrays
+    # into a MaskedColumn which can be written out to an Ibis-format file
+    cdef MaskedColumn result = MaskedColumn()
 
-        schema = self._schema_from_sqla_table(
-            sqla_table, schema=self._schemas.get(name)
-        )
-        node = ops.DatabaseTable(
-            name=name, schema=schema, source=self, namespace=namespace
-        )
-        return node.to_expr()
+    check_numpy_compat(mask, IbisType.BOOLEAN)
+    check_numpy_compat(values, ibis_type)
 
-    def _insert_dataframe(
-        self, table_name: str, df: pd.DataFrame, overwrite: bool
-    ) -> None:
-        schema = self._current_schema
-
-        t = self._get_sqla_table(table_name, schema=schema)
-        with self.con.begin() as con:
-            if overwrite:
-                con.execute(t.delete())
-            con.execute(t.insert(), df.to_dict(orient="records"))
-
-    def insert(
-        self,
-        table_name: str,
-        obj: pd.DataFrame | ir.Table | list | dict,
-        database: str | None = None,
-        overwrite: bool = False,
-    ) -> None:
-        """Insert data into a table.
+    if len(mask) != len(values):
+        raise ValueError('arrays different lengths')
 
-        Parameters
-        ----------
-        table_name
-            The name of the table to which data needs will be inserted
-        obj
-            The source data or expression to insert
-        database
-            Name of the attached database that the table is located in.
-        overwrite
-            If `True` then replace existing contents of table
-
-        Raises
-        ------
-        NotImplementedError
-            If inserting data from a different database
-        ValueError
-            If the type of `obj` isn't supported
-        """
-
-        import pandas as pd
-
-        if database == self.current_database:
-            # avoid fully qualified name
-            database = None
-
-        if database is not None:
-            raise NotImplementedError(
-                'Inserting data to a table from a different database is not '
-                'yet implemented'
-            )
-
-        # If we've been passed a `memtable`, pull out the underlying dataframe
-        if isinstance(obj, ir.Table) and isinstance(
-            in_mem_table := obj.op(), ops.InMemoryTable
-        ):
-            obj = in_mem_table.data.to_frame()
-
-        if isinstance(obj, pd.DataFrame):
-            self._insert_dataframe(table_name, obj, overwrite=overwrite)
-        elif isinstance(obj, ir.Table):
-            to_table_expr = self.table(table_name)
-            to_table_schema = to_table_expr.schema()
-
-            if overwrite:
-                self.drop_table(table_name, database=database)
-                self.create_table(table_name, schema=to_table_schema, database=database)
-
-            to_table = self._get_sqla_table(table_name, schema=database)
-
-            from_table_expr = obj
-
-            with self.begin() as bind:
-                if from_table_expr is not None:
-                    compiled = from_table_expr.compile()
-                    columns = [
-                        self.con.dialect.normalize_name(c)
-                        for c in from_table_expr.columns
-                    ]
-                    bind.execute(to_table.insert().from_select(columns, compiled))
-        elif isinstance(obj, (list, dict)):
-            to_table = self._get_sqla_table(table_name, schema=database)
-
-            with self.begin() as bind:
-                if overwrite:
-                    bind.execute(to_table.delete())
-                bind.execute(to_table.insert().values(obj))
+    # TODO: conversion of strings / other non-natively mapping types
 
-        else:
-            raise ValueError(
-                "No operation is being performed. Either the obj parameter "
-                "is not a pandas DataFrame or is not a ibis Table."
-                f"The given obj is of type {type(obj).__name__} ."
-            )
-
-    def _compile_opaque_udf(self, udf_node: ops.ScalarUDF) -> str:
-        return None
-
-    def _compile_python_udf(self, udf_node: ops.ScalarUDF) -> str:
-        if self.supports_python_udfs:
-            raise NotImplementedError(
-                f"The {self.name} backend does not support Python scalar UDFs"
-            )
-
-    def _compile_pandas_udf(self, udf_node: ops.ScalarUDF) -> str:
-        if self.supports_python_udfs:
-            raise NotImplementedError(
-                f"The {self.name} backend does not support Pandas-based vectorized scalar UDFs"
-            )
-
-    def _compile_pyarrow_udf(self, udf_node: ops.ScalarUDF) -> str:
-        if self.supports_python_udfs:
-            raise NotImplementedError(
-                f"The {self.name} backend does not support PyArrow-based vectorized scalar UDFs"
-            )
-
-    def _define_udf_translation_rules(self, expr):
-        for udf_node in expr.op().find(ops.ScalarUDF):
-            udf_node_type = type(udf_node)
-
-            if udf_node_type not in self.compiler.translator_class._registry:
-
-                @self.add_operation(udf_node_type)
-                def _(t, op):
-                    generator = sa.func
-                    if (namespace := op.__udf_namespace__) is not None:
-                        generator = getattr(generator, namespace)
-                    func = getattr(generator, type(op).__name__)
-                    return func(*map(t.translate, op.args))
-
-    def _register_udfs(self, expr: ir.Expr) -> None:
-        with self.begin() as con:
-            for udf_node in expr.op().find(ops.ScalarUDF):
-                compile_func = getattr(
-                    self, f"_compile_{udf_node.__input_type__.name.lower()}_udf"
-                )
-                if sql := compile_func(udf_node):
-                    con.exec_driver_sql(sql)
-
-    def _quote(self, name: str) -> str:
-        """Quote an identifier."""
-        preparer = self.con.dialect.identifier_preparer
-        if self.compiler.translator_class._quote_table_names:
-            return preparer.quote_identifier(name)
-        return preparer.quote(name)
-
-    def _get_temp_view_definition(
-        self, name: str, definition: sa.sql.compiler.Compiled
-    ) -> str:
-        raise NotImplementedError(
-            f"The {self.name} backend does not implement temporary view creation"
-        )
+    result.dtype = ibis_type
+    result.stride = values.dtype.itemsize
+    result.length = len(values)
+    result.null_mask = <uint8_t*> mask.data
+    result.data = <uint8_t*> values.data
 
-    def _register_temp_view_cleanup(self, name: str, raw_name: str) -> None:
-        query = f"DROP VIEW IF EXISTS {name}"
+    # Prevent these arrays from being garbage-collected
+    result.obj_refs = [values, mask]
 
-        def drop(self, raw_name: str, query: str):
-            with self.begin() as con:
-                con.exec_driver_sql(query)
-            self._temp_views.discard(raw_name)
-
-        atexit.register(drop, self, raw_name, query)
-
-    def _get_compiled_statement(
-        self,
-        definition: sa.sql.Selectable,
-        name: str,
-        compile_kwargs: Mapping[str, Any] | None = None,
-    ):
-        if compile_kwargs is None:
-            compile_kwargs = {}
-        compiled = definition.compile(
-            dialect=self.con.dialect, compile_kwargs=compile_kwargs
-        )
-        lines = self._get_temp_view_definition(name, definition=compiled)
-        return lines, compiled.params
+    return result
 
-    def _create_temp_view(self, view: sa.Table, definition: sa.sql.Selectable) -> None:
-        raw_name = view.name
-        if raw_name not in self._temp_views and raw_name in self.list_tables():
-            raise ValueError(f"{raw_name} already exists as a table or view")
-        name = self._quote(raw_name)
-        self._execute_view_creation(name, definition)
-        self._temp_views.add(raw_name)
-        self._register_temp_view_cleanup(name, raw_name)
-
-    def _execute_view_creation(self, name, definition):
-        lines, params = self._get_compiled_statement(definition, name)
-        with self.begin() as con:
-            for line in lines:
-                con.exec_driver_sql(line, parameters=params or ())
-
-    @abc.abstractmethod
-    def _metadata(self, query: str) -> Iterable[tuple[str, dt.DataType]]:
-        ...
-
-    def _get_schema_using_query(self, query: str) -> sch.Schema:
-        """Return an ibis Schema from a backend-specific SQL string."""
-        return sch.Schema.from_tuples(self._metadata(query))
-
-    def _load_into_cache(self, name, expr):
-        self.create_table(name, expr, schema=expr.schema(), temp=True)
-
-    def _clean_up_cached_table(self, op):
-        self.drop_table(op.name)
-
-    def create_view(
-        self,
-        name: str,
-        obj: ir.Table,
-        *,
-        database: str | None = None,
-        overwrite: bool = False,
-    ) -> ir.Table:
-        from sqlalchemy_views import CreateView
-
-        source = self.compile(obj)
-        view = CreateView(
-            sa.Table(
-                name,
-                sa.MetaData(),
-                schema=database,
-                quote=self.compiler.translator_class._quote_table_names,
-            ),
-            source,
-            or_replace=overwrite,
-        )
-        with self.begin() as con:
-            con.execute(view)
-        return self.table(name, database=database)
-
-    def drop_view(
-        self, name: str, *, database: str | None = None, force: bool = False
-    ) -> None:
-        from sqlalchemy_views import DropView
-
-        view = DropView(
-            sa.Table(
-                name,
-                sa.MetaData(),
-                schema=database,
-                quote=self.compiler.translator_class._quote_table_names,
-            ),
-            if_exists=not force,
+
+cdef class IbisTableWriter:
+    """
+    Writes the Ibis binary file format (in production this will be produced by
+    Impala, but we need to be able to produce it ourselves mostly for testing
+    purposes, and we can verify successful roundtrips to and from Impala
+    separately.
+
+    This class assumes that the passed columns are all formatted in the same
+    way. If it becomes necessary, we can fairly easily revise the binary format
+    to allow multiple formats per file.
+    """
+    cdef:
+        object columns
+        InternTable intern_table
+
+        uint32_t ncols
+        uint8_t* dtypes
+        uint32_t* col_offsets
+        uint32_t length
+        uint8_t col_format
+
+    def __cinit__(self, columns, InternTable intern_table=None):
+        if len(columns) == 0:
+            raise ValueError('must be at least one column')
+
+        self.columns = columns
+        self.intern_table = intern_table
+
+        self.col_offsets = self.dtypes = NULL
+        self._populate_metadata()
+
+    def _populate_metadata(self):
+        cdef:
+            int i
+            IbisColumn col
+
+        self.length = len(self.columns[0])
+        self.ncols = len(self.columns)
+
+        self.dtypes = <uint8_t*> malloc(self.ncols)
+        if self.dtypes == NULL:
+            raise MemoryError
+
+        self.col_offsets = <uint32_t*> malloc((self.ncols + 1) * 4)
+        if self.col_offsets == NULL:
+            free(self.dtypes)
+            raise MemoryError
+
+        cdef size_t offset = 0
+        for i in range(self.ncols):
+            col = self.columns[i]
+            self.dtypes[i] = col.dtype
+            self.col_offsets[i] = offset
+            offset += col.nbytes()
+
+        # "End cap" for easy arithmetic, also serves to mark the total number
+        # of bytes for all columns
+        self.col_offsets[self.ncols] = offset
+        self.col_format = self.columns[0].format_code
+
+    def total_size(self):
+        cdef size_t total = 0
+
+        # Total up preamble
+        total = (
+            4 +  # Magic
+            4 +  # num columns
+            8 +  # length
+            self.ncols + # dtypes
+            4 * (self.ncols + 1) + # column byte offsets
+            1    # column format
         )
 
-        with self.begin() as con:
-            con.execute(view)
+        # Add column bytes
+        total += self.col_offsets[self.ncols]
+
+        # TODO: Add intern table bytes
+
+        return total
+
+    def __dealloc__(self):
+        if self.col_offsets != NULL:
+            free(self.col_offsets)
+
+        if self.dtypes != NULL:
+            free(self.dtypes)
+
+    def write(self, BufferLike obj):
+        # Check there's enough space in the buffer object (e.g. a memory map)
+        # to hold the results
+        if obj.size < self.total_size():
+            raise ValueError('Buffer is too small to hold the whole table')
+
+        self.write_to(obj.get_buffer())
+
+    cdef write_to(self, uint8_t* buf):
+        cdef:
+            BufferInterface writer = BufferInterface()
+            IbisColumn col
+
+        # Write table preamble
+        writer.set_buffer(buf)
+
+        writer.write_uint32(IMPALA_MAGIC_UINT32)
+        writer.write_uint32(self.ncols)
+        writer.write_uint64(self.length)
+
+        writer.write_array(self.dtypes, self.ncols, 1)
+        writer.write_array(self.col_offsets, self.ncols + 1, 4)
+        writer.write_uint8(self.col_format)
+
+        buf += writer.pos
+
+        # Write columns
+        for col in self.columns:
+            col.write_buffer(buf)
+            buf += col.nbytes()
+
+        # Write string intern table and any other data
+        if self.intern_table is not None:
+            self.intern_table.write_buffer(buf)
+
+
+cdef class BufferInterface:
+    """
+    File-like object for reading/writing bytes into some memory region
+    """
+    cdef:
+        uint8_t* buf
+        int pos
+
+    def __cinit__(self):
+        self.pos = 0
+
+    cdef set_buffer(self, uint8_t* buf):
+        self.buf = buf
+
+    cdef seek(self, int pos):
+        self.pos = pos
+
+    cdef inline void write_array(self, void* data, int length, int stride):
+        memcpy(self.buf + self.pos, data, length * stride)
+        self.pos += length * stride
+
+    cdef inline void write_uint8(self, uint8_t val):
+        (self.buf + self.pos)[0] = val
+        self.pos += 1
+
+    cdef inline uint8_t read_uint8(self):
+        cdef uint8_t val = (self.buf + self.pos)[0]
+        self.pos += 1
+        return val
+
+    cdef inline void write_uint32(self, uint32_t val):
+        (<uint32_t*> (self.buf + self.pos))[0] = val
+        self.pos += 4
+
+    cdef inline uint32_t read_uint32(self):
+        cdef uint32_t val = (<uint32_t*> (self.buf + self.pos))[0]
+        self.pos += 4
+        return val
+
+    cdef inline void write_uint64(self, uint64_t val):
+        (<uint64_t*> (self.buf + self.pos))[0] = val
+        self.pos += 8
+
+    cdef inline uint64_t read_uint64(self):
+        cdef uint64_t val = (<uint64_t*> (self.buf + self.pos))[0]
+        self.pos += 8
+        return val
+
+#----------------------------------------------------------------------
+
+# Faster UUIDs with libuuid
+
+cdef extern from "Python.h":
+    object _PyLong_FromByteArray(unsigned char *bytes, unsigned int n,
+                                 int little_endian, int is_signed)
+    char *PyString_AS_STRING(object s)
+
+
+cdef class UUID:
+    cdef:
+        object bytes, int
+
+    def __init__(self, version=4, *args, **kwargs):
+        cdef object buf = cp.PyBytes_FromStringAndSize(NULL, 16)
+        cdef unsigned char *_bytes = <unsigned char*>PyString_AS_STRING(buf)
+        if version == 1:
+            uuid_generate_time(_bytes)
+        elif version == 4:
+            uuid_generate_random(_bytes)
+        self.bytes = buf
+        self.int = _PyLong_FromByteArray(_bytes, 16, 0, 0)
+
+    def get_bytes(self):
+        return self.bytes
+
+    def get_hex(self):
+        return '%032x' % self.int
+
+uuid = UUID
+
+def uuid1_bytes():
+    cdef object bytes = cp.PyBytes_FromStringAndSize(NULL, 16)
+    uuid_generate_time(<unsigned char*>PyString_AS_STRING(bytes))
+    return bytes
+
+def uuid4_bytes():
+    cdef object bytes = cp.PyBytes_FromStringAndSize(NULL, 16)
+    uuid_generate_random(<unsigned char*>PyString_AS_STRING(bytes))
+    return bytes
+
+def uuid4_hex():
+    cdef uuid_t guid
+    uuid_generate_random(<unsigned char*> guid)
+    return '%032x' % _PyLong_FromByteArray(<unsigned char*> guid, 16, 0, 0)
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/base/sql/compiler/base.py` & `ibis-framework-v0.6.0/ibis/sql/transforms.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,117 +1,131 @@
-from __future__ import annotations
+# Copyright 2015 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-import abc
-from itertools import chain
 
-import toolz
-
-import ibis.expr.analysis as an
+import ibis.expr.analysis as L
 import ibis.expr.operations as ops
-from ibis import util
+import ibis.expr.types as ir
+import ibis.util as util
 
 
-class DML(abc.ABC):
-    @abc.abstractmethod
-    def compile(self):
-        pass
+class ExistsExpr(ir.AnalyticExpr):
 
+    def type(self):
+        return 'exists'
 
-class DDL(abc.ABC):
-    @abc.abstractmethod
-    def compile(self):
-        pass
 
+class ExistsSubquery(ir.Node):
 
-class QueryAST:
-    __slots__ = 'context', 'dml', 'setup_queries', 'teardown_queries'
+    """
+    Helper class
+    """
 
-    def __init__(self, context, dml, setup_queries=None, teardown_queries=None):
-        self.context = context
-        self.dml = dml
-        self.setup_queries = setup_queries
-        self.teardown_queries = teardown_queries
-
-    @property
-    def queries(self):
-        return [self.dml]
-
-    def compile(self):
-        compiled_setup_queries = [q.compile() for q in self.setup_queries]
-        compiled_queries = [q.compile() for q in self.queries]
-        compiled_teardown_queries = [q.compile() for q in self.teardown_queries]
-        return self.context.collapse(
-            list(
-                chain(
-                    compiled_setup_queries,
-                    compiled_queries,
-                    compiled_teardown_queries,
-                )
-            )
-        )
-
-
-class SetOp(DML):
-    def __init__(self, tables, node, context, distincts):
-        assert isinstance(node, ops.Node)
-        assert all(isinstance(table, ops.Node) for table in tables)
+    def __init__(self, foreign_table, predicates):
+        self.foreign_table = foreign_table
+        self.predicates = predicates
+        ir.Node.__init__(self, [foreign_table, predicates])
+
+    def output_type(self):
+        return ExistsExpr
+
+
+class NotExistsSubquery(ir.Node):
+
+    def __init__(self, foreign_table, predicates):
+        self.foreign_table = foreign_table
+        self.predicates = predicates
+        ir.Node.__init__(self, [foreign_table, predicates])
+
+    def output_type(self):
+        return ExistsExpr
+
+
+class AnyToExistsTransform(object):
+
+    """
+    Some code duplication with the correlated ref check; should investigate
+    better code reuse.
+    """
+
+    def __init__(self, context, expr, parent_table):
         self.context = context
-        self.tables = tables
-        self.table_set = node
-        self.distincts = distincts
-        self.filters = []
-
-    @classmethod
-    def keyword(cls, distinct):
-        return cls._keyword + (not distinct) * " ALL"
-
-    def _get_keyword_list(self):
-        return map(self.keyword, self.distincts)
-
-    def _extract_subqueries(self):
-        # extract any subquery to avoid generating incorrect sql when at least
-        # one of the set operands is invalid outside of being a subquery
-        #
-        # for example: SELECT * FROM t ORDER BY x UNION ...
-        self.subqueries = an.find_subqueries(
-            [self.table_set, *self.filters], min_dependents=1
-        )
-        for subquery in self.subqueries:
-            self.context.set_extracted(subquery)
-
-    def format_subqueries(self):
-        context = self.context
-        subqueries = self.subqueries
-
-        return ',\n'.join(
-            '{} AS (\n{}\n)'.format(
-                context.get_ref(expr),
-                util.indent(context.get_compiled_expr(expr), 2),
-            )
-            for expr in subqueries
-        )
-
-    def format_relation(self, expr):
-        ref = self.context.get_ref(expr)
-        if ref is not None:
-            return f'SELECT *\nFROM {ref}'
-        return self.context.get_compiled_expr(expr)
-
-    def compile(self):
-        self._extract_subqueries()
-
-        extracted = self.format_subqueries()
-
-        buf = []
-
-        if extracted:
-            buf.append(f'WITH {extracted}')
-
-        buf.extend(
-            toolz.interleave(
-                (
-                    map(self.format_relation, self.tables),
-                    self._get_keyword_list(),
-                )
-            )
-        )
-        return '\n'.join(buf)
+        self.expr = expr
+        self.parent_table = parent_table
+
+        qroots = self.parent_table._root_tables()
+        self.query_roots = util.IbisSet.from_list(qroots)
+
+    def get_result(self):
+        self.foreign_table = None
+        self.predicates = []
+
+        self._visit(self.expr)
+
+        if type(self.expr.op()) == ops.Any:
+            op = ExistsSubquery(self.foreign_table, self.predicates)
+        else:
+            op = NotExistsSubquery(self.foreign_table, self.predicates)
+
+        return ir.BooleanArray(op)
+
+    def _visit(self, expr):
+        node = expr.op()
+
+        for arg in node.flat_args():
+            if isinstance(arg, ir.TableExpr):
+                self._visit_table(arg)
+            elif isinstance(arg, ir.BooleanArray):
+                for sub_expr in L.unwrap_ands(arg):
+                    self.predicates.append(sub_expr)
+                    self._visit(sub_expr)
+            elif isinstance(arg, ir.Expr):
+                self._visit(arg)
+            else:
+                continue
+
+    def _visit_table(self, expr):
+        node = expr.op()
+
+        if isinstance(expr, ir.TableExpr):
+            base_table = _find_blocking_table(expr)
+            if base_table is not None:
+                base_node = base_table.op()
+                if self._is_root(base_node):
+                    pass
+                else:
+                    # Foreign ref
+                    self.foreign_table = expr
+        else:
+            if not isinstance(node, ir.BlockingTableNode):
+                for arg in node.flat_args():
+                    if isinstance(arg, ir.Expr):
+                        self._visit(arg)
+
+    def _is_root(self, what):
+        if isinstance(what, ir.Expr):
+            what = what.op()
+        return what in self.query_roots
+
+
+def _find_blocking_table(expr):
+    node = expr.op()
+
+    if isinstance(node, ir.BlockingTableNode):
+        return expr
+
+    for arg in node.flat_args():
+        if isinstance(arg, ir.Expr):
+            result = _find_blocking_table(arg)
+            if result is not None:
+                return result
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/base/sql/compiler/select_builder.py` & `ibis-framework-v0.6.0/ibis/impala/tests/test_client.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,329 +1,336 @@
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import pandas as pd
+
+from ibis.compat import unittest
+from ibis.impala.tests.common import IbisTestEnv, ImpalaE2E, connect_test
+from ibis.tests.util import assert_equal
+import ibis
+
+import ibis.common as com
+import ibis.config as config
+import ibis.expr.types as ir
+import ibis.util as util
+
+
+def approx_equal(a, b, eps):
+    assert abs(a - b) < eps
+
+
+ENV = IbisTestEnv()
+
+
+class TestImpalaClient(ImpalaE2E, unittest.TestCase):
+
+    def test_execute_exprs_default_backend(self):
+        cases = [
+            (ibis.literal(2), 2)
+        ]
+
+        ibis.options.default_backend = None
+        client = connect_test(ENV, with_hdfs=False)
+        assert ibis.options.default_backend is client
+
+        for expr, expected in cases:
+            result = expr.execute()
+            assert result == expected
+
+    def test_raise_ibis_error_no_hdfs(self):
+        # #299
+        client = connect_test(ENV, with_hdfs=False)
+        self.assertRaises(com.IbisError, getattr, client, 'hdfs')
+
+    def test_get_table_ref(self):
+        table = self.db.functional_alltypes
+        assert isinstance(table, ir.TableExpr)
+
+        table = self.db['functional_alltypes']
+        assert isinstance(table, ir.TableExpr)
+
+    def test_run_sql(self):
+        query = """SELECT li.*
+FROM {0}.tpch_lineitem li
+""".format(self.test_data_db)
+        table = self.con.sql(query)
+
+        li = self.con.table('tpch_lineitem')
+        assert isinstance(table, ir.TableExpr)
+        assert_equal(table.schema(), li.schema())
+
+        expr = table.limit(10)
+        result = expr.execute()
+        assert len(result) == 10
+
+    def test_sql_with_limit(self):
+        query = """\
+SELECT *
+FROM functional_alltypes
+LIMIT 10"""
+        table = self.con.sql(query)
+        ex_schema = self.con.get_schema('functional_alltypes')
+        assert_equal(table.schema(), ex_schema)
+
+    def test_raw_sql(self):
+        query = 'SELECT * from functional_alltypes limit 10'
+        cur = self.con.raw_sql(query, results=True)
+        rows = cur.fetchall()
+        cur.release()
+        assert len(rows) == 10
+
+    def test_explain(self):
+        t = self.con.table('functional_alltypes')
+        expr = t.group_by('string_col').size()
+        result = self.con.explain(expr)
+        assert isinstance(result, str)
+
+    def test_get_schema(self):
+        t = self.con.table('tpch_lineitem')
+        schema = self.con.get_schema('tpch_lineitem',
+                                     database=self.test_data_db)
+        assert_equal(t.schema(), schema)
+
+    def test_result_as_dataframe(self):
+        expr = self.alltypes.limit(10)
+
+        ex_names = expr.schema().names
+        result = self.con.execute(expr)
+
+        assert isinstance(result, pd.DataFrame)
+        assert list(result.columns) == ex_names
+        assert len(result) == 10
+
+    def test_adapt_scalar_array_results(self):
+        table = self.alltypes
+
+        expr = table.double_col.sum()
+        result = self.con.execute(expr)
+        assert isinstance(result, float)
+
+        with config.option_context('interactive', True):
+            result2 = expr.execute()
+            assert isinstance(result2, float)
+
+        expr = (table.group_by('string_col')
+                .aggregate([table.count().name('count')])
+                .string_col)
+
+        result = self.con.execute(expr)
+        assert isinstance(result, pd.Series)
+
+    def test_interactive_repr_call_failure(self):
+        t = self.con.table('tpch_lineitem').limit(100000)
+
+        t = t[t, t.l_receiptdate.cast('timestamp').name('date')]
+
+        keys = [t.date.year().name('year'), 'l_linestatus']
+        filt = t.l_linestatus.isin(['F'])
+        expr = (t[filt]
+                .group_by(keys)
+                .aggregate(t.l_extendedprice.mean().name('avg_px')))
+
+        w2 = ibis.trailing_window(9, group_by=expr.l_linestatus,
+                                  order_by=expr.year)
+
+        metric = expr['avg_px'].mean().over(w2)
+        enriched = expr[expr, metric]
+        with config.option_context('interactive', True):
+            repr(enriched)
+
+    def test_array_default_limit(self):
+        t = self.alltypes
+
+        result = self.con.execute(t.float_col, limit=100)
+        assert len(result) == 100
+
+    def test_limit_overrides_expr(self):
+        # #418
+        t = self.alltypes
+        result = self.con.execute(t.limit(10), limit=5)
+        assert len(result) == 5
+
+    def test_limit_equals_none_no_limit(self):
+        t = self.alltypes
+
+        with config.option_context('sql.default_limit', 10):
+            result = t.execute(limit=None)
+            assert len(result) > 10
+
+    def test_verbose_log_queries(self):
+        queries = []
+
+        def logger(x):
+            queries.append(x)
+
+        with config.option_context('verbose', True):
+            with config.option_context('verbose_log', logger):
+                self.con.table('tpch_orders', database=self.test_data_db)
+
+        assert len(queries) == 1
+        expected = 'DESCRIBE {0}.`tpch_orders`'.format(self.test_data_db)
+        assert queries[0] == expected
+
+    def test_sql_query_limits(self):
+        table = self.con.table('tpch_nation', database=self.test_data_db)
+        with config.option_context('sql.default_limit', 100000):
+            # table has 25 rows
+            assert len(table.execute()) == 25
+            # comply with limit arg for TableExpr
+            assert len(table.execute(limit=10)) == 10
+            # state hasn't changed
+            assert len(table.execute()) == 25
+            # non-TableExpr ignores default_limit
+            assert table.count().execute() == 25
+            # non-TableExpr doesn't observe limit arg
+            assert table.count().execute(limit=10) == 25
+        with config.option_context('sql.default_limit', 20):
+            # TableExpr observes default limit setting
+            assert len(table.execute()) == 20
+            # explicit limit= overrides default
+            assert len(table.execute(limit=15)) == 15
+            assert len(table.execute(limit=23)) == 23
+            # non-TableExpr ignores default_limit
+            assert table.count().execute() == 25
+            # non-TableExpr doesn't observe limit arg
+            assert table.count().execute(limit=10) == 25
+        # eliminating default_limit doesn't break anything
+        with config.option_context('sql.default_limit', None):
+            assert len(table.execute()) == 25
+            assert len(table.execute(limit=15)) == 15
+            assert len(table.execute(limit=10000)) == 25
+            assert table.count().execute() == 25
+            assert table.count().execute(limit=10) == 25
+
+    def test_expr_compile_verify(self):
+        table = self.db.functional_alltypes
+        expr = table.double_col.sum()
+
+        assert isinstance(expr.compile(), str)
+        assert expr.verify()
+
+    def test_api_compile_verify(self):
+        t = self.db.functional_alltypes
+
+        s = t.string_col
+
+        supported = s.lower()
+        unsupported = s.replace('foo', 'bar')
+
+        assert ibis.impala.verify(supported)
+        assert not ibis.impala.verify(unsupported)
+
+    def test_database_repr(self):
+        assert self.test_data_db in repr(self.db)
+
+    def test_database_drop(self):
+        tmp_name = '__ibis_test_{0}'.format(util.guid())
+        self.con.create_database(tmp_name)
+
+        db = self.con.database(tmp_name)
+        self.temp_databases.append(tmp_name)
+        db.drop()
+        assert not self.con.exists_database(tmp_name)
+
+    def test_database_default_current_database(self):
+        db = self.con.database()
+        assert db.name == self.con.current_database
+
+    def test_namespace(self):
+        ns = self.db.namespace('tpch_')
+
+        assert 'tpch_' in repr(ns)
+
+        table = ns.lineitem
+        expected = self.db.tpch_lineitem
+        attrs = dir(ns)
+        assert 'lineitem' in attrs
+        assert 'functional_alltypes' not in attrs
+
+        assert_equal(table, expected)
+
+    def test_close_drops_temp_tables(self):
+        from posixpath import join as pjoin
+
+        hdfs_path = pjoin(self.test_data_dir, 'parquet/tpch_region')
+
+        client = connect_test(ENV)
+        table = client.parquet_file(hdfs_path)
+
+        name = table.op().name
+        assert self.con.exists_table(name) is True
+        client.close()
+
+        assert not self.con.exists_table(name)
+
+    def test_execute_async_simple(self):
+        t = self.db.functional_alltypes
+        expr = t.double_col.sum()
+
+        q = expr.execute(async=True)
+        result = q.get_result()
+        expected = expr.execute()
+        assert result == expected
+
+    def test_query_cancel(self):
+        import time
+        t = self.db.functional_alltypes
+
+        t2 = t.union(t).union(t)
+
+        # WM: this query takes about 90 seconds to execute for me locally, so
+        # I'm eyeballing an acceptable time frame for the cancel to work
+        expr = t2.join(t2).count()
+
+        start = time.clock()
+        q = expr.execute(async=True)
+        q.cancel()
+        end = time.clock()
+        elapsed = end - start
+        assert elapsed < 5
+
+        assert q.is_finished()
+
+    def test_set_compression_codec(self):
+        old_opts = self.con.get_options()
+        assert old_opts['COMPRESSION_CODEC'].upper() == 'NONE'
+
+        self.con.set_compression_codec('snappy')
+        opts = self.con.get_options()
+        assert opts['COMPRESSION_CODEC'].upper() == 'SNAPPY'
+
+        self.con.set_compression_codec(None)
+        opts = self.con.get_options()
+        assert opts['COMPRESSION_CODEC'].upper() == 'NONE'
+
+    def test_disable_codegen(self):
+        self.con.disable_codegen(False)
+        opts = self.con.get_options()
+        assert opts['DISABLE_CODEGEN'] == '0'
+
+        self.con.disable_codegen()
+        opts = self.con.get_options()
+        assert opts['DISABLE_CODEGEN'] == '1'
+
+        impala_con = self.con.con
+        cur1 = impala_con.execute('SET')
+        cur2 = impala_con.execute('SET')
 
-import functools
-from collections.abc import Mapping
-from typing import NamedTuple
-
-import ibis.expr.analysis as an
-import ibis.expr.datatypes as dt
-import ibis.expr.operations as ops
-
-
-class _LimitSpec(NamedTuple):
-    n: int
-    offset: int
-
-
-def _get_scalar(field):
-    def scalar_handler(results):
-        return results[field][0]
-
-    return scalar_handler
-
-
-def _get_column(name):
-    def column_handler(results):
-        return results[name]
-
-    return column_handler
-
-
-class SelectBuilder:
-    """Transforms expression IR to a query pipeline.
-
-    There will typically be a primary SELECT query, perhaps with some
-    subqueries and other DDL to ingest and tear down intermediate data sources.
-
-    Walks the expression tree and catalogues distinct query units,
-    builds select statements (and other DDL types, where necessary), and
-    records relevant query unit aliases to be used when actually
-    generating SQL.
-    """
-
-    def to_select(
-        self,
-        select_class,
-        table_set_formatter_class,
-        node,
-        context,
-        translator_class,
-    ):
-        self.select_class = select_class
-        self.table_set_formatter_class = table_set_formatter_class
-        self.context = context
-        self.translator_class = translator_class
-
-        self.op = node.to_expr().as_table().op()
-        assert isinstance(self.op, ops.Node), type(self.op)
-
-        self.table_set = None
-        self.select_set = None
-        self.group_by = None
-        self.having = None
-        self.filters = []
-        self.limit = None
-        self.order_by = []
-        self.subqueries = []
-        self.distinct = False
-
-        select_query = self._build_result_query()
-
-        self.queries = [select_query]
-
-        return select_query
-
-    def _build_result_query(self):
-        self._collect_elements()
-        self._analyze_subqueries()
-        self._populate_context()
-
-        return self.select_class(
-            self.table_set,
-            list(self.select_set),
-            translator_class=self.translator_class,
-            table_set_formatter_class=self.table_set_formatter_class,
-            context=self.context,
-            subqueries=self.subqueries,
-            where=self.filters,
-            group_by=self.group_by,
-            having=self.having,
-            limit=self.limit,
-            order_by=self.order_by,
-            distinct=self.distinct,
-            parent_op=self.op,
-        )
-
-    def _populate_context(self):
-        # Populate aliases for the distinct relations used to output this
-        # select statement.
-        if self.table_set is not None:
-            self._make_table_aliases(self.table_set)
-
-    # TODO(kszucs): should be rewritten using lin.traverse()
-    def _make_table_aliases(self, node):
-        ctx = self.context
-
-        if isinstance(node, ops.Join):
-            for arg in node.args:
-                if isinstance(arg, ops.TableNode):
-                    self._make_table_aliases(arg)
-        elif not ctx.is_extracted(node):
-            ctx.make_alias(node)
-        else:
-            # The compiler will apply a prefix only if the current context
-            # contains two or more table references. So, if we've extracted
-            # a subquery into a CTE, we need to propagate that reference
-            # down to child contexts so that they aren't missing any refs.
-            ctx.set_ref(node, ctx.top_context.get_ref(node))
-
-    # ---------------------------------------------------------------------
-    # Analysis of table set
-
-    def _collect_elements(self):
-        # If expr is a Value, we must seek out the Tables that it
-        # references, build their ASTs, and mark them in our QueryContext
-
-        # For now, we need to make the simplifying assumption that a value
-        # expression that is being translated only depends on a single table
-        # expression.
-
-        if isinstance(self.op, ops.TableNode):
-            self._collect(self.op, toplevel=True)
-        else:
-            self.select_set = [self.op]
-
-    def _collect(self, op, toplevel=False):
-        method = f'_collect_{type(op).__name__}'
-
-        if hasattr(self, method):
-            f = getattr(self, method)
-            f(op, toplevel=toplevel)
-        elif isinstance(op, (ops.PhysicalTable, ops.SQLQueryResult)):
-            self._collect_PhysicalTable(op, toplevel=toplevel)
-        elif isinstance(op, ops.Join):
-            self._collect_Join(op, toplevel=toplevel)
-        else:
-            raise NotImplementedError(type(op))
-
-    def _collect_Distinct(self, op, toplevel=False):
-        if toplevel:
-            self.distinct = True
-
-        self._collect(op.table, toplevel=toplevel)
-
-    def _collect_DropNa(self, op, toplevel=False):
-        if toplevel:
-            if op.subset is None:
-                columns = [
-                    ops.TableColumn(op.table, name) for name in op.table.schema.names
-                ]
-            else:
-                columns = op.subset
-            if columns:
-                filters = [
-                    functools.reduce(
-                        ops.And if op.how == "any" else ops.Or,
-                        [ops.NotNull(c) for c in columns],
-                    )
-                ]
-            elif op.how == "all":
-                filters = [ops.Literal(False, dtype=dt.bool)]
-            else:
-                filters = []
-            self.table_set = op.table
-            self.select_set = [op.table]
-            self.filters = filters
-
-    def _collect_FillNa(self, op, toplevel=False):
-        if toplevel:
-            table = op.table.to_expr()
-            if isinstance(op.replacements, Mapping):
-                mapping = op.replacements
-            else:
-                mapping = {
-                    name: op.replacements
-                    for name, type in table.schema().items()
-                    if type.nullable
-                }
-            new_op = table.mutate(
-                [
-                    table[name].fillna(value).name(name)
-                    for name, value in mapping.items()
-                ]
-            ).op()
-            self._collect(new_op, toplevel=toplevel)
-
-    def _collect_Limit(self, op, toplevel=False):
-        if not toplevel:
-            return
-
-        n = op.n
-        offset = op.offset or 0
-
-        if self.limit is None:
-            self.limit = _LimitSpec(n, offset)
-        else:
-            self.limit = _LimitSpec(
-                min(n, self.limit.n),
-                offset + self.limit.offset,
-            )
-
-        self._collect(op.table, toplevel=toplevel)
-
-    def _collect_Union(self, op, toplevel=False):
-        if toplevel:
-            self.table_set = op
-            self.select_set = [op]
-
-    def _collect_Difference(self, op, toplevel=False):
-        if toplevel:
-            self.table_set = op
-            self.select_set = [op]
-
-    def _collect_Intersection(self, op, toplevel=False):
-        if toplevel:
-            self.table_set = op
-            self.select_set = [op]
-
-    def _collect_Aggregation(self, op, toplevel=False):
-        # The select set includes the grouping keys (if any), and these are
-        # duplicated in the group_by set. SQL translator can decide how to
-        # format these depending on the database. Most likely the
-        # GROUP BY 1, 2, ... style
-        if toplevel:
-            sub_op = an.substitute_parents(op)
-
-            self.group_by = self._convert_group_by(sub_op.by)
-            self.having = sub_op.having
-            self.select_set = sub_op.by + sub_op.metrics
-            self.table_set = sub_op.table
-            self.filters = sub_op.predicates
-            self.order_by = sub_op.sort_keys
-
-            self._collect(op.table)
-
-    def _collect_Selection(self, op, toplevel=False):
-        table = op.table
-
-        if toplevel:
-            if isinstance(table, ops.Join):
-                self._collect_Join(table)
-            else:
-                self._collect(table)
-
-            selections = op.selections
-            sort_keys = op.sort_keys
-            filters = op.predicates
-
-            if not selections:
-                # select *
-                selections = [table]
-
-            self.order_by = sort_keys
-            self.select_set = selections
-            self.table_set = table
-            self.filters = filters
-
-    def _collect_InMemoryTable(self, node, toplevel=False):
-        if toplevel:
-            self.select_set = [node]
-            self.table_set = node
-
-    def _convert_group_by(self, nodes):
-        return list(range(len(nodes)))
-
-    def _collect_Join(self, op, toplevel=False):
-        if toplevel:
-            subbed = an.substitute_parents(op)
-            self.table_set = subbed
-            self.select_set = [subbed]
-
-    def _collect_PhysicalTable(self, op, toplevel=False):
-        if toplevel:
-            self.select_set = [op]
-            self.table_set = op
-
-    def _collect_DummyTable(self, op, toplevel=False):
-        if toplevel:
-            self.select_set = list(op.values)
-            self.table_set = None
-
-    def _collect_SelfReference(self, op, toplevel=False):
-        if toplevel:
-            self._collect(op.table, toplevel=toplevel)
-
-    # --------------------------------------------------------------------
-    # Subquery analysis / extraction
-
-    def _analyze_subqueries(self):
-        # Somewhat temporary place for this. A little bit tricky, because
-        # subqueries can be found in many places
-        # - With the table set
-        # - Inside the where clause (these may be able to place directly, some
-        #   cases not)
-        # - As support queries inside certain expressions (possibly needing to
-        #   be extracted and joined into the table set where they are
-        #   used). More complex transformations should probably not occur here,
-        #   though.
-        #
-        # Duplicate subqueries might appear in different parts of the query
-        # structure, e.g. beneath two aggregates that are joined together, so
-        # we have to walk the entire query structure.
-        #
-        # The default behavior is to only extract into a WITH clause when a
-        # subquery appears multiple times (for DRY reasons). At some point we
-        # can implement a more aggressive policy so that subqueries always
-        # appear in the WITH part of the SELECT statement, if that's what you
-        # want.
-
-        # Find the subqueries, and record them in the passed query context.
-        subqueries = an.find_subqueries(
-            [self.table_set, *self.filters], min_dependents=2
-        )
-
-        self.subqueries = []
-        for node in subqueries:
-            # See #173. Might have been extracted already in a parent context.
-            if not self.context.is_extracted(node):
-                self.subqueries.append(node)
-                self.context.set_extracted(node)
+        opts1 = dict(cur1.fetchall())
+        cur1.release()
+
+        opts2 = dict(cur2.fetchall())
+        cur2.release()
+
+        assert opts1['DISABLE_CODEGEN'] == '1'
+        assert opts2['DISABLE_CODEGEN'] == '1'
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/base/sql/ddl.py` & `ibis-framework-v0.6.0/ibis/expr/datatypes.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,469 +1,495 @@
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import re
+import six
 
-import sqlglot as sg
+import ibis.expr.types as ir
+import ibis.common as com
+import ibis.util as util
 
-import ibis.expr.datatypes as dt
-import ibis.expr.schema as sch
-from ibis.backends.base.sql.compiler import DDL, DML
-from ibis.backends.base.sql.registry import quote_identifier, type_to_sql_string
+if six.PY3:
+    from io import StringIO
+else:
+    from io import BytesIO as StringIO
 
-fully_qualified_re = re.compile(r"(.*)\.(?:`(.*)`|(.*))")
-_format_aliases = {'TEXT': 'TEXTFILE'}
 
+class Schema(object):
 
-def _sanitize_format(format):
-    if format is None:
-        return None
-    format = format.upper()
-    format = _format_aliases.get(format, format)
-    if format not in ('PARQUET', 'AVRO', 'TEXTFILE'):
-        raise ValueError(f'Invalid format: {format!r}')
+    """
+    Holds table schema information
+    """
 
-    return format
+    def __init__(self, names, types):
+        if not isinstance(names, list):
+            names = list(names)
+        self.names = names
+        self.types = [validate_type(x) for x in types]
 
+        self._name_locs = dict((v, i) for i, v in enumerate(self.names))
 
-def is_fully_qualified(x):
-    return bool(fully_qualified_re.search(x))
+        if len(self._name_locs) < len(self.names):
+            raise com.IntegrityError('Duplicate column names')
 
+    def __repr__(self):
+        return self._repr()
 
-def _is_quoted(x):
-    regex = re.compile(r"(?:`(.*)`|(.*))")
-    quoted, _ = regex.match(x).groups()
-    return quoted is not None
+    def __len__(self):
+        return len(self.names)
 
+    def __iter__(self):
+        return iter(self.names)
 
-def format_schema(schema):
-    elements = [
-        _format_schema_element(name, t) for name, t in zip(schema.names, schema.types)
-    ]
-    return '({})'.format(',\n '.join(elements))
+    def _repr(self):
+        buf = StringIO()
+        space = 2 + max(len(x) for x in self.names)
+        for name, tipo in zip(self.names, self.types):
+            buf.write('\n{0}{1}'.format(name.ljust(space), str(tipo)))
 
+        return "ibis.Schema {{{0}\n}}".format(util.indent(buf.getvalue(), 2))
 
-def _format_schema_element(name, t):
-    return f'{quote_identifier(name, force=True)} {type_to_sql_string(t)}'
+    def __contains__(self, name):
+        return name in self._name_locs
 
+    def __getitem__(self, name):
+        return self.types[self._name_locs[name]]
 
-def _format_partition_kv(k, v, type):
-    if type == dt.string:
-        value_formatted = f'"{v}"'
-    else:
-        value_formatted = str(v)
+    def delete(self, names_to_delete):
+        for name in names_to_delete:
+            if name not in self:
+                raise KeyError(name)
 
-    return f'{k}={value_formatted}'
+        new_names, new_types = [], []
+        for name, type_ in zip(self.names, self.types):
+            if name in names_to_delete:
+                continue
+            new_names.append(name)
+            new_types.append(type_)
 
+        return Schema(new_names, new_types)
 
-def format_partition(partition, partition_schema):
-    tokens = []
-    if isinstance(partition, dict):
-        for name in partition_schema:
-            if name in partition:
-                tok = _format_partition_kv(
-                    name, partition[name], partition_schema[name]
-                )
-            else:
-                # dynamic partitioning
-                tok = name
-            tokens.append(tok)
-    else:
-        for name, value in zip(partition_schema, partition):
-            tok = _format_partition_kv(name, value, partition_schema[name])
-            tokens.append(tok)
+    @classmethod
+    def from_tuples(cls, values):
+        if not isinstance(values, (list, tuple)):
+            values = list(values)
+
+        if len(values):
+            names, types = zip(*values)
+        else:
+            names, types = [], []
+        return Schema(names, types)
 
-    return 'PARTITION ({})'.format(', '.join(tokens))
+    @classmethod
+    def from_dict(cls, values):
+        names = list(values.keys())
+        types = values.values()
+        return Schema(names, types)
 
+    def equals(self, other):
+        return ((self.names == other.names) and
+                (self.types == other.types))
 
-def _format_properties(props):
-    tokens = []
-    for k, v in sorted(props.items()):
-        tokens.append(f"  '{k}'='{v}'")
+    def __eq__(self, other):
+        return self.equals(other)
 
-    return '(\n{}\n)'.format(',\n'.join(tokens))
+    def get_type(self, name):
+        return self.types[self._name_locs[name]]
 
+    def append(self, schema):
+        names = self.names + schema.names
+        types = self.types + schema.types
+        return Schema(names, types)
 
-def format_tblproperties(props):
-    formatted_props = _format_properties(props)
-    return f'TBLPROPERTIES {formatted_props}'
+    def items(self):
+        return zip(self.names, self.types)
 
 
-def _serdeproperties(props):
-    formatted_props = _format_properties(props)
-    return f'SERDEPROPERTIES {formatted_props}'
+class HasSchema(object):
 
+    """
+    Base class representing a structured dataset with a well-defined
+    schema.
 
-class _BaseQualifiedSQLStatement:
-    def _get_scoped_name(self, obj_name, database):
-        if is_fully_qualified(obj_name):
-            return obj_name
-        if _is_quoted(obj_name):
-            obj_name = obj_name[1:-1]
-        return sg.table(obj_name, db=database, quoted=True).sql(dialect="hive")
+    Base implementation is for tables that do not reference a particular
+    concrete dataset or database table.
+    """
 
+    def __init__(self, schema, name=None):
+        assert isinstance(schema, Schema)
+        self._schema = schema
+        self._name = name
 
-class BaseDDL(DDL, _BaseQualifiedSQLStatement):
-    pass
+    def __repr__(self):
+        return self._repr()
 
+    def _repr(self):
+        return "%s(%s)" % (type(self).__name__, repr(self.schema))
 
-class _BaseDML(DML, _BaseQualifiedSQLStatement):
-    pass
+    @property
+    def schema(self):
+        return self._schema
 
+    def get_schema(self):
+        return self._schema
 
-class _CreateDDL(BaseDDL):
-    def _if_exists(self):
-        return 'IF NOT EXISTS ' if self.can_exist else ''
-
-
-class CreateTable(_CreateDDL):
-    def __init__(
-        self,
-        table_name,
-        database=None,
-        external=False,
-        format='parquet',
-        can_exist=False,
-        partition=None,
-        path=None,
-        tbl_properties=None,
-    ):
-        self.table_name = table_name
-        self.database = database
-        self.partition = partition
-        self.path = path
-        self.external = external
-        self.can_exist = can_exist
-        self.format = _sanitize_format(format)
-        self.tbl_properties = tbl_properties
+    def has_schema(self):
+        return True
 
     @property
-    def _prefix(self):
-        if self.external:
-            return 'CREATE EXTERNAL TABLE'
-        else:
-            return 'CREATE TABLE'
+    def name(self):
+        return self._name
 
-    def _create_line(self):
-        scoped_name = self._get_scoped_name(self.table_name, self.database)
-        return f'{self._prefix} {self._if_exists()}{scoped_name}'
-
-    def _location(self):
-        return f"LOCATION '{self.path}'" if self.path else None
-
-    def _storage(self):
-        # By the time we're here, we have a valid format
-        return f'STORED AS {self.format}'
+    def equals(self, other):
+        if type(self) != type(other):
+            return False
+        return self.schema.equals(other.schema)
 
-    @property
-    def pieces(self):
-        yield self._create_line()
-        yield from filter(None, self._pieces)
-
-    def compile(self):
-        return '\n'.join(self.pieces)
-
-
-class CTAS(CreateTable):
-    """Create Table As Select."""
-
-    def __init__(
-        self,
-        table_name,
-        select,
-        database=None,
-        external=False,
-        format='parquet',
-        can_exist=False,
-        path=None,
-        partition=None,
-    ):
-        super().__init__(
-            table_name,
-            database=database,
-            external=external,
-            format=format,
-            can_exist=can_exist,
-            path=path,
-            partition=partition,
-        )
-        self.select = select
+    def root_tables(self):
+        return [self]
 
-    @property
-    def _pieces(self):
-        yield self._partitioned_by()
-        yield self._storage()
-        yield self._location()
-        yield 'AS'
-        yield self.select.compile()
-
-    def _partitioned_by(self):
-        if self.partition is not None:
-            return 'PARTITIONED BY ({})'.format(
-                ', '.join(quote_identifier(expr.get_name()) for expr in self.partition)
-            )
-        return None
 
+class DataType(object):
 
-class CreateView(CTAS):
-    """Create a view."""
+    def __init__(self, nullable=True):
+        self.nullable = nullable
 
-    def __init__(self, table_name, select, database=None, can_exist=False):
-        super().__init__(table_name, select, database=database, can_exist=can_exist)
+    def __call__(self, nullable=True):
+        return self._factory(nullable=nullable)
 
-    @property
-    def _pieces(self):
-        yield 'AS'
-        yield self.select.compile()
+    def _factory(self, nullable=True):
+        return type(self)(nullable=nullable)
 
-    @property
-    def _prefix(self):
-        return 'CREATE VIEW'
+    def __eq__(self, other):
+        return self.equals(other)
 
+    def __ne__(self, other):
+        return not (self == other)
 
-class CreateTableWithSchema(CreateTable):
-    def __init__(self, table_name, schema, table_format=None, **kwargs):
-        super().__init__(table_name, **kwargs)
-        self.schema = schema
-        self.table_format = table_format
+    def __hash__(self):
+        return hash(type(self))
+
+    def __repr__(self):
+        name = self.name()
+        if not self.nullable:
+            name = '{0}[non-nullable]'.format(name)
+        return name
+
+    def name(self):
+        return type(self).__name__.lower()
+
+    def equals(self, other):
+        if isinstance(other, six.string_types):
+            other = validate_type(other)
+
+        return (isinstance(other, type(self)) and
+                self.nullable == other.nullable)
+
+    def can_implicit_cast(self, other):
+        return self.equals(other)
+
+    def scalar_type(self):
+        name = type(self).__name__
+        return getattr(ir, '{0}Scalar'.format(name))
+
+    def array_type(self):
+        name = type(self).__name__
+        return getattr(ir, '{0}Array'.format(name))
+
+
+class Any(DataType):
+    pass
 
-    @property
-    def _pieces(self):
-        if self.partition is not None:
-            main_schema = self.schema
-            part_schema = self.partition
-            if not isinstance(part_schema, sch.Schema):
-                part_fields = {name: self.schema[name] for name in part_schema}
-                part_schema = sch.Schema(part_fields)
-
-            to_delete = {name for name in self.partition if name in self.schema}
-            fields = {
-                name: dtype
-                for name, dtype in main_schema.items()
-                if name not in to_delete
-            }
-            main_schema = sch.Schema(fields)
 
-            yield format_schema(main_schema)
-            yield f'PARTITIONED BY {format_schema(part_schema)}'
+class Primitive(DataType):
+    pass
+
+
+class Null(DataType):
+    pass
+
+
+class Variadic(DataType):
+    pass
+
+
+class Boolean(Primitive):
+    pass
+
+
+class Integer(Primitive):
+
+    def can_implicit_cast(self, other):
+        if isinstance(other, Integer):
+            return ((type(self) == Integer) or
+                    (other._nbytes <= self._nbytes))
         else:
-            yield format_schema(self.schema)
+            return False
+
+
+class String(Variadic):
+    pass
+
+
+class Timestamp(Primitive):
+    pass
+
+
+class SignedInteger(Integer):
+    pass
+
 
-        if self.table_format is not None:
-            yield '\n'.join(self.table_format.to_ddl())
+class Floating(Primitive):
+
+    def can_implicit_cast(self, other):
+        if isinstance(other, Integer):
+            return True
+        elif isinstance(other, Floating):
+            # return other._nbytes <= self._nbytes
+            return True
         else:
-            yield self._storage()
+            return False
 
-        yield self._location()
 
+class Int8(Integer):
 
-class CreateDatabase(_CreateDDL):
-    def __init__(self, name, path=None, can_exist=False):
-        self.name = name
-        self.path = path
-        self.can_exist = can_exist
+    _nbytes = 1
+    bounds = (-128, 127)
 
-    def compile(self):
-        name = quote_identifier(self.name)
 
-        create_decl = 'CREATE DATABASE'
-        create_line = f'{create_decl} {self._if_exists()}{name}'
-        if self.path is not None:
-            create_line += f"\nLOCATION '{self.path}'"
+class Int16(Integer):
 
-        return create_line
+    _nbytes = 2
+    bounds = (-32768, 32767)
 
 
-class DropObject(BaseDDL):
-    def __init__(self, must_exist=True):
-        self.must_exist = must_exist
+class Int32(Integer):
 
-    def compile(self):
-        if_exists = '' if self.must_exist else 'IF EXISTS '
-        object_name = self._object_name()
-        return f'DROP {self._object_type} {if_exists}{object_name}'
+    _nbytes = 4
+    bounds = (-2147483648, 2147483647)
 
 
-class DropDatabase(DropObject):
-    _object_type = 'DATABASE'
+class Int64(Integer):
 
-    def __init__(self, name, must_exist=True):
-        super().__init__(must_exist=must_exist)
-        self.name = name
+    _nbytes = 8
+    bounds = (-9223372036854775808, 9223372036854775807)
 
-    def _object_name(self):
-        return self.name
 
+class Float(Floating):
 
-class DropTable(DropObject):
-    _object_type = 'TABLE'
+    _nbytes = 4
 
-    def __init__(self, table_name, database=None, must_exist=True):
-        super().__init__(must_exist=must_exist)
-        self.table_name = table_name
-        self.database = database
 
-    def _object_name(self):
-        return self._get_scoped_name(self.table_name, self.database)
+class Double(Floating):
 
+    _nbytes = 8
 
-class DropView(DropTable):
-    _object_type = 'VIEW'
 
+class Decimal(DataType):
+    # Decimal types are parametric, we store the parameters in this object
 
-class TruncateTable(BaseDDL):
-    _object_type = 'TABLE'
+    def __init__(self, precision, scale, nullable=True):
+        self.precision = precision
+        self.scale = scale
+        DataType.__init__(self, nullable=nullable)
 
-    def __init__(self, table_name, database=None):
-        self.table_name = table_name
-        self.database = database
+    def _base_type(self):
+        return 'decimal'
 
-    def compile(self):
-        name = self._get_scoped_name(self.table_name, self.database)
-        return f'TRUNCATE TABLE {name}'
+    def __repr__(self):
+        return ('decimal(precision=%s, scale=%s)'
+                % (self.precision, self.scale))
 
+    def __hash__(self):
+        return hash((self.precision, self.scale))
 
-class InsertSelect(_BaseDML):
-    def __init__(
-        self,
-        table_name,
-        select_expr,
-        database=None,
-        partition=None,
-        partition_schema=None,
-        overwrite=False,
-    ):
-        self.table_name = table_name
-        self.database = database
-        self.select = select_expr
+    def __ne__(self, other):
+        return not self.__eq__(other)
 
-        self.partition = partition
-        self.partition_schema = partition_schema
+    def __eq__(self, other):
+        if not isinstance(other, Decimal):
+            return False
 
-        self.overwrite = overwrite
+        return (self.precision == other.precision and
+                self.scale == other.scale)
 
-    def compile(self):
-        if self.overwrite:
-            cmd = 'INSERT OVERWRITE'
-        else:
-            cmd = 'INSERT INTO'
+    @classmethod
+    def can_implicit_cast(cls, other):
+        return isinstance(other, (Floating, Decimal))
 
-        if self.partition is not None:
-            part = format_partition(self.partition, self.partition_schema)
-            partition = f' {part} '
-        else:
-            partition = ''
+    def array_type(self):
+        def constructor(op, name=None):
+            from ibis.expr.types import DecimalArray
+            return DecimalArray(op, self, name=name)
+        return constructor
+
+    def scalar_type(self):
+        def constructor(op, name=None):
+            from ibis.expr.types import DecimalScalar
+            return DecimalScalar(op, self, name=name)
+        return constructor
+
+
+class Category(DataType):
 
-        select_query = self.select.compile()
-        scoped_name = self._get_scoped_name(self.table_name, self.database)
-        return f'{cmd} {scoped_name}{partition}\n{select_query}'
-
-
-class AlterTable(BaseDDL):
-    def __init__(
-        self,
-        table,
-        location=None,
-        format=None,
-        tbl_properties=None,
-        serde_properties=None,
-    ):
-        self.table = table
-        self.location = location
-        self.format = _sanitize_format(format)
-        self.tbl_properties = tbl_properties
-        self.serde_properties = serde_properties
-
-    def _wrap_command(self, cmd):
-        return f'ALTER TABLE {cmd}'
-
-    def _format_properties(self, prefix=''):
-        tokens = []
-
-        if self.location is not None:
-            tokens.append(f"LOCATION '{self.location}'")
-
-        if self.format is not None:
-            tokens.append(f"FILEFORMAT {self.format}")
+    def __init__(self, cardinality=None, nullable=True):
+        self.cardinality = cardinality
+        DataType.__init__(self, nullable=nullable)
 
-        if self.tbl_properties is not None:
-            tokens.append(format_tblproperties(self.tbl_properties))
+    def _base_type(self):
+        return 'category'
 
-        if self.serde_properties is not None:
-            tokens.append(_serdeproperties(self.serde_properties))
+    def __repr__(self):
+        card = (self.cardinality if self.cardinality is not None
+                else 'unknown')
+        return ('category(K=%s)' % card)
 
-        if len(tokens) > 0:
-            return '\n{}{}'.format(prefix, '\n'.join(tokens))
+    def __hash__(self):
+        return hash((self.cardinality))
+
+    def __eq__(self, other):
+        if not isinstance(other, Category):
+            return False
+
+        return self.cardinality == other.cardinality
+
+    def to_integer_type(self):
+        if self.cardinality is None:
+            return 'int64'
+        elif self.cardinality < (2 ** 7 - 1):
+            return 'int8'
+        elif self.cardinality < (2 ** 15 - 1):
+            return 'int16'
+        elif self.cardinality < (2 ** 31 - 1):
+            return 'int32'
         else:
-            return ''
+            return 'int64'
+
+    def array_type(self):
+        def constructor(op, name=None):
+            from ibis.expr.types import CategoryArray
+            return CategoryArray(op, self, name=name)
+        return constructor
+
+    def scalar_type(self):
+        def constructor(op, name=None):
+            from ibis.expr.types import CategoryScalar
+            return CategoryScalar(op, self, name=name)
+        return constructor
+
+
+class Struct(DataType):
+
+    def __init__(self, names, types, nullable=True):
+        DataType.__init__(self, nullable=nullable)
+
+
+class Array(Variadic):
+
+    def __init__(self, value_type, nullable=True):
+        Variadic.__init__(self, nullable=nullable)
+
+
+class Enum(DataType):
+
+    def __init__(self, rep_type, value_type, nullable=True):
+        DataType.__init__(self, nullable=nullable)
+
+
+class Map(DataType):
+
+    def __init__(self, key_type, value_type, nullable=True):
+        DataType.__init__(self, nullable=nullable)
+
+
+# ---------------------------------------------------------------------
+
+
+any = Any()
+null = Null()
+boolean = Boolean()
+int_ = Integer()
+int8 = Int8()
+int16 = Int16()
+int32 = Int32()
+int64 = Int64()
+float = Float()
+double = Double()
+string = String()
+timestamp = Timestamp()
+
+
+_primitive_types = {
+    'any': any,
+    'null': null,
+    'boolean': boolean,
+    'int8': int8,
+    'int16': int16,
+    'int32': int32,
+    'int64': int64,
+    'float': float,
+    'double': double,
+    'string': string,
+    'timestamp': timestamp
+}
+
+
+def validate_type(t):
+    if isinstance(t, DataType):
+        return t
+
+    parsed_type = _parse_type(t)
+    if parsed_type is not None:
+        return parsed_type
+
+    if t in _primitive_types:
+        return _primitive_types[t]
+    else:
+        raise ValueError('Invalid type: %s' % repr(t))
+
+
+_DECIMAL_RE = re.compile('decimal\((\d+),[\s]*(\d+)\)')
+
+
+def _parse_decimal(t):
+    m = _DECIMAL_RE.match(t)
+    if m:
+        precision, scale = m.groups()
+        return Decimal(int(precision), int(scale))
+
+    if t == 'decimal':
+        # From the Impala documentation
+        return Decimal(9, 0)
+
+
+_type_parsers = [
+    _parse_decimal
+]
+
+
+def _parse_type(t):
+    for parse_fn in _type_parsers:
+        parsed = parse_fn(t)
+        if parsed is not None:
+            return parsed
+    return None
+
+
+def array_type(t):
+    # compatibility
+    return validate_type(t).array_type()
+
 
-    def compile(self):
-        props = self._format_properties()
-        action = f'{self.table} SET {props}'
-        return self._wrap_command(action)
-
-
-class DropFunction(DropObject):
-    def __init__(self, name, inputs, must_exist=True, aggregate=False, database=None):
-        super().__init__(must_exist=must_exist)
-        self.name = name
-        self.inputs = tuple(map(dt.dtype, inputs))
-        self.must_exist = must_exist
-        self.aggregate = aggregate
-        self.database = database
-
-    def _object_name(self):
-        return self.name
-
-    def compile(self):
-        tokens = ['DROP']
-        if self.aggregate:
-            tokens.append('AGGREGATE')
-        tokens.append('FUNCTION')
-        if not self.must_exist:
-            tokens.append('IF EXISTS')
-
-        tokens.append(self._impala_signature())
-        return ' '.join(tokens)
-
-
-class RenameTable(AlterTable):
-    def __init__(
-        self,
-        old_name: str,
-        new_name: str,
-        old_database: str | None = None,
-        new_database: str | None = None,
-        dialect: str = "hive",
-    ):
-        self._old = sg.table(old_name, db=old_database, quoted=True).sql(
-            dialect=dialect
-        )
-        self._new = sg.table(new_name, db=new_database, quoted=True).sql(
-            dialect=dialect
-        )
-
-    def compile(self):
-        return self._wrap_command(f"{self._old} RENAME TO {self._new}")
-
-
-__all__ = (
-    'fully_qualified_re',
-    'is_fully_qualified',
-    'format_schema',
-    'format_partition',
-    'format_tblproperties',
-    'BaseDDL',
-    'CreateTable',
-    'CTAS',
-    'CreateView',
-    'CreateTableWithSchema',
-    'CreateDatabase',
-    'DropObject',
-    'DropDatabase',
-    'DropTable',
-    'DropView',
-    'TruncateTable',
-    'InsertSelect',
-    'AlterTable',
-    'DropFunction',
-    'RenameTable',
-)
+def scalar_type(t):
+    # compatibility
+    return validate_type(t).scalar_type()
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/base/sql/registry/identifiers.py` & `ibis-framework-v0.6.0/ibis/impala/identifiers.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,26 +1,24 @@
-from __future__ import annotations
-
 # Copyright 2014 Cloudera Inc.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 # http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-# Base identifiers
+# Impala identifiers
 
-base_identifiers = [
+impala_identifiers = [
     'add',
     'aggregate',
     'all',
     'alter',
     'and',
     'api_version',
     'as',
@@ -157,9 +155,9 @@
     'update_fn',
     'use',
     'using',
     'values',
     'view',
     'when',
     'where',
-    'with',
+    'with'
 ]
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/bigquery/__init__.py` & `ibis-framework-v0.6.0/ibis/impala/ddl.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,719 +1,811 @@
-"""BigQuery public API."""
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from __future__ import annotations
+from ibis.compat import StringIO
+import re
 
-import contextlib
-import warnings
-from typing import TYPE_CHECKING, Any, Callable, Iterable, Mapping
-from urllib.parse import parse_qs, urlparse
-
-import google.auth.credentials
-import google.cloud.bigquery as bq
-import google.cloud.bigquery_storage_v1 as bqstorage
-import pandas as pd
-import pydata_google_auth
-from pydata_google_auth import cache
-
-import ibis
-import ibis.common.exceptions as com
-import ibis.expr.operations as ops
-import ibis.expr.types as ir
-from ibis.backends.base import CanCreateSchema, CanListDatabases, Database
-from ibis.backends.base.sql import BaseSQLBackend
-from ibis.backends.bigquery.client import (
-    BigQueryCursor,
-    bigquery_param,
-    parse_project_and_dataset,
-    rename_partitioned_column,
-    schema_from_bigquery_table,
-)
-from ibis.backends.bigquery.compiler import BigQueryCompiler
-from ibis.backends.bigquery.datatypes import BigQuerySchema, BigQueryType
-from ibis.formats.pandas import PandasData
-
-with contextlib.suppress(ImportError):
-    from ibis.backends.bigquery.udf import udf  # noqa: F401
-
-if TYPE_CHECKING:
-    import pyarrow as pa
-    from google.cloud.bigquery.table import RowIterator
-
-SCOPES = ["https://www.googleapis.com/auth/bigquery"]
-EXTERNAL_DATA_SCOPES = [
-    "https://www.googleapis.com/auth/bigquery",
-    "https://www.googleapis.com/auth/cloud-platform",
-    "https://www.googleapis.com/auth/drive",
-]
-CLIENT_ID = "546535678771-gvffde27nd83kfl6qbrnletqvkdmsese.apps.googleusercontent.com"
-CLIENT_SECRET = "iU5ohAF2qcqrujegE3hQ1cPt"
-
-
-def _create_user_agent(application_name: str) -> str:
-    user_agent = []
-
-    if application_name:
-        user_agent.append(application_name)
-
-    user_agent_default_template = f"ibis/{ibis.__version__}"
-    user_agent.append(user_agent_default_template)
-
-    return " ".join(user_agent)
-
-
-def _create_client_info(application_name):
-    from google.api_core.client_info import ClientInfo
-
-    return ClientInfo(user_agent=_create_user_agent(application_name))
-
-
-def _create_client_info_gapic(application_name):
-    from google.api_core.gapic_v1.client_info import ClientInfo
-
-    return ClientInfo(user_agent=_create_user_agent(application_name))
-
-
-class Backend(BaseSQLBackend, CanCreateSchema, CanListDatabases):
-    name = "bigquery"
-    compiler = BigQueryCompiler
-    supports_in_memory_tables = False
-    supports_python_udfs = False
-
-    def _from_url(self, url: str, **kwargs):
-        result = urlparse(url)
-        params = parse_qs(result.query)
-        return self.connect(
-            project_id=result.netloc or params.get("project_id", [""])[0],
-            dataset_id=result.path[1:] or params.get("dataset_id", [""])[0],
-            **kwargs,
-        )
-
-    def do_connect(
-        self,
-        project_id: str | None = None,
-        dataset_id: str = "",
-        credentials: google.auth.credentials.Credentials | None = None,
-        application_name: str | None = None,
-        auth_local_webserver: bool = True,
-        auth_external_data: bool = False,
-        auth_cache: str = "default",
-        partition_column: str | None = "PARTITIONTIME",
-        client: bq.Client | None = None,
-        storage_client: bqstorage.BigQueryReadClient | None = None,
-    ) -> Backend:
-        """Create a `Backend` for use with Ibis.
-
-        Parameters
-        ----------
-        project_id
-            A BigQuery project id.
-        dataset_id
-            A dataset id that lives inside of the project indicated by
-            `project_id`.
-        credentials
-            Optional credentials.
-        application_name
-            A string identifying your application to Google API endpoints.
-        auth_local_webserver
-            Use a local webserver for the user authentication.  Binds a
-            webserver to an open port on localhost between 8080 and 8089,
-            inclusive, to receive authentication token. If not set, defaults to
-            False, which requests a token via the console.
-        auth_external_data
-            Authenticate using additional scopes required to `query external
-            data sources
-            <https://cloud.google.com/bigquery/external-data-sources>`_,
-            such as Google Sheets, files in Google Cloud Storage, or files in
-            Google Drive. If not set, defaults to False, which requests the
-            default BigQuery scopes.
-        auth_cache
-            Selects the behavior of the credentials cache.
-
-            ``'default'``
-                Reads credentials from disk if available, otherwise
-                authenticates and caches credentials to disk.
-
-            ``'reauth'``
-                Authenticates and caches credentials to disk.
-
-            ``'none'``
-                Authenticates and does **not** cache credentials.
-
-            Defaults to ``'default'``.
-        partition_column
-            Identifier to use instead of default ``_PARTITIONTIME`` partition
-            column. Defaults to ``'PARTITIONTIME'``.
-        client
-            A ``Client`` from the ``google.cloud.bigquery`` package. If not
-            set, one is created using the ``project_id`` and ``credentials``.
-        storage_client
-            A ``BigQueryReadClient`` from the
-            ``google.cloud.bigquery_storage_v1`` package. If not set, one is
-            created using the ``project_id`` and ``credentials``.
-
-        Returns
-        -------
-        Backend
-            An instance of the BigQuery backend.
-        """
-        default_project_id = client.project if client is not None else project_id
-
-        # Only need `credentials` to create a `client` and
-        # `storage_client`, so only one or the other needs to be set.
-        if (client is None or storage_client is None) and credentials is None:
-            scopes = SCOPES
-            if auth_external_data:
-                scopes = EXTERNAL_DATA_SCOPES
-
-            if auth_cache == "default":
-                credentials_cache = cache.ReadWriteCredentialsCache(
-                    filename="ibis.json"
-                )
-            elif auth_cache == "reauth":
-                credentials_cache = cache.WriteOnlyCredentialsCache(
-                    filename="ibis.json"
-                )
-            elif auth_cache == "none":
-                credentials_cache = cache.NOOP
-            else:
-                raise ValueError(
-                    f"Got unexpected value for auth_cache = '{auth_cache}'. "
-                    "Expected one of 'default', 'reauth', or 'none'."
-                )
-
-            credentials, default_project_id = pydata_google_auth.default(
-                scopes,
-                client_id=CLIENT_ID,
-                client_secret=CLIENT_SECRET,
-                credentials_cache=credentials_cache,
-                use_local_webserver=auth_local_webserver,
-            )
-
-        project_id = project_id or default_project_id
-
-        (
-            self.data_project,
-            self.billing_project,
-            self.dataset,
-        ) = parse_project_and_dataset(project_id, dataset_id)
+from ibis.sql.compiler import DDL
+from .compiler import quote_identifier, _type_to_sql_string
 
-        if client is not None:
-            self.client = client
-        else:
-            self.client = bq.Client(
-                project=self.billing_project,
-                credentials=credentials,
-                client_info=_create_client_info(application_name),
-            )
+from ibis.expr.datatypes import validate_type
+from ibis.compat import py_string
+import ibis.expr.rules as rules
+
+
+fully_qualified_re = re.compile("(.*)\.(?:`(.*)`|(.*))")
+
+
+def _is_fully_qualified(x):
+    m = fully_qualified_re.search(x)
+    return bool(m)
 
-        if storage_client is not None:
-            self.storage_client = storage_client
-        else:
-            self.storage_client = bqstorage.BigQueryReadClient(
-                credentials=credentials,
-                client_info=_create_client_info_gapic(application_name),
-            )
-
-        self.partition_column = partition_column
-
-    def _parse_project_and_dataset(self, dataset) -> tuple[str, str]:
-        if not dataset and not self.dataset:
-            raise ValueError("Unable to determine BigQuery dataset.")
-        project, _, dataset = parse_project_and_dataset(
-            self.billing_project,
-            dataset or f"{self.data_project}.{self.dataset}",
-        )
-        return project, dataset
-
-    @property
-    def project_id(self):
-        return self.data_project
-
-    @property
-    def dataset_id(self):
-        return self.dataset
-
-    def create_schema(
-        self,
-        name: str,
-        database: str | None = None,
-        force: bool = False,
-        collate: str | None = None,
-        **options: Any,
-    ) -> None:
-        create_stmt = "CREATE SCHEMA"
-        if force:
-            create_stmt += " IF NOT EXISTS"
-
-        create_stmt += " "
-        create_stmt += ".".join(filter(None, [database, name]))
-
-        if collate is not None:
-            create_stmt += f" DEFAULT COLLATION {collate}"
-
-        options_str = ", ".join(f"{name}={value!r}" for name, value in options.items())
-        if options_str:
-            create_stmt += f" OPTIONS({options_str})"
-        self.raw_sql(create_stmt)
-
-    def drop_schema(
-        self,
-        name: str,
-        database: str | None = None,
-        force: bool = False,
-        cascade: bool = False,
-    ) -> None:
-        drop_stmt = "DROP SCHEMA"
-        if force:
-            drop_stmt += " IF EXISTS"
-
-        drop_stmt += " "
-        drop_stmt += ".".join(filter(None, [database, name]))
-        drop_stmt += " CASCADE" if cascade else " RESTRICT"
-        self.raw_sql(drop_stmt)
-
-    def table(self, name: str, database: str | None = None) -> ir.TableExpr:
-        if database is None:
-            database = f"{self.data_project}.{self.current_schema}"
-        table_id = self._fully_qualified_name(name, database)
-        t = super().table(table_id)
-        bq_table = self.client.get_table(table_id)
-        return rename_partitioned_column(t, bq_table, self.partition_column)
-
-    def _fully_qualified_name(self, name, database):
-        parts = name.split(".")
-        if len(parts) == 3:
-            return name
-
-        default_project, default_dataset = self._parse_project_and_dataset(database)
-        if len(parts) == 2:
-            return f"{default_project}.{name}"
-        elif len(parts) == 1:
-            return f"{default_project}.{default_dataset}.{name}"
-        raise ValueError(f"Got too many components in table name: {name}")
-
-    def _get_schema_using_query(self, query):
-        job_config = bq.QueryJobConfig(dry_run=True, use_query_cache=False)
-        job = self.client.query(query, job_config=job_config)
-        return BigQuerySchema.to_ibis(job.schema)
-
-    def _get_table_schema(self, qualified_name):
-        dataset, table = qualified_name.rsplit(".", 1)
-        assert dataset is not None, "dataset is None"
-        return self.get_schema(table, database=dataset)
-
-    def _execute(self, stmt, results=True, query_parameters=None):
-        job_config = bq.job.QueryJobConfig()
-        job_config.query_parameters = query_parameters or []
-        job_config.use_legacy_sql = False  # False by default in >=0.28
-        query = self.client.query(
-            stmt, job_config=job_config, project=self.billing_project
-        )
-        query.result()  # blocks until finished
-        return BigQueryCursor(query)
-
-    def raw_sql(self, query: str, results=False, params=None):
-        query_parameters = [
-            bigquery_param(
-                param.type(),
-                value,
-                (
-                    param.get_name()
-                    if not isinstance(op := param.op(), ops.Alias)
-                    else op.arg.name
-                ),
-            )
-            for param, value in (params or {}).items()
-        ]
-        return self._execute(query, results=results, query_parameters=query_parameters)
-
-    @property
-    def current_database(self) -> str:
-        warnings.warn(
-            "current_database will return the current *data project* in ibis 7.0.0; "
-            "use current_schema for the current BigQuery dataset",
-            category=FutureWarning,
-        )
-        # TODO: return self.data_project in ibis 7.0.0
-        return self.dataset
-
-    @property
-    def current_schema(self) -> str | None:
-        return self.dataset
-
-    def database(self, name=None):
-        if name is None and not self.dataset:
-            raise ValueError(
-                "Unable to determine BigQuery dataset. Call "
-                "client.database('my_dataset') or set_database('my_dataset') "
-                "to assign your client a dataset."
-            )
-        return Database(name or self.dataset, self)
-
-    def execute(self, expr, params=None, limit="default", **kwargs):
-        """Compile and execute the given Ibis expression.
-
-        Compile and execute Ibis expression using this backend client
-        interface, returning results in-memory in the appropriate object type
-
-        Parameters
-        ----------
-        expr
-            Ibis expression to execute
-        limit
-            Retrieve at most this number of values/rows. Overrides any limit
-            already set on the expression.
-        params
-            Query parameters
-        kwargs
-            Extra arguments specific to the backend
-
-        Returns
-        -------
-        pd.DataFrame | pd.Series | scalar
-            Output from execution
-        """
-        # TODO: upstream needs to pass params to raw_sql, I think.
-        kwargs.pop("timecontext", None)
-        query_ast = self.compiler.to_ast_ensure_limit(expr, limit, params=params)
-        sql = query_ast.compile()
-        self._log(sql)
-        cursor = self.raw_sql(sql, params=params, **kwargs)
-
-        result = self.fetch_from_cursor(cursor, expr.as_table().schema())
-
-        return expr.__pandas_result__(result)
-
-    def fetch_from_cursor(self, cursor, schema):
-        arrow_t = self._cursor_to_arrow(cursor)
-        df = arrow_t.to_pandas(timestamp_as_object=True)
-        return PandasData.convert_table(df, schema)
-
-    def _cursor_to_arrow(
-        self,
-        cursor,
-        *,
-        method: Callable[[RowIterator], pa.Table | Iterable[pa.RecordBatch]]
-        | None = None,
-        chunk_size: int | None = None,
-    ):
-        if method is None:
-            method = lambda result: result.to_arrow(
-                progress_bar_type=None,
-                bqstorage_client=self.storage_client,
-            )
-        query = cursor.query
-        query_result = query.result(page_size=chunk_size)
-        # workaround potentially not having the ability to create read sessions
-        # in the dataset project
-        orig_project = query_result._project
-        query_result._project = self.billing_project
-        try:
-            arrow_obj = method(query_result)
-        finally:
-            query_result._project = orig_project
-        return arrow_obj
-
-    def to_pyarrow(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        **kwargs: Any,
-    ) -> pa.Table:
-        self._import_pyarrow()
-        query_ast = self.compiler.to_ast_ensure_limit(expr, limit, params=params)
-        sql = query_ast.compile()
-        cursor = self.raw_sql(sql, params=params, **kwargs)
-        table = self._cursor_to_arrow(cursor)
-        return expr.__pyarrow_result__(table)
-
-    def to_pyarrow_batches(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        chunk_size: int = 1_000_000,
-        **kwargs: Any,
-    ):
-        pa = self._import_pyarrow()
-
-        schema = expr.as_table().schema()
-
-        query_ast = self.compiler.to_ast_ensure_limit(expr, limit, params=params)
-        sql = query_ast.compile()
-        cursor = self.raw_sql(sql, params=params, **kwargs)
-        batch_iter = self._cursor_to_arrow(
-            cursor,
-            method=lambda result: result.to_arrow_iterable(
-                bqstorage_client=self.storage_client
-            ),
-            chunk_size=chunk_size,
-        )
-        return pa.RecordBatchReader.from_batches(schema.to_pyarrow(), batch_iter)
-
-    def get_schema(self, name, database=None):
-        table_id = self._fully_qualified_name(name, database)
-        table_ref = bq.TableReference.from_string(table_id)
-        table = self.client.get_table(table_ref)
-        return schema_from_bigquery_table(table)
-
-    def list_schemas(self, like=None):
-        results = [
-            dataset.dataset_id
-            for dataset in self.client.list_datasets(project=self.data_project)
-        ]
-        return self._filter_with_like(results, like)
-
-    @ibis.util.deprecated(
-        instead="use `list_schemas()`", as_of="6.1.0", removed_in="8.0.0"
-    )
-    def list_databases(self, like=None):
-        return self.list_schemas(like=like)
-
-    def list_tables(self, like=None, database=None):
-        project, dataset = self._parse_project_and_dataset(database)
-        dataset_ref = bq.DatasetReference(project, dataset)
-        result = [table.table_id for table in self.client.list_tables(dataset_ref)]
-        return self._filter_with_like(result, like)
-
-    def set_database(self, name):
-        self.data_project, self.dataset = self._parse_project_and_dataset(name)
-
-    @property
-    def version(self):
-        return bq.__version__
-
-    def create_table(
-        self,
-        name: str,
-        obj: pd.DataFrame | pa.Table | ir.Table | None = None,
-        *,
-        schema: ibis.Schema | None = None,
-        database: str | None = None,
-        temp: bool | None = None,
-        overwrite: bool = False,
-        default_collate: str | None = None,
-        partition_by: str | None = None,
-        cluster_by: Iterable[str] | None = None,
-        options: Mapping[str, Any] | None = None,
-    ) -> ir.Table:
-        """Create a table in BigQuery.
-
-        Parameters
-        ----------
-        name
-            Name of the table to create
-        obj
-            The data with which to populate the table; optional, but one of `obj`
-            or `schema` must be specified
-        schema
-            The schema of the table to create; optional, but one of `obj` or
-            `schema` must be specified
-        database
-            The BigQuery *dataset* in which to create the table; optional
-        temp
-            This parameter is not yet supported in the BigQuery backend
-        overwrite
-            If `True`, replace the table if it already exists, otherwise fail if
-            the table exists
-        default_collate
-            Default collation for string columns. See BigQuery's documentation
-            for more details: https://cloud.google.com/bigquery/docs/reference/standard-sql/collation-concepts
-        partition_by
-            Partition the table by the given expression. See BigQuery's documentation
-            for more details: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#partition_expression
-        cluster_by
-            List of columns to cluster the table by. See BigQuery's documentation
-            for more details: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#clustering_column_list
-        options
-            BigQuery-specific table options; see the BigQuery documentation for
-            details: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#table_option_list
-
-        Returns
-        -------
-        Table
-            The table that was just created
-        """
-        if obj is None and schema is None:
-            raise com.IbisError("One of the `schema` or `obj` parameter is required")
-
-        if temp:
-            # TODO: these require a BQ session; figure out how to handle that
-            raise NotImplementedError(
-                "Temporary tables in the BigQuery backend are not yet supported"
-            )
-
-        create_stmt = "CREATE"
-
-        if overwrite:
-            create_stmt += " OR REPLACE"
-
-        table_ref = self._fully_qualified_name(name, database)
-
-        create_stmt += f" TABLE `{table_ref}`"
-
-        if isinstance(obj, ir.Table) and schema is not None:
-            if not schema.equals(obj.schema()):
-                raise com.IbisTypeError(
-                    "Provided schema and Ibis table schema are incompatible. Please "
-                    "align the two schemas, or provide only one of the two arguments."
-                )
-
-        if schema is not None:
-            schema_str = ", ".join(
-                (
-                    f"{name} {BigQueryType.from_ibis(typ)}"
-                    + " NOT NULL" * (not typ.nullable)
-                )
-                for name, typ in schema.items()
-            )
-            create_stmt += f" ({schema_str})"
-
-        if default_collate is not None:
-            create_stmt += f" DEFAULT COLLATE {default_collate!r}"
-
-        if partition_by is not None:
-            create_stmt += f" PARTITION BY {partition_by}"
-
-        if cluster_by is not None:
-            create_stmt += f" CLUSTER BY {', '.join(cluster_by)}"
-
-        if options:
-            pairs = ", ".join(f"{k}={v!r}" for k, v in options.items())
-            create_stmt += f" OPTIONS({pairs})"
 
-        if obj is not None:
-            import pyarrow as pa
+def _is_quoted(x):
+    regex = re.compile("(?:`(.*)`|(.*))")
+    quoted, unquoted = regex.match(x).groups()
+    return quoted is not None
 
-            if isinstance(obj, (pd.DataFrame, pa.Table)):
-                table = ibis.memtable(obj, schema=schema)
+
+class ImpalaDDL(DDL):
+
+    def _get_scoped_name(self, obj_name, database):
+        if database:
+            scoped_name = '{0}.`{1}`'.format(database, obj_name)
+        else:
+            if not _is_fully_qualified(obj_name):
+                if _is_quoted(obj_name):
+                    return obj_name
+                else:
+                    return '`{0}`'.format(obj_name)
             else:
-                table = obj
+                return obj_name
+        return scoped_name
+
+
+class CreateDDL(ImpalaDDL):
+
+    def _if_exists(self):
+        return 'IF NOT EXISTS ' if self.can_exist else ''
+
 
-            create_stmt += f" AS ({self.compile(table)})"
+_format_aliases = {
+    'TEXT': 'TEXTFILE'
+}
 
-        self.raw_sql(create_stmt)
 
-        return self.table(table_ref)
+def _sanitize_format(format):
+    if format is None:
+        return
+    format = format.upper()
+    format = _format_aliases.get(format, format)
+    if format not in ('PARQUET', 'AVRO', 'TEXTFILE'):
+        raise ValueError('Invalid format: {0}'.format(format))
 
-    def drop_table(
-        self, name: str, *, database: str | None = None, force: bool = False
-    ) -> None:
-        table_id = self._fully_qualified_name(name, database)
-        drop_stmt = "DROP TABLE"
-        if force:
-            drop_stmt += " IF EXISTS"
-        drop_stmt += f" `{table_id}`"
-        self.raw_sql(drop_stmt)
-
-    def create_view(
-        self,
-        name: str,
-        obj: ir.Table,
-        *,
-        database: str | None = None,
-        overwrite: bool = False,
-    ) -> ir.Table:
-        or_replace = "OR REPLACE " * overwrite
-        sql_select = self.compile(obj)
-        table_id = self._fully_qualified_name(name, database)
-        code = f"CREATE {or_replace}VIEW `{table_id}` AS {sql_select}"
-        self.raw_sql(code)
-        return self.table(name, database=database)
-
-    def drop_view(
-        self, name: str, *, database: str | None = None, force: bool = False
-    ) -> None:
-        # default_project, default_dataset = self._parse_project_and_dataset(database)
-        table_id = self._fully_qualified_name(name, database)
-        drop_stmt = "DROP VIEW"
-        if force:
-            drop_stmt += " IF EXISTS"
-        drop_stmt += f" `{table_id}`"
-        self.raw_sql(drop_stmt)
-
-
-def compile(expr, params=None, **kwargs):
-    """Compile an expression for BigQuery."""
-    backend = Backend()
-    return backend.compile(expr, params=params, **kwargs)
-
-
-def connect(
-    project_id: str | None = None,
-    dataset_id: str = "",
-    credentials: google.auth.credentials.Credentials | None = None,
-    application_name: str | None = None,
-    auth_local_webserver: bool = False,
-    auth_external_data: bool = False,
-    auth_cache: str = "default",
-    partition_column: str | None = "PARTITIONTIME",
-) -> Backend:
-    """Create a :class:`Backend` for use with Ibis.
+    return format
+
+
+def _format_properties(props):
+    tokens = []
+    for k, v in sorted(props.items()):
+        tokens.append("'{0!s}'='{1!s}'".format(k, v))
+
+    return '({0})'.format(', '.join(tokens))
+
+
+class CreateTable(CreateDDL):
+
+    """
 
     Parameters
     ----------
-    project_id
-        A BigQuery project id.
-    dataset_id
-        A dataset id that lives inside of the project indicated by
-        `project_id`.
-    credentials
-        Optional credentials.
-    application_name
-        A string identifying your application to Google API endpoints.
-    auth_local_webserver
-        Use a local webserver for the user authentication.  Binds a
-        webserver to an open port on localhost between 8080 and 8089,
-        inclusive, to receive authentication token. If not set, defaults
-        to False, which requests a token via the console.
-    auth_external_data
-        Authenticate using additional scopes required to `query external
-        data sources
-        <https://cloud.google.com/bigquery/external-data-sources>`_,
-        such as Google Sheets, files in Google Cloud Storage, or files in
-        Google Drive. If not set, defaults to False, which requests the
-        default BigQuery scopes.
-    auth_cache
-        Selects the behavior of the credentials cache.
-
-        ``'default'``
-            Reads credentials from disk if available, otherwise
-            authenticates and caches credentials to disk.
-
-        ``'reauth'``
-            Authenticates and caches credentials to disk.
-
-        ``'none'``
-            Authenticates and does **not** cache credentials.
-
-        Defaults to ``'default'``.
-    partition_column
-        Identifier to use instead of default ``_PARTITIONTIME`` partition
-        column. Defaults to ``'PARTITIONTIME'``.
-
-    Returns
-    -------
-    Backend
-        An instance of the BigQuery backend
+    partition :
+
+    """
+
+    def __init__(self, table_name, database=None, external=False,
+                 format='parquet', can_exist=False,
+                 partition=None, path=None):
+        self.table_name = table_name
+        self.database = database
+        self.partition = partition
+        self.path = path
+        self.external = external
+        self.can_exist = can_exist
+        self.format = _sanitize_format(format)
+
+    def _create_line(self):
+        scoped_name = self._get_scoped_name(self.table_name, self.database)
+
+        if self.external:
+            create_decl = 'CREATE EXTERNAL TABLE'
+        else:
+            create_decl = 'CREATE TABLE'
+
+        create_line = '{0} {1}{2}'.format(create_decl, self._if_exists(),
+                                          scoped_name)
+        return create_line
+
+    def _location(self):
+        if self.path:
+            return "\nLOCATION '{0}'".format(self.path)
+        return ''
+
+    def _storage(self):
+        storage_lines = {
+            'PARQUET': '\nSTORED AS PARQUET',
+            'AVRO': '\nSTORED AS AVRO'
+        }
+        return storage_lines[self.format]
+
+
+class CTAS(CreateTable):
+
+    """
+    Create Table As Select
+    """
+
+    def __init__(self, table_name, select, database=None,
+                 external=False, format='parquet', can_exist=False,
+                 path=None):
+        self.select = select
+        CreateTable.__init__(self, table_name, database=database,
+                             external=external, format=format,
+                             can_exist=can_exist, path=path)
+
+    def compile(self):
+        buf = StringIO()
+        buf.write(self._create_line())
+        buf.write(self._storage())
+        buf.write(self._location())
+
+        select_query = self.select.compile()
+        buf.write('\nAS\n{0}'.format(select_query))
+        return buf.getvalue()
+
+
+class CreateView(CreateDDL):
+
+    """
+    Create Table As Select
+    """
+
+    def __init__(self, name, select, database=None, can_exist=False):
+        self.name = name
+        self.database = database
+        self.select = select
+        self.can_exist = can_exist
+
+    def compile(self):
+        buf = StringIO()
+        buf.write(self._create_line())
+
+        select_query = self.select.compile()
+        buf.write('\nAS\n{0}'.format(select_query))
+        return buf.getvalue()
+
+    def _create_line(self):
+        scoped_name = self._get_scoped_name(self.name, self.database)
+        return '{0} {1}{2}'.format('CREATE VIEW', self._if_exists(),
+                                   scoped_name)
+
+
+class CreateTableParquet(CreateTable):
+
+    def __init__(self, table_name, path,
+                 example_file=None,
+                 example_table=None,
+                 schema=None,
+                 external=True,
+                 **kwargs):
+        self.example_file = example_file
+        self.example_table = example_table
+        self.schema = schema
+        CreateTable.__init__(self, table_name, external=external,
+                             format='parquet', path=path, **kwargs)
+
+        self._validate()
+
+    def _validate(self):
+        pass
+
+    def compile(self):
+        buf = StringIO()
+        buf.write(self._create_line())
+
+        if self.example_file is not None:
+            buf.write("\nLIKE PARQUET '{0}'".format(self.example_file))
+        elif self.example_table is not None:
+            buf.write("\nLIKE {0}".format(self.example_table))
+        elif self.schema is not None:
+            schema = format_schema(self.schema)
+            buf.write('\n{0}'.format(schema))
+        else:
+            raise NotImplementedError
+
+        buf.write(self._storage())
+        buf.write(self._location())
+        return buf.getvalue()
+
+
+class CreateTableWithSchema(CreateTable):
+
+    def __init__(self, table_name, schema, table_format, **kwargs):
+        self.schema = schema
+        self.table_format = table_format
+
+        CreateTable.__init__(self, table_name, **kwargs)
+
+    def compile(self):
+        from ibis.expr.api import Schema
+
+        buf = StringIO()
+        buf.write(self._create_line())
+
+        def _push_schema(x):
+            formatted = format_schema(x)
+            buf.write('{0}'.format(formatted))
+
+        if self.partition is not None:
+            main_schema = self.schema
+            part_schema = self.partition
+            if not isinstance(part_schema, Schema):
+                part_schema = Schema(
+                    part_schema,
+                    [self.schema[name] for name in part_schema])
+
+            to_delete = []
+            for name in self.partition:
+                if name in self.schema:
+                    to_delete.append(name)
+
+            if len(to_delete):
+                main_schema = main_schema.delete(to_delete)
+
+            buf.write('\n')
+            _push_schema(main_schema)
+            buf.write('\nPARTITIONED BY ')
+            _push_schema(part_schema)
+        else:
+            buf.write('\n')
+            _push_schema(self.schema)
+
+        format_ddl = self.table_format.to_ddl()
+        if format_ddl:
+            buf.write(format_ddl)
+
+        buf.write(self._location())
+
+        return buf.getvalue()
+
+
+class NoFormat(object):
+
+    def to_ddl(self):
+        return None
+
+
+class DelimitedFormat(object):
+
+    def __init__(self, path, delimiter=None, escapechar=None,
+                 na_rep=None, lineterminator=None):
+        self.path = path
+        self.delimiter = delimiter
+        self.escapechar = escapechar
+        self.lineterminator = lineterminator
+        self.na_rep = na_rep
+
+    def to_ddl(self):
+        buf = StringIO()
+
+        buf.write("\nROW FORMAT DELIMITED")
+
+        if self.delimiter is not None:
+            buf.write("\nFIELDS TERMINATED BY '{0}'".format(self.delimiter))
+
+        if self.escapechar is not None:
+            buf.write("\nESCAPED BY '{0}'".format(self.escapechar))
+
+        if self.lineterminator is not None:
+            buf.write("\nLINES TERMINATED BY '{0}'"
+                      .format(self.lineterminator))
+
+        buf.write("\nLOCATION '{0}'".format(self.path))
+
+        if self.na_rep is not None:
+            buf.write("\nTBLPROPERTIES('serialization.null.format'='{0}')"
+                      .format(self.na_rep))
+
+        return buf.getvalue()
+
+
+class AvroFormat(object):
+
+    def __init__(self, path, avro_schema):
+        self.path = path
+        self.avro_schema = avro_schema
+
+    def to_ddl(self):
+        import json
+
+        buf = StringIO()
+        buf.write('\nSTORED AS AVRO')
+        buf.write("\nLOCATION '{0}'".format(self.path))
+
+        schema = json.dumps(self.avro_schema, indent=2, sort_keys=True)
+        schema = '\n'.join([x.rstrip() for x in schema.split('\n')])
+        buf.write("\nTBLPROPERTIES ('avro.schema.literal'='{0}')"
+                  .format(schema))
+
+        return buf.getvalue()
+
+
+class CreateTableDelimited(CreateTableWithSchema):
+
+    def __init__(self, table_name, path, schema,
+                 delimiter=None, escapechar=None, lineterminator=None,
+                 na_rep=None, external=True, **kwargs):
+        table_format = DelimitedFormat(path, delimiter=delimiter,
+                                       escapechar=escapechar,
+                                       lineterminator=lineterminator,
+                                       na_rep=na_rep)
+        CreateTableWithSchema.__init__(self, table_name, schema,
+                                       table_format, external=external,
+                                       **kwargs)
+
+
+class CreateTableAvro(CreateTable):
+
+    def __init__(self, table_name, path, avro_schema, external=True, **kwargs):
+        self.table_format = AvroFormat(path, avro_schema)
+
+        CreateTable.__init__(self, table_name, external=external, **kwargs)
+
+    def compile(self):
+        buf = StringIO()
+        buf.write(self._create_line())
+
+        format_ddl = self.table_format.to_ddl()
+        buf.write(format_ddl)
+
+        return buf.getvalue()
+
+
+class InsertSelect(ImpalaDDL):
+
+    def __init__(self, table_name, select_expr, database=None,
+                 partition=None,
+                 partition_schema=None,
+                 overwrite=False):
+        self.table_name = table_name
+        self.database = database
+        self.select = select_expr
+
+        self.partition = partition
+        self.partition_schema = partition_schema
+
+        self.overwrite = overwrite
+
+    def compile(self):
+        if self.overwrite:
+            cmd = 'INSERT OVERWRITE'
+        else:
+            cmd = 'INSERT INTO'
+
+        if self.partition is not None:
+            part = _format_partition(self.partition,
+                                     self.partition_schema)
+            partition = ' {0} '.format(part)
+        else:
+            partition = ''
+
+        select_query = self.select.compile()
+        scoped_name = self._get_scoped_name(self.table_name, self.database)
+        return'{0} {1}{2}\n{3}'.format(cmd, scoped_name, partition,
+                                       select_query)
+
+
+def _format_partition(partition, partition_schema):
+    tokens = []
+    if isinstance(partition, dict):
+        for name in partition_schema:
+            if name in partition:
+                tok = '{0}={1}'.format(name, partition[name])
+            else:
+                # dynamic partitioning
+                tok = name
+            tokens.append(tok)
+    else:
+        for name, value in zip(partition_schema, partition):
+            tok = '{0}={1}'.format(name, value)
+            tokens.append(tok)
+
+    return 'PARTITION ({0})'.format(', '.join(tokens))
+
+
+class LoadData(ImpalaDDL):
+
+    """
+    Generate DDL for LOAD DATA command. Cannot be cancelled
     """
-    backend = Backend()
-    return backend.connect(
-        project_id=project_id,
-        dataset_id=dataset_id,
-        credentials=credentials,
-        application_name=application_name,
-        auth_local_webserver=auth_local_webserver,
-        auth_external_data=auth_external_data,
-        auth_cache=auth_cache,
-        partition_column=partition_column,
-    )
-
-
-__all__ = [
-    "Backend",
-    "compile",
-    "connect",
-]
+
+    def __init__(self, table_name, path, database=None,
+                 partition=None, partition_schema=None,
+                 overwrite=False):
+        self.table_name = table_name
+        self.database = database
+        self.path = path
+
+        self.partition = partition
+        self.partition_schema = partition_schema
+
+        self.overwrite = overwrite
+
+    def compile(self):
+        overwrite = 'OVERWRITE ' if self.overwrite else ''
+
+        if self.partition is not None:
+            partition = '\n' + _format_partition(self.partition,
+                                                 self.partition_schema)
+        else:
+            partition = ''
+
+        scoped_name = self._get_scoped_name(self.table_name, self.database)
+        return ("LOAD DATA INPATH '{0}' {1}INTO TABLE {2}{3}"
+                .format(self.path, overwrite, scoped_name, partition))
+
+
+class AlterTable(ImpalaDDL):
+
+    def __init__(self, table, location=None, format=None, tbl_properties=None,
+                 serde_properties=None):
+        self.table = table
+        self.location = location
+        self.format = _sanitize_format(format)
+        self.tbl_properties = tbl_properties
+        self.serde_properties = serde_properties
+
+    def _wrap_command(self, cmd):
+        return 'ALTER TABLE {0}'.format(cmd)
+
+    def _format_properties(self, prefix=''):
+        tokens = []
+
+        if self.location is not None:
+            tokens.append("LOCATION '{0}'".format(self.location))
+
+        if self.format is not None:
+            tokens.append("FILEFORMAT {0}".format(self.format))
+
+        if self.tbl_properties is not None:
+            props = _format_properties(self.tbl_properties)
+            tokens.append('TBLPROPERTIES {0}'.format(props))
+
+        if self.serde_properties is not None:
+            props = _format_properties(self.serde_properties)
+            tokens.append('SERDEPROPERTIES {0}'.format(props))
+
+        if len(tokens) > 0:
+            return '\n{0}{1}'.format(prefix, '\n'.join(tokens))
+        else:
+            return ''
+
+    def compile(self):
+        props = self._format_properties()
+        action = '{0} SET {1}'.format(self.table, props)
+        return self._wrap_command(action)
+
+
+class PartitionProperties(AlterTable):
+
+    def __init__(self, table, partition, partition_schema,
+                 location=None, format=None,
+                 tbl_properties=None, serde_properties=None):
+        self.partition = partition
+        self.partition_schema = partition_schema
+
+        AlterTable.__init__(self, table, location=location, format=format,
+                            tbl_properties=tbl_properties,
+                            serde_properties=serde_properties)
+
+    def _compile(self, cmd, property_prefix=''):
+        part = _format_partition(self.partition, self.partition_schema)
+        if cmd:
+            part = '{0} {1}'.format(cmd, part)
+
+        props = self._format_properties(property_prefix)
+        action = '{0} {1}{2}'.format(self.table, part, props)
+        return self._wrap_command(action)
+
+
+class AddPartition(PartitionProperties):
+
+    def __init__(self, table, partition, partition_schema, location=None):
+        PartitionProperties.__init__(self, table, partition,
+                                     partition_schema,
+                                     location=location)
+
+    def compile(self):
+        return self._compile('ADD')
+
+
+class AlterPartition(PartitionProperties):
+
+    def compile(self):
+        return self._compile('', 'SET ')
+
+
+class DropPartition(PartitionProperties):
+
+    def __init__(self, table, partition, partition_schema):
+        PartitionProperties.__init__(self, table, partition,
+                                     partition_schema)
+
+    def compile(self):
+        return self._compile('DROP')
+
+
+class RenameTable(AlterTable):
+
+    def __init__(self, old_name, new_name, old_database=None,
+                 new_database=None):
+        # if either database is None, the name is assumed to be fully scoped
+        self.old_name = old_name
+        self.old_database = old_database
+        self.new_name = new_name
+        self.new_database = new_database
+
+        new_qualified_name = new_name
+        if new_database is not None:
+            new_qualified_name = self._get_scoped_name(new_name, new_database)
+
+        old_qualified_name = old_name
+        if old_database is not None:
+            old_qualified_name = self._get_scoped_name(old_name, old_database)
+
+        self.old_qualified_name = old_qualified_name
+        self.new_qualified_name = new_qualified_name
+
+    def compile(self):
+        cmd = '{0} RENAME TO {1}'.format(self.old_qualified_name,
+                                         self.new_qualified_name)
+        return self._wrap_command(cmd)
+
+
+class DropObject(ImpalaDDL):
+
+    def __init__(self, must_exist=True):
+        self.must_exist = must_exist
+
+    def compile(self):
+        if_exists = '' if self.must_exist else 'IF EXISTS '
+        object_name = self._object_name()
+        drop_line = 'DROP {0} {1}{2}'.format(self._object_type, if_exists,
+                                             object_name)
+        return drop_line
+
+
+class DropTable(DropObject):
+
+    _object_type = 'TABLE'
+
+    def __init__(self, table_name, database=None, must_exist=True):
+        self.table_name = table_name
+        self.database = database
+        DropObject.__init__(self, must_exist=must_exist)
+
+    def _object_name(self):
+        return self._get_scoped_name(self.table_name, self.database)
+
+
+class TruncateTable(ImpalaDDL):
+
+    _object_type = 'TABLE'
+
+    def __init__(self, table_name, database=None):
+        self.table_name = table_name
+        self.database = database
+
+    def compile(self):
+        name = self._get_scoped_name(self.table_name, self.database)
+        return 'TRUNCATE TABLE {0}'.format(name)
+
+
+class DropView(DropTable):
+
+    _object_type = 'VIEW'
+
+
+class CacheTable(ImpalaDDL):
+
+    def __init__(self, table_name, database=None, pool='default'):
+        self.table_name = table_name
+        self.database = database
+        self.pool = pool
+
+    def compile(self):
+        scoped_name = self._get_scoped_name(self.table_name, self.database)
+        cache_line = ('ALTER TABLE {0} SET CACHED IN \'{1}\''
+                      .format(scoped_name, self.pool))
+        return cache_line
+
+
+class CreateDatabase(CreateDDL):
+
+    def __init__(self, name, path=None, can_exist=False):
+        self.name = name
+        self.path = path
+        self.can_exist = can_exist
+
+    def compile(self):
+        name = quote_identifier(self.name)
+
+        create_decl = 'CREATE DATABASE'
+        create_line = '{0} {1}{2}'.format(create_decl, self._if_exists(),
+                                          name)
+        if self.path is not None:
+            create_line += "\nLOCATION '{0}'".format(self.path)
+
+        return create_line
+
+
+class DropDatabase(DropObject):
+
+    _object_type = 'DATABASE'
+
+    def __init__(self, name, must_exist=True):
+        self.name = name
+        DropObject.__init__(self, must_exist=must_exist)
+
+    def _object_name(self):
+        return self.name
+
+
+def format_schema(schema):
+    elements = [_format_schema_element(name, t)
+                for name, t in zip(schema.names, schema.types)]
+    return '({0})'.format(',\n '.join(elements))
+
+
+def _format_schema_element(name, t):
+    return '{0} {1}'.format(quote_identifier(name, force=True),
+                            _type_to_sql_string(t))
+
+
+class CreateFunctionBase(ImpalaDDL):
+
+    _object_type = 'FUNCTION'
+
+    def __init__(self, lib_path, inputs, output, name, database=None):
+        self.lib_path = lib_path
+
+        self.inputs, self.output = inputs, output
+        self.input_sig = _impala_signature(inputs)
+        self.output_sig = _arg_to_string(output)
+
+        self.name = name
+        self.database = database
+
+    def _create_line(self):
+        scoped_name = self._get_scoped_name(self.name, self.database)
+        return ('{0!s}({1!s}) returns {2!s}'
+                .format(scoped_name, self.input_sig, self.output_sig))
+
+
+class CreateFunction(CreateFunctionBase):
+
+    def __init__(self, lib_path, so_symbol, inputs, output,
+                 name, database=None):
+        self.so_symbol = so_symbol
+
+        CreateFunctionBase.__init__(self, lib_path, inputs, output,
+                                    name, database=database)
+
+    def compile(self):
+        create_decl = 'CREATE FUNCTION'
+        create_line = self._create_line()
+        param_line = ("location '{0!s}' symbol='{1!s}'"
+                      .format(self.lib_path, self.so_symbol))
+        full_line = ' '.join([create_decl, create_line, param_line])
+        return full_line
+
+
+class CreateAggregateFunction(CreateFunction):
+
+    def __init__(self, lib_path, inputs, output, update_fn, init_fn,
+                 merge_fn, serialize_fn, finalize_fn, name, database):
+        self.init = init_fn
+        self.update = update_fn
+        self.merge = merge_fn
+        self.serialize = serialize_fn
+        self.finalize = finalize_fn
+
+        CreateFunctionBase.__init__(self, lib_path, inputs, output,
+                                    name, database=database)
+
+    def compile(self):
+        create_decl = 'CREATE AGGREGATE FUNCTION'
+        create_line = self._create_line()
+        tokens = ["location '{0!s}'".format(self.lib_path)]
+
+        if self.init is not None:
+            tokens.append("init_fn='{0}'".format(self.init))
+
+        tokens.append("update_fn='{0}'".format(self.update))
+
+        if self.merge is not None:
+            tokens.append("merge_fn='{0}'".format(self.merge))
+
+        if self.serialize is not None:
+            tokens.append("serialize_fn='{0}'".format(self.serialize))
+
+        if self.finalize is not None:
+            tokens.append("finalize_fn='{0}'".format(self.finalize))
+
+        full_line = (' '.join([create_decl, create_line]) + ' ' +
+                     '\n'.join(tokens))
+        return full_line
+
+
+class DropFunction(DropObject):
+
+    def __init__(self, name, inputs, must_exist=True,
+                 aggregate=False, database=None):
+        self.name = name
+
+        self.inputs = inputs
+        self.input_sig = _impala_signature(inputs)
+
+        self.must_exist = must_exist
+        self.aggregate = aggregate
+        self.database = database
+        DropObject.__init__(self, must_exist=must_exist)
+
+    def _object_name(self):
+        return self.name
+
+    def _function_sig(self):
+        full_name = self._get_scoped_name(self.name, self.database)
+        return '{0!s}({1!s})'.format(full_name, self.input_sig)
+
+    def compile(self):
+        tokens = ['DROP']
+        if self.aggregate:
+            tokens.append('AGGREGATE')
+        tokens.append('FUNCTION')
+        if not self.must_exist:
+            tokens.append('IF EXISTS')
+
+        tokens.append(self._function_sig())
+        return ' '.join(tokens)
+
+
+class ListFunction(ImpalaDDL):
+
+    def __init__(self, database, like=None, aggregate=False):
+        self.database = database
+        self.like = like
+        self.aggregate = aggregate
+
+    def compile(self):
+        statement = 'SHOW '
+        if self.aggregate:
+            statement += 'AGGREGATE '
+        statement += 'FUNCTIONS IN {0}'.format(self.database)
+        if self.like:
+            statement += " LIKE '{0}'".format(self.like)
+        return statement
+
+
+def _impala_signature(sig):
+    if isinstance(sig, rules.TypeSignature):
+        if isinstance(sig, rules.VarArgs):
+            val = _arg_to_string(sig.arg_type)
+            return '{0}...'.format(val)
+        else:
+            return ', '.join([_arg_to_string(arg) for arg in sig.types])
+    else:
+        return ', '.join([_type_to_sql_string(validate_type(x))
+                          for x in sig])
+
+
+def _arg_to_string(arg):
+    if isinstance(arg, rules.ValueTyped):
+        types = arg.types
+        if len(types) > 1:
+            raise NotImplementedError
+        return _type_to_sql_string(types[0])
+    elif isinstance(arg, py_string):
+        return _type_to_sql_string(validate_type(arg))
+    else:
+        raise NotImplementedError
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/clickhouse/__init__.py` & `ibis-framework-v0.6.0/ibis/sql/alchemy.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,671 +1,875 @@
-from __future__ import annotations
+# Copyright 2015 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-import ast
-import json
-from contextlib import closing, suppress
-from functools import partial
-from typing import TYPE_CHECKING, Any, Iterable, Iterator, Literal, Mapping
+import operator
+import six
 
-import clickhouse_connect as cc
-import pyarrow as pa
 import sqlalchemy as sa
-import sqlglot as sg
-import toolz
-from clickhouse_connect.driver.external import ExternalData
+import sqlalchemy.sql as sql
 
-import ibis
-import ibis.common.exceptions as com
-import ibis.config
-import ibis.expr.analysis as an
+from ibis.client import SQLClient, AsyncQuery, Query
+from ibis.sql.compiler import Select, Union, TableSetFormatter
+import ibis.common as com
+import ibis.expr.datatypes as dt
 import ibis.expr.operations as ops
-import ibis.expr.schema as sch
 import ibis.expr.types as ir
-from ibis import util
-from ibis.backends.base import BaseBackend, CanCreateDatabase
-from ibis.backends.clickhouse.compiler import translate
-from ibis.backends.clickhouse.datatypes import parse, serialize
+import ibis.sql.compiler as comp
+import ibis.sql.transforms as transforms
+import ibis.util as util
+import ibis
 
-if TYPE_CHECKING:
-    import pandas as pd
 
-    from ibis.common.typing import SupportsSchema
+_ibis_type_to_sqla = {
+    dt.Int8: sa.types.SmallInteger,
+    dt.Int16: sa.types.SmallInteger,
+    dt.Int32: sa.types.Integer,
+    dt.Int64: sa.types.BigInteger,
+
+    # Mantissa-based
+    dt.Float: sa.types.Float(precision=24),
+    dt.Double: sa.types.Float(precision=53),
+
+    dt.Boolean: sa.types.Boolean,
+
+    dt.String: sa.types.String,
+
+    dt.Timestamp: sa.types.DateTime,
+
+    dt.Decimal: sa.types.NUMERIC,
+}
+
+_sqla_type_mapping = {
+    sa.types.SmallInteger: dt.Int16,
+    sa.types.INTEGER: dt.Int64,
+    sa.types.BOOLEAN: dt.Boolean,
+    sa.types.BIGINT: dt.Int64,
+    sa.types.FLOAT: dt.Double,
+    sa.types.REAL: dt.Double,
+
+    sa.types.TEXT: dt.String,
+    sa.types.NullType: dt.String,
+    sa.types.Text: dt.String,
+}
+
+_sqla_type_to_ibis = dict((v, k) for k, v in
+                          _ibis_type_to_sqla.items())
+_sqla_type_to_ibis.update(_sqla_type_mapping)
+
+
+def schema_from_table(table):
+    # Convert SQLA table to Ibis schema
+    names = table.columns.keys()
+
+    types = []
+    for c in table.columns.values():
+        type_class = type(c.type)
+
+        if isinstance(c.type, sa.types.NUMERIC):
+            t = dt.Decimal(c.type.precision,
+                           c.type.scale,
+                           nullable=c.nullable)
+        else:
+            if c.type in _sqla_type_to_ibis:
+                ibis_class = _sqla_type_to_ibis[c.type]
+            elif type_class in _sqla_type_to_ibis:
+                ibis_class = _sqla_type_to_ibis[type_class]
+            else:
+                raise NotImplementedError(c.type)
+            t = ibis_class(c.nullable)
 
+        types.append(t)
 
-def _to_memtable(v):
-    return ibis.memtable(v).op() if not isinstance(v, ops.InMemoryTable) else v
+    return dt.Schema(names, types)
 
 
-class ClickhouseTable(ir.Table):
-    """References a physical table in Clickhouse."""
+def table_from_schema(name, meta, schema):
+    # Convert Ibis schema to SQLA table
+    sqla_cols = []
+
+    for cname, itype in zip(schema.names, schema.types):
+        ctype = _to_sqla_type(itype)
+
+        col = sa.Column(cname, ctype, nullable=itype.nullable)
+        sqla_cols.append(col)
+
+    return sa.Table(name, meta, *sqla_cols)
+
+
+def _to_sqla_type(itype):
+    if isinstance(itype, dt.Decimal):
+        return sa.types.NUMERIC(itype.precision, itype.scale)
+    else:
+        return _ibis_type_to_sqla[type(itype)]
+
+
+def fixed_arity(sa_func, arity):
+    if isinstance(sa_func, six.string_types):
+        sa_func = getattr(sa.func, sa_func)
+
+    def formatter(t, expr):
+        if arity != len(expr.op().args):
+            raise com.IbisError('incorrect number of args')
+
+        return _varargs_call(sa_func, t, expr)
+
+    return formatter
+
+
+def varargs(sa_func):
+    def formatter(t, expr):
+        op = expr.op()
+        trans_args = [t.translate(arg) for arg in op.args]
+        return sa_func(*trans_args)
+    return formatter
+
+
+def _varargs_call(sa_func, t, expr):
+    op = expr.op()
+    trans_args = [t.translate(arg) for arg in op.args]
+    return sa_func(*trans_args)
+
+
+def _table_column(t, expr):
+    op = expr.op()
+    ctx = t.context
+    table = op.table
+
+    sa_table = _get_sqla_table(ctx, table)
+    out_expr = getattr(sa_table.c, op.name)
+
+    # If the column does not originate from the table set in the current SELECT
+    # context, we should format as a subquery
+    if t.permit_subquery and ctx.is_foreign_expr(table):
+        return sa.select([out_expr])
+
+    return out_expr
+
+
+def _get_sqla_table(ctx, table):
+    if ctx.has_ref(table):
+        ctx_level = ctx
+        sa_table = ctx_level.get_table(table)
+        while sa_table is None and ctx_level.parent is not ctx_level:
+            ctx_level = ctx_level.parent
+            sa_table = ctx_level.get_table(table)
+    else:
+        sa_table = table.op().sqla_table
+
+    return sa_table
+
+
+def _table_array_view(t, expr):
+    ctx = t.context
+    table = ctx.get_compiled_expr(expr.op().table)
+    return table
+
+
+def _exists_subquery(t, expr):
+    op = expr.op()
+    ctx = t.context
+
+    filtered = (op.foreign_table.filter(op.predicates)
+                .projection([ir.literal(1).name(ir.unnamed)]))
+
+    sub_ctx = ctx.subcontext()
+    clause = to_sqlalchemy(filtered, context=sub_ctx, exists=True)
+
+    if isinstance(op, transforms.NotExistsSubquery):
+        clause = sa.not_(clause)
+
+    return clause
+
+
+def _cast(t, expr):
+    op = expr.op()
+    arg, target_type = op.args
+    sa_arg = t.translate(arg)
+    sa_type = t.get_sqla_type(target_type)
+
+    if isinstance(arg, ir.CategoryValue) and target_type == 'int32':
+        return sa_arg
+    else:
+        return sa.cast(sa_arg, sa_type)
+
+
+def _contains(t, expr):
+    op = expr.op()
+
+    left, right = [t.translate(arg) for arg in op.args]
+    return left.in_(right)
+
+
+def _reduction(sa_func):
+    def formatter(t, expr):
+        op = expr.op()
+
+        # HACK: support trailing arguments
+        arg, where = op.args[:2]
+
+        return _reduction_format(t, sa_func, arg, where)
+    return formatter
+
+
+def _reduction_format(t, sa_func, arg, where):
+    if where is not None:
+        case = where.ifelse(arg, ibis.NA)
+        arg = t.translate(case)
+    else:
+        arg = t.translate(arg)
+
+    return sa_func(arg)
+
+
+def _literal(t, expr):
+    return sa.literal(expr.op().value)
+
+
+def _value_list(t, expr):
+    return [t.translate(x) for x in expr.op().values]
 
-    @property
-    def _client(self):
-        return self.op().source
+
+def _is_null(t, expr):
+    arg = t.translate(expr.op().args[0])
+    return arg.is_(sa.null())
+
+
+def _not_null(t, expr):
+    arg = t.translate(expr.op().args[0])
+    return arg.isnot(sa.null())
+
+
+def _round(t, expr):
+    op = expr.op()
+    arg, digits = op.args
+    sa_arg = t.translate(arg)
+
+    f = sa.func.round
+
+    if digits is not None:
+        sa_digits = t.translate(digits)
+        return f(sa_arg, sa_digits)
+    else:
+        return f(sa_arg)
+
+
+def _count_distinct(t, expr):
+    arg, = expr.op().args
+    sa_arg = t.translate(arg)
+    return sa.func.count(sa_arg.distinct())
+
+
+def _simple_case(t, expr):
+    op = expr.op()
+
+    cases = [op.base == case for case in op.cases]
+    return _translate_case(t, cases, op.results, op.default)
+
+
+def _searched_case(t, expr):
+    op = expr.op()
+    return _translate_case(t, op.cases, op.results, op.default)
+
+
+def _translate_case(t, cases, results, default):
+    case_args = [t.translate(arg) for arg in cases]
+    result_args = [t.translate(arg) for arg in results]
+
+    whens = zip(case_args, result_args)
+    default = t.translate(default)
+
+    return sa.case(whens, else_=default)
+
+
+def unary(sa_func):
+    return fixed_arity(sa_func, 1)
+
+
+_operation_registry = {
+    ops.And: fixed_arity(sql.and_, 2),
+    ops.Or: fixed_arity(sql.or_, 2),
+
+    ops.Abs: unary(sa.func.abs),
+
+    ops.Cast: _cast,
+
+    ops.Coalesce: varargs(sa.func.coalesce),
+
+    ops.NullIf: fixed_arity(sa.func.nullif, 2),
+
+    ops.Contains: _contains,
+
+    ops.Count: _reduction(sa.func.count),
+    ops.Sum: _reduction(sa.func.sum),
+    ops.Mean: _reduction(sa.func.avg),
+    ops.Min: _reduction(sa.func.min),
+    ops.Max: _reduction(sa.func.max),
+
+    ops.CountDistinct: _count_distinct,
+
+    ops.GroupConcat: fixed_arity(sa.func.group_concat, 2),
+
+    ops.Between: fixed_arity(sa.between, 3),
+
+    ops.IsNull: _is_null,
+    ops.NotNull: _not_null,
+    ops.Negate: unary(sa.not_),
+
+    ops.Round: _round,
+
+    ops.TypeOf: unary(sa.func.typeof),
+
+    ir.Literal: _literal,
+    ir.ValueList: _value_list,
+    ir.NullLiteral: lambda *args: sa.null(),
+
+    ops.SimpleCase: _simple_case,
+    ops.SearchedCase: _searched_case,
+
+    ops.TableColumn: _table_column,
+    ops.TableArrayView: _table_array_view,
+
+    transforms.ExistsSubquery: _exists_subquery,
+    transforms.NotExistsSubquery: _exists_subquery,
+}
+
+
+# TODO: unit tests for each of these
+_binary_ops = {
+    # Binary arithmetic
+    ops.Add: operator.add,
+    ops.Subtract: operator.sub,
+    ops.Multiply: operator.mul,
+    ops.Divide: operator.truediv,
+    ops.Power: operator.pow,
+    ops.Modulus: operator.mod,
+
+    # Comparisons
+    ops.Equals: operator.eq,
+    ops.NotEquals: operator.ne,
+    ops.Less: operator.lt,
+    ops.LessEqual: operator.le,
+    ops.Greater: operator.gt,
+    ops.GreaterEqual: operator.ge,
+
+    # Boolean comparisons
+
+    # TODO
+}
+
+for _k, _v in _binary_ops.items():
+    _operation_registry[_k] = fixed_arity(_v, 2)
+
+
+class AlchemySelectBuilder(comp.SelectBuilder):
 
     @property
-    def name(self):
-        return self.op().name
+    def _select_class(self):
+        return AlchemySelect
 
-    def insert(self, obj, settings: Mapping[str, Any] | None = None, **kwargs):
-        import pandas as pd
+    def _convert_group_by(self, exprs):
+        return exprs
 
-        if not isinstance(obj, pd.DataFrame):
-            raise com.IbisError(
-                f"Invalid input type {type(obj)}; only pandas DataFrames are accepted as input"
-            )
 
-        return self._client.con.insert_df(self.name, obj, settings=settings, **kwargs)
+class AlchemyContext(comp.QueryContext):
 
+    def __init__(self, *args, **kwargs):
+        self._table_objects = {}
+        self.dialect = kwargs.pop('dialect', AlchemyDialect)
+        comp.QueryContext.__init__(self, *args, **kwargs)
 
-class Backend(BaseBackend, CanCreateDatabase):
-    name = "clickhouse"
+    def subcontext(self):
+        return type(self)(dialect=self.dialect, parent=self)
 
-    # ClickHouse itself does, but the client driver does not
-    supports_temporary_tables = False
+    def _to_sql(self, expr, ctx):
+        return to_sqlalchemy(expr, context=ctx)
 
-    class Options(ibis.config.Config):
-        """Clickhouse options.
+    def _compile_subquery(self, expr):
+        sub_ctx = self.subcontext()
+        return self._to_sql(expr, sub_ctx)
 
-        Attributes
-        ----------
-        bool_type : str
-            Type to use for boolean columns
+    def has_table(self, expr, parent_contexts=False):
+        key = self._get_table_key(expr)
+        return self._key_in(key, '_table_objects',
+                            parent_contexts=parent_contexts)
+
+    def set_table(self, expr, obj):
+        key = self._get_table_key(expr)
+        self._table_objects[key] = obj
+
+    def get_table(self, expr):
+        """
+        Get the memoized SQLAlchemy expression object
         """
+        return self._get_table_item('_table_objects', expr)
 
-        bool_type: Literal["Bool", "UInt8", "Int8"] = "Bool"
 
-    def _log(self, sql: str) -> None:
-        """Log `sql`.
+class AlchemyQueryBuilder(comp.QueryBuilder):
+
+    select_builder = AlchemySelectBuilder
+
+    def __init__(self, expr, context=None, dialect=None):
+        if dialect is None:
+            dialect = AlchemyDialect
+
+        self.dialect = dialect
+        comp.QueryBuilder.__init__(self, expr, context=context)
+
+    def _make_context(self):
+        return AlchemyContext(dialect=self.dialect)
+
+    @property
+    def _union_class(self):
+        return AlchemyUnion
+
+
+def to_sqlalchemy(expr, context=None, exists=False, dialect=None):
+    if context is not None:
+        dialect = dialect or context.dialect
+
+    ast = build_ast(expr, context=context, dialect=dialect)
+    query = ast.queries[0]
+
+    if exists:
+        query.exists = exists
+
+    return query.compile()
+
+
+def build_ast(expr, context=None, dialect=None):
+    builder = AlchemyQueryBuilder(expr, context=context, dialect=dialect)
+    return builder.get_result()
+
+
+class AlchemyTable(ops.DatabaseTable):
+
+    def __init__(self, table, source):
+        self.sqla_table = table
+
+        schema = schema_from_table(table)
+        name = table.name
+
+        ops.TableNode.__init__(self, [name, schema, source])
+        ops.HasSchema.__init__(self, schema, name=name)
+
+
+class AlchemyExprTranslator(comp.ExprTranslator):
+
+    _registry = _operation_registry
+    _rewrites = comp.ExprTranslator._rewrites.copy()
+    _type_map = _ibis_type_to_sqla
+
+    def name(self, translated, name, force=True):
+        return translated.label(name)
+
+    @property
+    def _context_class(self):
+        return AlchemyContext
+
+    def get_sqla_type(self, data_type):
+        return self._type_map[type(data_type)]
 
-        This method can be implemented by subclasses. Logging occurs when
-        `ibis.options.verbose` is `True`.
-        """
-        util.log(sql)
 
-    def sql(
-        self,
-        query: str,
-        schema: SupportsSchema | None = None,
-        dialect: str | None = None,
-    ) -> ir.Table:
-        query = self._transpile_sql(query, dialect=dialect)
-        if schema is None:
-            schema = self._get_schema_using_query(query)
-        return ops.SQLQueryResult(query, ibis.schema(schema), self).to_expr()
+rewrites = AlchemyExprTranslator.rewrites
+compiles = AlchemyExprTranslator.compiles
 
-    def _from_url(self, url: str, **kwargs) -> BaseBackend:
-        """Connect to a backend using a URL `url`.
+
+class AlchemyQuery(Query):
+
+    def _fetch(self, cursor):
+        # No guarantees that the DBAPI cursor has data types
+        import pandas as pd
+        proxy = cursor.proxy
+        rows = proxy.fetchall()
+        colnames = proxy.keys()
+        return pd.DataFrame.from_records(rows, columns=colnames,
+                                         coerce_float=True)
+
+
+class AlchemyAsyncQuery(AsyncQuery):
+    pass
+
+
+class AlchemyDialect(object):
+
+    translator = AlchemyExprTranslator
+
+
+class AlchemyClient(SQLClient):
+
+    dialect = AlchemyDialect
+    sync_query = AlchemyQuery
+
+    @property
+    def async_query(self):
+        raise NotImplementedError
+
+    def create_table(self, name, expr=None, schema=None, database=None):
+        pass
+
+    def list_tables(self, like=None, database=None):
+        """
+        List tables in the current (or indicated) database.
 
         Parameters
         ----------
-        url
-            URL with which to connect to a backend.
-        kwargs
-            Additional keyword arguments
+        like : string, default None
+          Checks for this string contained in name
+        database : string, default None
+          If not passed, uses the current/default database
 
         Returns
         -------
-        BaseBackend
-            A backend instance
+        tables : list of strings
         """
-        url = sa.engine.make_url(url)
+        if database is None:
+            database = self.current_database
+        names = self.con.table_names(schema=database)
+        if like is not None:
+            names = [x for x in names if like in x]
+        return names
 
-        kwargs = toolz.merge(
-            {
-                name: value
-                for name in ("host", "port", "database", "password")
-                if (value := getattr(url, name, None))
-            },
-            kwargs,
-        )
-        if username := url.username:
-            kwargs["user"] = username
-
-        kwargs.update(url.query)
-        self._convert_kwargs(kwargs)
-        return self.connect(**kwargs)
-
-    def _convert_kwargs(self, kwargs):
-        with suppress(KeyError):
-            kwargs["secure"] = bool(ast.literal_eval(kwargs["secure"]))
-
-    def do_connect(
-        self,
-        host: str = "localhost",
-        port: int | None = None,
-        database: str = "default",
-        user: str = "default",
-        password: str = "",
-        client_name: str = "ibis",
-        secure: bool | None = None,
-        compression: str | bool = True,
-        **kwargs: Any,
-    ):
-        """Create a ClickHouse client for use with Ibis.
+    def _execute(self, query, results=True):
+        return AlchemyProxy(self.con.execute(query))
 
-        Parameters
-        ----------
-        host
-            Host name of the clickhouse server
-        port
-            ClickHouse HTTP server's port. If not passed, the value depends on
-            whether `secure` is `True` or `False`.
-        database
-            Default database when executing queries
-        user
-            User to authenticate with
-        password
-            Password to authenticate with
-        client_name
-            Name of client that will appear in clickhouse server logs
-        secure
-            Whether or not to use an authenticated endpoint
-        compression
-            The kind of compression to use for requests. See
-            https://clickhouse.com/docs/en/integrations/python#compression for
-            more information.
-        kwargs
-            Client specific keyword arguments
-
-        Examples
-        --------
-        >>> import ibis
-        >>> client = ibis.clickhouse.connect()
-        >>> client
-        <ibis.clickhouse.client.ClickhouseClient object at 0x...>
-        """
-        self.con = cc.get_client(
-            host=host,
-            # 8123 is the default http port 443 is https
-            port=port if port is not None else 443 if secure else 8123,
-            database=database,
-            user=user,
-            password=password,
-            client_name=client_name,
-            query_limit=0,
-            compress=compression,
-            **kwargs,
-        )
+    def _build_ast(self, expr):
+        return build_ast(expr, dialect=self.dialect)
+
+    def _get_sqla_table(self, name):
+        return sa.Table(name, self.meta, autoload=True)
+
+    def _sqla_table_to_expr(self, table):
+        node = AlchemyTable(table, self)
+        return self._table_expr_klass(node)
+
+
+class AlchemySelect(Select):
+
+    def __init__(self, *args, **kwargs):
+        self.exists = kwargs.pop('exists', False)
+        Select.__init__(self, *args, **kwargs)
+
+    def compile(self):
+        # Can't tell if this is a hack or not. Revisit later
+        self.context.set_query(self)
+
+        self._compile_subqueries()
+
+        frag = self._compile_table_set()
+        steps = [self._add_select,
+                 self._add_groupby,
+                 self._add_where,
+                 self._add_order_by,
+                 self._add_limit]
+
+        for step in steps:
+            frag = step(frag)
+
+        return frag
+
+    def _compile_subqueries(self):
+        if len(self.subqueries) == 0:
+            return
+
+        for expr in self.subqueries:
+            result = self.context.get_compiled_expr(expr)
+            alias = self.context.get_ref(expr)
+            result = result.cte(alias)
+            self.context.set_table(expr, result)
+
+    def _compile_table_set(self):
+        if self.table_set is not None:
+            helper = _AlchemyTableSet(self, self.table_set)
+            return helper.get_result()
+        else:
+            return None
+
+    def _add_select(self, table_set):
+        to_select = []
+        for expr in self.select_set:
+            if isinstance(expr, ir.ValueExpr):
+                arg = self._translate(expr, named=True)
+            elif isinstance(expr, ir.TableExpr):
+                if expr.equals(self.table_set):
+                    cached_table = self.context.get_table(expr)
+                    if cached_table is None:
+                        # the select * case from materialized join
+                        arg = '*'
+                    else:
+                        arg = table_set
+                else:
+                    arg = self.context.get_table(expr)
+                    if arg is None:
+                        raise ValueError(expr)
+
+            to_select.append(arg)
+
+        if self.exists:
+            clause = sa.exists(to_select)
+        else:
+            clause = sa.select(to_select)
+
+        if self.distinct:
+            clause = clause.distinct()
+
+        if table_set is not None:
+            return clause.select_from(table_set)
+        else:
+            return clause
+
+    def _add_groupby(self, fragment):
+        # GROUP BY and HAVING
+        if not len(self.group_by):
+            return fragment
+
+        group_keys = [self._translate(arg) for arg in self.group_by]
+        fragment = fragment.group_by(*group_keys)
+
+        if len(self.having) > 0:
+            having_args = [self._translate(arg) for arg in self.having]
+            having_clause = _and_all(having_args)
+            fragment = fragment.having(having_clause)
+
+        return fragment
+
+    def _add_where(self, fragment):
+        if not len(self.where):
+            return fragment
+
+        args = [self._translate(pred, permit_subquery=True)
+                for pred in self.where]
+        clause = _and_all(args)
+        return fragment.where(clause)
+
+    def _add_order_by(self, fragment):
+        if not len(self.order_by):
+            return fragment
+
+        clauses = []
+        for expr in self.order_by:
+            key = expr.op()
+            sort_expr = key.expr
+
+            # here we have to determine if key.expr is in the select set (as it
+            # will be in the case of order_by fused with an aggregation
+            if _can_lower_sort_column(self.table_set, sort_expr):
+                arg = sort_expr.get_name()
+            else:
+                arg = self._translate(sort_expr)
+
+            if not key.ascending:
+                arg = sa.desc(arg)
+
+            clauses.append(arg)
+
+        return fragment.order_by(*clauses)
+
+    def _among_select_set(self, expr):
+        for other in self.select_set:
+            if expr.equals(other):
+                return True
+        return False
+
+    def _add_limit(self, fragment):
+        if self.limit is None:
+            return fragment
+
+        n, offset = self.limit['n'], self.limit['offset']
+        fragment = fragment.limit(n)
+        if offset is not None and offset != 0:
+            fragment = fragment.offset(offset)
+
+        return fragment
 
     @property
-    def version(self) -> str:
-        return self.con.server_version
+    def translator(self):
+        return self.dialect.translator
 
     @property
-    def current_database(self) -> str:
-        with closing(self.raw_sql("SELECT currentDatabase()")) as result:
-            [(db,)] = result.result_rows
-        return db
-
-    def list_databases(self, like: str | None = None) -> list[str]:
-        with closing(self.raw_sql("SELECT name FROM system.databases")) as result:
-            results = result.result_columns
+    def dialect(self):
+        return self.context.dialect
 
-        if results:
-            (databases,) = results
-        else:
-            databases = []
-        return self._filter_with_like(databases, like)
 
-    def list_tables(
-        self, like: str | None = None, database: str | None = None
-    ) -> list[str]:
-        query = "SHOW TABLES" + (f" FROM `{database}`" * (database is not None))
-        with closing(self.raw_sql(query)) as result:
-            results = result.result_columns
+class _AlchemyTableSet(TableSetFormatter):
 
-        if results:
-            (tables,) = results
+    def get_result(self):
+        # Got to unravel the join stack; the nesting order could be
+        # arbitrary, so we do a depth first search and push the join tokens
+        # and predicates onto a flat list, then format them
+        op = self.expr.op()
+
+        if isinstance(op, ops.Join):
+            self._walk_join_tree(op)
         else:
-            tables = []
-        return self._filter_with_like(tables, like)
+            self.join_tables.append(self._format_table(self.expr))
 
-    def _normalize_external_tables(self, external_tables=None) -> ExternalData | None:
-        """Merge registered external tables with any new external tables."""
-        external_data = ExternalData()
-        n = 0
-        for name, obj in (external_tables or {}).items():
-            n += 1
-            if not (schema := obj.schema):
-                raise TypeError(f'Schema is empty for external table {name}')
-
-            structure = [
-                f"{name} {serialize(typ.copy(nullable=not typ.is_nested()))}"
-                for name, typ in schema.items()
-            ]
-            external_data.add_file(
-                file_name=name,
-                data=obj.data.to_pyarrow_bytes(schema=schema),
-                structure=structure,
-                fmt="Arrow",
-            )
-        if not n:
-            return None
-        return external_data
+        result = self.join_tables[0]
+        for jtype, table, preds in zip(self.join_types,
+                                       self.join_tables[1:],
+                                       self.join_predicates):
+            if len(preds):
+                sqla_preds = [self._translate(pred) for pred in preds]
+                onclause = _and_all(sqla_preds)
+            else:
+                onclause = None
+
+            if jtype in (ops.InnerJoin, ops.CrossJoin):
+                result = result.join(table, onclause)
+            elif jtype is ops.LeftJoin:
+                result = result.join(table, onclause, isouter=True)
+            elif jtype is ops.RightJoin:
+                result = table.join(result, onclause, isouter=True)
+            elif jtype is ops.OuterJoin:
+                result = result.outerjoin(table, onclause)
+            else:
+                raise NotImplementedError(jtype)
+
+        return result
+
+    def _get_join_type(self, op):
+        return type(op)
+
+    def _format_table(self, expr):
+        ctx = self.context
+        ref_expr = expr
+        op = ref_op = expr.op()
+
+        if isinstance(op, ops.SelfReference):
+            ref_expr = op.table
+            ref_op = ref_expr.op()
 
-    def _collect_in_memory_tables(
-        self, expr: ir.TableExpr | None, *, external_tables: Mapping | None = None
-    ):
-        return toolz.merge(
-            (
-                {op.name: op for op in an.find_memtables(expr.op())}
-                if expr is not None
-                else {}
-            ),
-            external_tables or {},
-        )
-
-    def to_pyarrow(
-        self,
-        expr: ir.Expr,
-        *,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        limit: int | str | None = None,
-        external_tables: Mapping[str, Any] | None = None,
-        **kwargs: Any,
-    ):
-        # we convert to batches first to avoid a bunch of rigmarole resulting
-        # from the following rough edges
-        #
-        # 1. clickhouse's awkward
-        #    client-settings-are-permissioned-on-the-server "feature"
-        # 2. the bizarre conversion of `DateTime64` without scale to arrow
-        #    uint32 inside of clickhouse
-        # 3. the fact that uint32 cannot be cast to pa.timestamp without first
-        #    casting it to int64
-        #
-        # the extra code to make this dance work without first converting to
-        # record batches isn't worth it without some benchmarking
-        with self.to_pyarrow_batches(
-            expr=expr,
-            params=params,
-            limit=limit,
-            external_tables=external_tables,
-            **kwargs,
-        ) as reader:
-            table = reader.read_all()
-
-        return expr.__pyarrow_result__(table)
-
-    def to_pyarrow_batches(
-        self,
-        expr: ir.Expr,
-        *,
-        limit: int | str | None = None,
-        params: Mapping[ir.Scalar, Any] | None = None,
-        external_tables: Mapping[str, Any] | None = None,
-        chunk_size: int = 1_000_000,
-        **_: Any,
-    ) -> pa.ipc.RecordBatchReader:
-        """Execute expression and return an iterator of pyarrow record batches.
+        alias = ctx.get_ref(expr)
 
-        This method is eager and will execute the associated expression
-        immediately.
+        if isinstance(ref_op, AlchemyTable):
+            result = ref_op.sqla_table
+        else:
+            # A subquery
+            if ctx.is_extracted(ref_expr):
+                # Was put elsewhere, e.g. WITH block, we just need to grab
+                # its alias
+                alias = ctx.get_ref(expr)
+
+                # hack
+                if isinstance(op, ops.SelfReference):
+                    table = ctx.get_table(ref_expr)
+                    self_ref = table.alias(alias)
+                    ctx.set_table(expr, self_ref)
+                    return self_ref
+                else:
+                    return ctx.get_table(expr)
+
+            result = ctx.get_compiled_expr(expr)
+            alias = ctx.get_ref(expr)
+
+        result = result.alias(alias)
+        ctx.set_table(expr, result)
+        return result
+
+
+def _can_lower_sort_column(table_set, expr):
+    # we can currently sort by just-appeared aggregate metrics, but the way
+    # these are references in the expression DSL is as a SortBy (blocking
+    # table operation) on an aggregation. There's a hack in _collect_SortBy
+    # in the generic SQL compiler that "fuses" the sort with the
+    # aggregation so they appear in same query. It's generally for
+    # cosmetics and doesn't really affect query semantics.
+    bases = ir.find_all_base_tables(expr)
+    if len(bases) > 1:
+        return False
+
+    base = list(bases.values())[0]
+    base_op = base.op()
+
+    if isinstance(base_op, ops.Aggregation):
+        return base_op.table.equals(table_set)
+    elif isinstance(base_op, ops.Projection):
+        return base.equals(table_set)
+    else:
+        return False
+
+
+def _and_all(clauses):
+    result = clauses[0]
+    for clause in clauses[1:]:
+        result = sql.and_(result, clause)
+    return result
 
-        Parameters
-        ----------
-        expr
-            Ibis expression to export to pyarrow
-        limit
-            An integer to effect a specific row limit. A value of `None` means
-            "no limit". The default is in `ibis/config.py`.
-        params
-            Mapping of scalar parameter expressions to value.
-        external_tables
-            External data
-        chunk_size
-            Maximum number of row to return in a single chunk
 
-        Returns
-        -------
-        results
-            RecordBatchReader
+class AlchemyUnion(Union):
 
-        Notes
-        -----
-        There are a variety of ways to implement clickhouse -> record batches.
-
-        1. FORMAT ArrowStream -> record batches via raw_query
-           This has the same type conversion problem(s) as `to_pyarrow`.
-           It's harder to address due to lack of `cast` on `RecordBatch`.
-           However, this is a ClickHouse problem: we should be able to get
-           string data out without a bunch of settings/permissions rigmarole.
-        2. Native -> Python objects -> pyarrow batches
-           This is what is implemented, using `query_column_block_stream`.
-        3. Native -> Python objects -> DataFrame chunks -> pyarrow batches
-           This is not implemented because it adds an unnecessary pandas step in
-           between Python object -> arrow. We can go directly to record batches
-           without pandas in the middle.
-        """
-        table = expr.as_table()
-        sql = self.compile(table, limit=limit, params=params)
+    def compile(self):
+        context = self.context
 
-        external_data = self._normalize_external_tables(
-            self._collect_in_memory_tables(expr, external_tables=external_tables)
-        )
-
-        def batcher(sql: str, *, schema: pa.Schema) -> Iterator[pa.RecordBatch]:
-            settings = {}
-
-            # readonly != 1 means that the server setting is writable
-            if self.con.server_settings["max_block_size"].readonly != 1:
-                settings["max_block_size"] = chunk_size
-            with self.con.query_column_block_stream(
-                sql, external_data=external_data, settings=settings
-            ) as blocks:
-                yield from map(
-                    partial(pa.RecordBatch.from_arrays, schema=schema), blocks
-                )
-
-        self._log(sql)
-        schema = table.schema().to_pyarrow()
-        return pa.RecordBatchReader.from_batches(schema, batcher(sql, schema=schema))
-
-    def execute(
-        self,
-        expr: ir.Expr,
-        limit: str | None = 'default',
-        external_tables: Mapping[str, pd.DataFrame] | None = None,
-        **kwargs: Any,
-    ) -> Any:
-        """Execute an expression."""
-        import pandas as pd
+        if self.distinct:
+            sa_func = sa.union
+        else:
+            sa_func = sa.union_all
 
-        table = expr.as_table()
-        sql = self.compile(table, limit=limit, **kwargs)
+        left_set = context.get_compiled_expr(self.left)
+        right_set = context.get_compiled_expr(self.right)
 
-        schema = table.schema()
-        self._log(sql)
+        return sa_func(left_set, right_set)
 
-        external_tables = self._collect_in_memory_tables(
-            expr, external_tables=toolz.valmap(_to_memtable, external_tables or {})
-        )
-        external_data = self._normalize_external_tables(external_tables)
-        df = self.con.query_df(
-            sql, external_data=external_data, use_na_values=False, use_none=True
-        )
-
-        if df.empty:
-            df = pd.DataFrame(columns=schema.names)
-
-        # TODO: remove the extra conversion
-        #
-        # the extra __pandas_result__ call is to work around slight differences
-        # in single column conversion and whole table conversion
-        return expr.__pandas_result__(table.__pandas_result__(df))
-
-    def compile(self, expr: ir.Expr, limit: str | None = None, params=None, **_: Any):
-        table_expr = expr.as_table()
-
-        if limit == "default":
-            limit = ibis.options.sql.default_limit
-        if limit is not None:
-            table_expr = table_expr.limit(limit)
-
-        if params is None:
-            params = {}
-
-        sql = translate(table_expr.op(), params=params)
-        assert not isinstance(sql, sg.exp.Subquery)
-
-        if isinstance(sql, sg.exp.Table):
-            sql = sg.select("*").from_(sql)
 
-        assert not isinstance(sql, sg.exp.Subquery)
-        return sql.sql(dialect="clickhouse", pretty=True)
+class AlchemyProxy(object):
+    """
+    Wraps a SQLAlchemy ResultProxy and ensures that .close() is called on
+    garbage collection
+    """
+    def __init__(self, proxy):
+        self.proxy = proxy
 
-    def _to_sql(self, expr: ir.Expr, **kwargs) -> str:
-        return str(self.compile(expr, **kwargs))
+    def __del__(self):
+        self._close_cursor()
 
-    def table(self, name: str, database: str | None = None) -> ir.Table:
-        """Construct a table expression.
+    def _close_cursor(self):
+        self.proxy.close()
 
-        Parameters
-        ----------
-        name
-            Table name
-        database
-            Database name
+    def __enter__(self):
+        return self
 
-        Returns
-        -------
-        Table
-            Table expression
-        """
-        schema = self.get_schema(name, database=database)
-        qname = self._fully_qualified_name(name, database)
-        return ClickhouseTable(ops.DatabaseTable(qname, schema, self))
-
-    def raw_sql(
-        self,
-        query: str,
-        external_tables: Mapping[str, pd.DataFrame] | None = None,
-        **kwargs,
-    ) -> Any:
-        """Execute a SQL string `query` against the database.
+    def __exit__(self, type, value, tb):
+        self._close_cursor()
 
-        Parameters
-        ----------
-        query
-            Raw SQL string
-        external_tables
-            Mapping of table name to pandas DataFrames providing
-            external datasources for the query
-        kwargs
-            Backend specific query arguments
+    def fetchall(self):
+        return self.proxy.fetchall()
 
-        Returns
-        -------
-        Cursor
-            Clickhouse cursor
-        """
-        external_tables = toolz.valmap(_to_memtable, external_tables or {})
-        external_data = self._normalize_external_tables(external_tables)
-        self._log(query)
-        return self.con.query(query, external_data=external_data, **kwargs)
 
-    def fetch_from_cursor(self, cursor, schema):
-        import pandas as pd
+@rewrites(ops.NullIfZero)
+def _nullifzero(expr):
+    arg = expr.op().args[0]
+    return (arg == 0).ifelse(ibis.NA, arg)
 
-        from ibis.formats.pandas import PandasData
 
-        df = pd.DataFrame.from_records(iter(cursor), columns=schema.names)
-        return PandasData.convert_table(df, schema)
+@compiles(ops.Divide)
+def _true_divide(t, expr):
+    op = expr.op()
+    left, right = op.args
 
-    def close(self) -> None:
-        """Close ClickHouse connection."""
-        self.con.close()
-
-    def _fully_qualified_name(self, name: str, database: str | None) -> str:
-        return sg.table(name, db=database or self.current_database or None).sql(
-            dialect="clickhouse"
-        )
+    if util.all_of(op.args, ir.IntegerValue):
+        new_expr = left.div(right.cast('double'))
+        return t.translate(new_expr)
 
-    def get_schema(self, table_name: str, database: str | None = None) -> sch.Schema:
-        """Return a Schema object for the indicated table and database.
+    return fixed_arity(lambda x, y: x / y, 2)(t, expr)
 
-        Parameters
-        ----------
-        table_name
-            May **not** be fully qualified. Use `database` if you want to
-            qualify the identifier.
-        database
-            Database name
 
-        Returns
-        -------
-        sch.Schema
-            Ibis schema
-        """
-        qualified_name = self._fully_qualified_name(table_name, database)
-        query = f"DESCRIBE {qualified_name}"
-        with closing(self.raw_sql(query)) as results:
-            names, types, *_ = results.result_columns
-        return sch.Schema(dict(zip(names, map(parse, types))))
-
-    def _get_schema_using_query(self, query: str) -> sch.Schema:
-        query = f"EXPLAIN json = 1, description = 0, header = 1 {query}"
-        with closing(self.raw_sql(query)) as results:
-            [[raw_plans]] = results.result_columns
-        [plan] = json.loads(raw_plans)
-        return sch.Schema(
-            {field["Name"]: parse(field["Type"]) for field in plan["Plan"]["Header"]}
-        )
-
-    @classmethod
-    def has_operation(cls, operation: type[ops.Value]) -> bool:
-        from ibis.backends.clickhouse.compiler.values import translate_val
-
-        return translate_val.dispatch(operation) is not translate_val.dispatch(object)
-
-    def create_database(
-        self, name: str, *, force: bool = False, engine: str = "Atomic"
-    ) -> None:
-        if_not_exists = "IF NOT EXISTS " * force
-        with closing(
-            self.raw_sql(f"CREATE DATABASE {if_not_exists}{name} ENGINE = {engine}")
-        ):
-            pass
-
-    def drop_database(self, name: str, *, force: bool = False) -> None:
-        if_exists = "IF EXISTS " * force
-        with closing(self.raw_sql(f"DROP DATABASE {if_exists}{name}")):
-            pass
-
-    def truncate_table(self, name: str, database: str | None = None) -> None:
-        ident = self._fully_qualified_name(name, database)
-        with closing(self.raw_sql(f"TRUNCATE TABLE {ident}")):
-            pass
-
-    def drop_table(
-        self, name: str, database: str | None = None, force: bool = False
-    ) -> None:
-        ident = self._fully_qualified_name(name, database)
-        with closing(self.raw_sql(f"DROP TABLE {'IF EXISTS ' * force}{ident}")):
-            pass
-
-    def create_table(
-        self,
-        name: str,
-        obj: pd.DataFrame | pa.Table | ir.Table | None = None,
-        *,
-        schema: ibis.Schema | None = None,
-        database: str | None = None,
-        temp: bool = False,
-        overwrite: bool = False,
-        # backend specific arguments
-        engine: str = "File(Native)",
-        order_by: Iterable[str] | None = None,
-        partition_by: Iterable[str] | None = None,
-        sample_by: str | None = None,
-        settings: Mapping[str, Any] | None = None,
-    ) -> ir.Table:
-        """Create a table in a ClickHouse database.
+@compiles(ops.FloorDivide)
+def _floor_divide(t, expr):
+    op = expr.op()
+    left, right = op.args
 
-        Parameters
-        ----------
-        name
-            Name of the table to create
-        obj
-            Optional data to create the table with
-        schema
-            Optional names and types of the table
-        database
-            Database to create the table in
-        temp
-            Create a temporary table. This is not yet supported, and exists for
-            API compatibility.
-        overwrite
-            Whether to overwrite the table
-        engine
-            The table engine to use. See [ClickHouse's `CREATE TABLE` documentation](https://clickhouse.com/docs/en/sql-reference/statements/create/table)
-            for specifics.
-        order_by
-            String column names to order by. Required for some table engines like `MergeTree`.
-        partition_by
-            String column names to partition by
-        sample_by
-            String column names to sample by
-        settings
-            Key-value pairs of settings for table creation
+    if util.any_of(op.args, ir.FloatingValue):
+        new_expr = expr.floor()
+        return t.translate(new_expr)
 
-        Returns
-        -------
-        Table
-            The new table
-        """
-        if temp:
-            raise com.IbisError(
-                "ClickHouse temporary tables are not yet supported due to a bug in `clickhouse_driver`"
-            )
-
-        tmp = "TEMPORARY " * temp
-        replace = "OR REPLACE " * overwrite
-        table = self._fully_qualified_name(name, database)
-        code = f"CREATE {replace}{tmp}TABLE {table}"
-
-        if obj is None and schema is None:
-            raise com.IbisError("The schema or obj parameter is required")
-
-        if obj is not None and not isinstance(obj, ir.Expr):
-            obj = ibis.memtable(obj, schema=schema)
-
-        if schema is None:
-            schema = obj.schema()
-
-        serialized_schema = ", ".join(
-            f"`{name}` {serialize(typ)}" for name, typ in schema.items()
-        )
-
-        code += f" ({serialized_schema}) ENGINE = {engine}"
-
-        if order_by is not None:
-            code += f" ORDER BY {', '.join(util.promote_list(order_by))}"
-
-        if partition_by is not None:
-            code += f" PARTITION BY {', '.join(util.promote_list(partition_by))}"
-
-        if sample_by is not None:
-            code += f" SAMPLE BY {sample_by}"
-
-        if settings:
-            kvs = ", ".join(f"{name}={value!r}" for name, value in settings.items())
-            code += f" SETTINGS {kvs}"
-
-        if obj is not None:
-            code += f" AS {self.compile(obj)}"
-
-        external_tables = self._collect_in_memory_tables(obj)
-
-        # create the table
-        self.con.raw_query(
-            code, external_data=self._normalize_external_tables(external_tables)
-        )
-
-        return self.table(name, database=database)
-
-    def create_view(
-        self,
-        name: str,
-        obj: ir.Table,
-        *,
-        database: str | None = None,
-        overwrite: bool = False,
-    ) -> ir.Table:
-        qualname = self._fully_qualified_name(name, database)
-        replace = "OR REPLACE " * overwrite
-        query = self.compile(obj)
-        code = f"CREATE {replace}VIEW {qualname} AS {query}"
-        with closing(
-            self.raw_sql(code, external_tables=self._collect_in_memory_tables(obj))
-        ):
-            pass
-        return self.table(name, database=database)
-
-    def drop_view(
-        self, name: str, *, database: str | None = None, force: bool = False
-    ) -> None:
-        name = self._fully_qualified_name(name, database)
-        if_exists = "IF EXISTS " * force
-        with closing(self.raw_sql(f"DROP VIEW {if_exists}{name}")):
-            pass
+    return fixed_arity(lambda x, y: x / y, 2)(t, expr)
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/impala/__init__.py` & `ibis-framework-v0.6.0/ibis/impala/client.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,129 +1,297 @@
-"""Impala backend."""
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from __future__ import annotations
-
-import contextlib
-import io
-import operator
-import os
+from posixpath import join as pjoin
 import re
+import six
+import threading
+import time
 import weakref
-from functools import cached_property
-from posixpath import join as pjoin
-from typing import TYPE_CHECKING, Any, Literal
 
-import fsspec
+import hdfs
 import numpy as np
-import sqlglot as sg
+import pandas as pd
 
-import ibis.common.exceptions as com
-import ibis.config
+import ibis.common as com
+
+from ibis.config import options
+from ibis.client import (Query, AsyncQuery, Database,
+                         DatabaseEntity, SQLClient)
+from ibis.compat import lzip
+from ibis.filesystems import HDFS, WebHDFS
+from ibis.impala import udf, ddl
+from ibis.impala.compat import impyla, ImpylaError, HS2Error
+from ibis.impala.compiler import build_ast
+from ibis.util import log
+from ibis.sql.compiler import DDL
 import ibis.expr.datatypes as dt
-import ibis.expr.rules as rlz
-import ibis.expr.schema as sch
+import ibis.expr.operations as ops
 import ibis.expr.types as ir
-from ibis import util
-from ibis.backends.base.sql import BaseSQLBackend
-from ibis.backends.base.sql.ddl import (
-    CTAS,
-    CreateDatabase,
-    CreateTableWithSchema,
-    CreateView,
-    DropDatabase,
-    DropTable,
-    DropView,
-    TruncateTable,
-    fully_qualified_re,
-    is_fully_qualified,
-)
-from ibis.backends.impala import ddl, udf
-from ibis.backends.impala.client import ImpalaConnection, ImpalaDatabase, ImpalaTable
-from ibis.backends.impala.compat import HS2Error, ImpylaError
-from ibis.backends.impala.compiler import ImpalaCompiler
-from ibis.backends.impala.pandas_interop import DataFrameWriter
-from ibis.backends.impala.udf import (
-    aggregate_function,
-    scalar_function,
-    wrap_uda,
-    wrap_udf,
-)
-from ibis.config import options
-from ibis.formats.pandas import PandasData
+import ibis.util as util
 
-if TYPE_CHECKING:
-    from pathlib import Path
 
-    import pandas as pd
+if six.PY2:
+    import Queue as queue
+else:
+    import queue
 
 
-__all__ = (
-    "Backend",
-    "aggregate_function",
-    "scalar_function",
-    "wrap_uda",
-    "wrap_udf",
-)
+class ImpalaDatabase(Database):
 
-_HS2_TTypeId_to_dtype = {
-    'BOOLEAN': 'bool',
-    'TINYINT': 'int8',
-    'SMALLINT': 'int16',
-    'INT': 'int32',
-    'BIGINT': 'int64',
-    'TIMESTAMP': 'datetime64[ns]',
-    'FLOAT': 'float32',
-    'DOUBLE': 'float64',
-    'STRING': 'object',
-    'DECIMAL': 'object',
-    'BINARY': 'object',
-    'VARCHAR': 'object',
-    'CHAR': 'object',
-    'DATE': 'datetime64[ns]',
-    'VOID': None,
-}
+    def create_table(self, table_name, obj=None, **kwargs):
+        """
+        Dispatch to ImpalaClient.create_table. See docs for more
+        """
+        return self.client.create_table(table_name, obj=obj,
+                                        database=self.name, **kwargs)
 
+    def list_udfs(self, like=None):
+        return self.client.list_udfs(like=self._qualify_like(like),
+                                     database=self.name)
 
-def _split_signature(x):
-    name, rest = x.split('(', 1)
-    return name, rest[:-1]
+    def list_udas(self, like=None):
+        return self.client.list_udas(like=self._qualify_like(like),
+                                     database=self.name)
 
 
-_arg_type = re.compile(r'(.*)\.\.\.|([^\.]*)')
+class ImpalaConnection(object):
 
+    """
+    Database connection wrapper
+    """
 
-class _type_parser:
-    NORMAL, IN_PAREN = 0, 1
+    def __init__(self, pool_size=8, database='default', **params):
+        self.params = params
+        self.database = database
 
-    def __init__(self, value):
-        self.value = value
-        self.state = self.NORMAL
-        self.buf = io.StringIO()
-        self.types = []
-        for c in value:
-            self._step(c)
-        self._push()
+        self.lock = threading.Lock()
 
-    def _push(self):
-        val = self.buf.getvalue().strip()
-        if val:
-            self.types.append(val)
-        self.buf = io.StringIO()
+        self.options = {}
 
-    def _step(self, c):
-        if self.state == self.NORMAL:
-            if c == '(':
-                self.state = self.IN_PAREN
-            elif c == ',':
-                self._push()
-                return
-        elif self.state == self.IN_PAREN:
-            if c == ')':
-                self.state = self.NORMAL
-        self.buf.write(c)
+        self.connection_pool = queue.Queue(pool_size)
+        self.connection_pool_size = 0
+        self.max_pool_size = pool_size
+
+        self._connections = weakref.WeakValueDictionary()
+
+        self.ping()
+
+    def set_options(self, options):
+        self.options.update(options)
+
+    def close(self):
+        """
+        Close all open Impyla sessions
+        """
+        for k, con in self._connections.items():
+            con.close()
+
+    def set_database(self, name):
+        self.database = name
+
+    def disable_codegen(self, disabled=True):
+        key = 'DISABLE_CODEGEN'
+        if disabled:
+            self.options[key] = '1'
+        elif key in self.options:
+            del self.options[key]
+
+    def execute(self, query, async=False):
+        if isinstance(query, DDL):
+            query = query.compile()
+
+        cursor = self._get_cursor()
+        self.log(query)
+
+        try:
+            cursor.execute(query, async=async)
+        except:
+            cursor.release()
+
+            import traceback
+            buf = six.StringIO()
+            traceback.print_exc(file=buf)
+            self.error('Exception caused by {0}: {1}'.format(query,
+                                                             buf.getvalue()))
+            raise
+
+        return cursor
+
+    def log(self, msg):
+        log(msg)
+
+    def error(self, msg):
+        self.log(msg)
+
+    def fetchall(self, query):
+        with self.execute(query) as cur:
+            results = cur.fetchall()
+        return results
+
+    def _get_cursor(self):
+        try:
+            cur = self.connection_pool.get(False)
+            if (cur.database != self.database or
+                    cur.options != self.options):
+                cur = self._new_cursor()
+
+            return cur
+        except queue.Empty:
+            if self.connection_pool_size < self.max_pool_size:
+                cursor = self._new_cursor()
+                self.connection_pool_size += 1
+                return cursor
+            else:
+                raise com.InternalError('Too many concurrent / hung queries')
+
+    def _new_cursor(self):
+        params = self.params.copy()
+        con = impyla.connect(database=self.database, **params)
+
+        self._connections[id(con)] = con
+
+        # make sure the connection works
+        cursor = con.cursor(convert_types=True)
+        cursor.ping()
+
+        wrapper = ImpalaCursor(cursor, self, con, self.database,
+                               self.options.copy())
+        wrapper.set_options()
+
+        return wrapper
+
+    def ping(self):
+        self._new_cursor()
+
+    def release(self, cur):
+        self.connection_pool.put(cur)
+
+
+class ImpalaCursor(object):
+
+    def __init__(self, cursor, con, impyla_con, database,
+                 options):
+        self.cursor = cursor
+        self.con = con
+        self.impyla_con = impyla_con
+        self.database = database
+        self.options = options
+
+    def __del__(self):
+        self._close_cursor()
+
+    def _close_cursor(self):
+        try:
+            self.cursor.close()
+        except HS2Error as e:
+            # connection was closed elsewhere
+            if 'invalid session' not in e.args[0].lower():
+                raise
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, type, value, tb):
+        self.release()
+
+    def set_options(self):
+        for k, v in self.options.items():
+            query = 'SET {0}={1}'.format(k, v)
+            self.cursor.execute(query)
+
+    @property
+    def description(self):
+        return self.cursor.description
+
+    def release(self):
+        self.con.release(self)
+
+    def execute(self, stmt, async=False):
+        self.cursor.execute_async(stmt)
+        if async:
+            return
+        else:
+            self._wait_synchronous()
+
+    def _wait_synchronous(self):
+        # Wait to finish, but cancel if KeyboardInterrupt
+        from impala.hiveserver2 import OperationalError
+        loop_start = time.time()
+
+        def _sleep_interval(start_time):
+            elapsed = time.time() - start_time
+            if elapsed < 0.05:
+                return 0.01
+            elif elapsed < 1.0:
+                return 0.05
+            elif elapsed < 10.0:
+                return 0.1
+            elif elapsed < 60.0:
+                return 0.5
+            return 1.0
+
+        cur = self.cursor
+        try:
+            while True:
+                state = cur.status()
+                if self.cursor._op_state_is_error(state):
+                    raise OperationalError("Operation is in ERROR_STATE")
+                if not cur._op_state_is_executing(state):
+                    break
+                time.sleep(_sleep_interval(loop_start))
+        except KeyboardInterrupt:
+            print('Canceling query')
+            self.cancel()
+            raise
+
+    def is_finished(self):
+        return not self.is_executing()
+
+    def is_executing(self):
+        return self.cursor.is_executing()
+
+    def cancel(self):
+        self.cursor.cancel_operation()
+
+    def fetchall(self, columnar=False):
+        if columnar:
+            return self.cursor.fetchcolumnar()
+        else:
+            return self.cursor.fetchall()
+
+
+class ImpalaQuery(Query):
+
+    def _fetch(self, cursor):
+        batches = cursor.fetchall(columnar=True)
+        names = [x[0] for x in cursor.description]
+        return _column_batches_to_dataframe(names, batches)
+
+    def _db_type_to_dtype(self, db_type):
+        return _HS2_TTypeId_to_dtype[db_type]
+
+
+def _column_batches_to_dataframe(names, batches):
+    from ibis.compat import zip as czip
+    cols = {}
+    for name, chunks in czip(names, czip(*[b.columns for b in batches])):
+        cols[name] = _chunks_to_pandas_array(chunks)
+    return pd.DataFrame(cols, columns=names)
 
 
 def _chunks_to_pandas_array(chunks):
     total_length = 0
     have_nulls = False
     for c in chunks:
         total_length += len(c)
@@ -131,28 +299,28 @@
 
     type_ = chunks[0].data_type
     numpy_type = _HS2_TTypeId_to_dtype[type_]
 
     def fill_nonnull(target, chunks):
         pos = 0
         for c in chunks:
-            target[pos : pos + len(c)] = c.values
+            target[pos: pos + len(c)] = c.values
             pos += len(c.values)
 
     def fill(target, chunks, na_rep):
         pos = 0
         for c in chunks:
             nulls = c.nulls.copy()
             nulls.bytereverse()
             bits = np.frombuffer(nulls.tobytes(), dtype='u1')
             mask = np.unpackbits(bits).view(np.bool_)
 
             k = len(c)
 
-            dest = target[pos : pos + k]
+            dest = target[pos: pos + k]
             dest[:] = c.values
             dest[mask[:k]] = na_rep
 
             pos += k
 
     if have_nulls:
         if numpy_type in ('bool', 'datetime64[ns]'):
@@ -169,1188 +337,1620 @@
     else:
         target = np.empty(total_length, dtype=numpy_type)
         fill_nonnull(target, chunks)
 
     return target
 
 
-def _column_batches_to_dataframe(names, batches):
-    import pandas as pd
+class ImpalaAsyncQuery(ImpalaQuery, AsyncQuery):
 
-    cols = {}
-    for name, chunks in zip(names, zip(*(b.columns for b in batches))):
-        cols[name] = _chunks_to_pandas_array(chunks)
-    return pd.DataFrame(cols, columns=names)
+    def __init__(self, client, ddl):
+        super(ImpalaAsyncQuery, self).__init__(client, ddl)
+        self._cursor = None
+        self._exception = None
+        self._execute_thread = None
+        self._execute_complete = False
+        self._operation_active = False
+
+    def __del__(self):
+        if self._cursor is not None:
+            self._cursor.release()
+
+    def execute(self):
+        if self._operation_active:
+            raise com.IbisError('operation already active')
+        con = self.client.con
+
+        # XXX: there is codegen overhead somewhere which causes execute_async
+        # to block, unfortunately. This threading hack works around it
+        def _async_execute():
+            try:
+                self._cursor = con.execute(self.compiled_ddl, async=True)
+            except Exception as e:
+                self._exception = e
+            self._execute_complete = True
+
+        self._execute_complete = False
+        self._operation_active = True
+        self._execute_thread = threading.Thread(target=_async_execute)
+        self._execute_thread.start()
+        return self
+
+    def _wait_execute(self):
+        if not self._operation_active:
+            raise com.IbisError('No active query')
+        if self._execute_thread.is_alive():
+            self._execute_thread.join()
+        elif self._exception is not None:
+            raise self._exception
 
+    def is_finished(self):
+        """
+        Return True if the operation is finished
+        """
+        from impala.error import ProgrammingError
+        self._wait_execute()
+        try:
+            return self._cursor.is_finished()
+        except ProgrammingError as e:
+            if 'state is not available' in e.args[0]:
+                return True
+            raise
 
-class Backend(BaseSQLBackend):
-    name = 'impala'
-    compiler = ImpalaCompiler
-
-    _sqlglot_dialect = "hive"  # not 100% accurate, but very close
-    _top_level_methods = ("hdfs_connect",)
-
-    class Options(ibis.config.Config):
-        """Impala specific options.
-
-        Parameters
-        ----------
-        temp_db : str, default "__ibis_tmp"
-            Database to use for temporary objects.
-        temp_hdfs_path : str, default "/tmp/ibis"
-            HDFS path for storage of temporary data.
-        """
-
-        temp_db: str = "__ibis_tmp"
-        temp_hdfs_path: str = "/tmp/hdfs"
-
-    @staticmethod
-    def hdfs_connect(
-        *args: Any,
-        protocol: str = "webhdfs",
-        **kwargs: Any,
-    ) -> fsspec.spec.AbstractFileSystem:
-        return fsspec.filesystem(protocol, *args, **kwargs)
-
-    def do_connect(
-        self,
-        host: str = "localhost",
-        port: int = 21050,
-        database: str = "default",
-        timeout: int = 45,
-        use_ssl: bool = False,
-        ca_cert: str | Path | None = None,
-        user: str | None = None,
-        password: str | None = None,
-        auth_mechanism: Literal["NOSASL", "PLAIN", "GSSAPI", "LDAP"] = "NOSASL",
-        kerberos_service_name: str = "impala",
-        pool_size: int = 8,
-        hdfs_client: fsspec.spec.AbstractFileSystem | None = None,
-    ):
-        """Create an Impala Backend for use with Ibis.
-
-        Parameters
-        ----------
-        host
-            Host name of the impalad or HiveServer2 in Hive
-        port
-            Impala's HiveServer2 port
-        database
-            Default database when obtaining new cursors
-        timeout
-            Connection timeout in seconds when communicating with HiveServer2
-        use_ssl
-            Use SSL when connecting to HiveServer2
-        ca_cert
-            Local path to 3rd party CA certificate or copy of server
-            certificate for self-signed certificates. If SSL is enabled, but
-            this argument is ``None``, then certificate validation is skipped.
-        user
-            LDAP user to authenticate
-        password
-            LDAP password to authenticate
-        auth_mechanism
-            |   Value    | Meaning                        |
-            | :--------: | :----------------------------- |
-            | `'NOSASL'` | insecure Impala connections    |
-            | `'PLAIN'`  | insecure Hive clusters         |
-            |  `'LDAP'`  | LDAP authenticated connections |
-            | `'GSSAPI'` | Kerberos-secured clusters      |
-        kerberos_service_name
-            Specify a particular `impalad` service principal.
-        pool_size
-            Size of the connection pool. Typically this is not necessary to configure.
-        hdfs_client
-            An existing HDFS client.
+    def cancel(self):
+        """
+        Cancel the query (or attempt to)
+        """
+        self._wait_execute()
+        return self._cursor.cancel()
 
-        Examples
-        --------
-        >>> import os
-        >>> import ibis
-        >>> hdfs_host = os.environ.get('IBIS_TEST_NN_HOST', 'localhost')
-        >>> hdfs_port = int(os.environ.get('IBIS_TEST_NN_PORT', 50070))
-        >>> impala_host = os.environ.get('IBIS_TEST_IMPALA_HOST', 'localhost')
-        >>> impala_port = int(os.environ.get('IBIS_TEST_IMPALA_PORT', 21050))
-        >>> hdfs = ibis.impala.hdfs_connect(host=hdfs_host, port=hdfs_port)
-        >>> client = ibis.impala.connect(
-        ...     host=impala_host,
-        ...     port=impala_port,
-        ...     hdfs_client=hdfs,
-        ... )
-        >>> client  # doctest: +ELLIPSIS
-        <ibis.backends.impala.Backend object at 0x...>
+    def status(self):
         """
-        self._temp_objects = set()
-        self._hdfs = hdfs_client
+        Retrieve Impala query status
+        """
+        self._wait_execute()
+        return self._cursor.status()
 
-        params = {}
-        if ca_cert is not None:
-            params["ca_cert"] = str(ca_cert)
-        self.con = ImpalaConnection(
-            pool_size=pool_size,
-            host=host,
-            port=port,
-            database=database,
-            timeout=timeout,
-            use_ssl=use_ssl,
-            user=user,
-            password=password,
-            auth_mechanism=auth_mechanism,
-            kerberos_service_name=kerberos_service_name,
-            **params,
-        )
+    def wait(self, progress_bar=True):
+        raise NotImplementedError
 
-        self._ensure_temp_db_exists()
+    def get_result(self):
+        """
+        Presuming the operation is completed, return the cursor result as would
+        be returned by the synchronous query API
+        """
+        self._wait_execute()
+        result = self._fetch(self._cursor)
+        return self._wrap_result(result)
 
-    @cached_property
-    def version(self):
-        with self._safe_raw_sql('select version()') as cursor:
-            (result,) = cursor.fetchone()
-        return result
 
-    @util.deprecated(instead='use equivalent methods in the backend')
-    def database(self, name: str | None = None):
-        """Return a `Database` object for the `name` database.
+_HS2_TTypeId_to_dtype = {
+    'BOOLEAN': 'bool',
+    'TINYINT': 'int8',
+    'SMALLINT': 'int16',
+    'INT': 'int32',
+    'BIGINT': 'int64',
+    'TIMESTAMP': 'datetime64[ns]',
+    'FLOAT': 'float32',
+    'DOUBLE': 'float64',
+    'STRING': 'object',
+    'DECIMAL': 'object',
+    'BINARY': 'object',
+    'VARCHAR': 'object',
+    'CHAR': 'object'
+}
 
-        Parameters
-        ----------
-        name
-            Name of the database to return the object for.
 
-        Returns
-        -------
-        Database
-            A database object for the specified database.
-        """
-        return ImpalaDatabase(name=name or self.current_database, client=self)
+class ImpalaClient(SQLClient):
 
-    def list_databases(self, like=None):
-        with self._safe_raw_sql('SHOW DATABASES') as cur:
-            databases = self._get_list(cur)
-        return self._filter_with_like(databases, like)
+    """
+    An Ibis client interface that uses Impala
+    """
+
+    database_class = ImpalaDatabase
+    sync_query = ImpalaQuery
+    async_query = ImpalaAsyncQuery
+
+    def __init__(self, con, hdfs_client=None, **params):
+        self.con = con
+
+        if isinstance(hdfs_client, hdfs.Client):
+            hdfs_client = WebHDFS(hdfs_client)
+        elif hdfs_client is not None and not isinstance(hdfs_client, HDFS):
+            raise TypeError(hdfs_client)
 
-    def list_tables(self, like=None, database=None):
-        statement = 'SHOW TABLES'
-        if database is not None:
-            statement += f' IN {database}'
-        if like:
-            if match := fully_qualified_re.match(like):
-                database, quoted, unquoted = match.groups()
-                like = quoted or unquoted
-                return self.list_tables(like=like, database=database)
-            statement += f" LIKE '{like}'"
+        self._hdfs = hdfs_client
 
-        with self._safe_raw_sql(statement) as cursor:
-            tables = [row[0] for row in cursor.fetchall()]
-        return self._filter_with_like(tables)
+        self._temp_objects = weakref.WeakValueDictionary()
 
-    def fetch_from_cursor(self, cursor, schema):
-        batches = cursor.fetchall(columnar=True)
-        names = [name for name, *_ in cursor.description]
-        df = _column_batches_to_dataframe(names, batches)
-        if schema:
-            return PandasData.convert_table(df, schema)
-        return df
-
-    @contextlib.contextmanager
-    def _safe_raw_sql(self, *args, **kwargs):
-        with contextlib.closing(self.raw_sql(*args, **kwargs)) as cursor:
-            yield cursor
-        with contextlib.suppress(AttributeError):
-            cursor.release()
+        self._ensure_temp_db_exists()
 
-    def _safe_exec_sql(self, *args, **kwargs):
-        with self._safe_raw_sql(*args, **kwargs):
-            pass
+    def _build_ast(self, expr):
+        return build_ast(expr)
 
-    @property
-    def hdfs(self):
+    def _get_hdfs(self):
         if self._hdfs is None:
-            raise com.IbisError(
-                'No HDFS connection; must pass connection '
-                'using the hdfs_client argument to '
-                'ibis.impala.connect'
-            )
+            raise com.IbisError('No HDFS connection; must pass connection '
+                                'using the hdfs_client argument to '
+                                'ibis.impala.connect')
         return self._hdfs
 
+    def _set_hdfs(self, hdfs):
+        if not isinstance(hdfs, HDFS):
+            raise TypeError('must be HDFS instance')
+        self._hdfs = hdfs
+
+    hdfs = property(fget=_get_hdfs, fset=_set_hdfs)
+
     @property
-    def kudu(self):
-        raise NotImplementedError(
-            "kudu support using kudu-python is no longer supported; "
-            "use impala facilities to manage kudu tables; "
-            "see https://kudu.apache.org/docs/kudu_impala_integration.html"
-        )
+    def _table_expr_klass(self):
+        return ImpalaTable
 
     def close(self):
-        """Close the connection and drop temporary objects."""
-        while self._temp_objects:
-            finalizer = self._temp_objects.pop()
-            with contextlib.suppress(HS2Error):
-                finalizer()
+        """
+        Close Impala connection and drop any temporary objects
+        """
+        for k, v in self._temp_objects.items():
+            try:
+                v.drop()
+            except HS2Error:
+                pass
 
         self.con.close()
 
     def disable_codegen(self, disabled=True):
-        """Turn off or on LLVM codegen in Impala query execution.
+        """
+        Turn off or on LLVM codegen in Impala query execution
 
         Parameters
         ----------
-        disabled
-            To disable codegen, pass with no argument or True. To enable
-            codegen, pass False.
+        disabled : boolean, default True
+          To disable codegen, pass with no argument or True. To enable codegen,
+          pass False
         """
         self.con.disable_codegen(disabled)
 
+    def log(self, msg):
+        log(msg)
+
     def _fully_qualified_name(self, name, database):
-        if is_fully_qualified(name):
+        if ddl._is_fully_qualified(name):
             return name
 
         database = database or self.current_database
-        return sg.table(name, db=database, quoted=True).sql(
-            dialect=getattr(self, "_sqlglot_dialect", self.name)
-        )
+        return '{0}.`{1}`'.format(database, name)
+
+    def list_tables(self, like=None, database=None):
+        """
+        List tables in the current (or indicated) database. Like the SHOW
+        TABLES command in the impala-shell.
+
+        Parameters
+        ----------
+        like : string, default None
+          e.g. 'foo*' to match all tables starting with 'foo'
+        database : string, default None
+          If not passed, uses the current/default database
+
+        Returns
+        -------
+        tables : list of strings
+        """
+        statement = 'SHOW TABLES'
+        if database:
+            statement += ' IN {0}'.format(database)
+        if like:
+            m = ddl.fully_qualified_re.match(like)
+            if m:
+                database, quoted, unquoted = m.groups()
+                like = quoted or unquoted
+                return self.list_tables(like=like, database=database)
+            statement += " LIKE '{0}'".format(like)
+
+        with self._execute(statement, results=True) as cur:
+            result = self._get_list(cur)
+
+        return result
 
     def _get_list(self, cur):
         tuples = cur.fetchall()
-        return list(map(operator.itemgetter(0), tuples))
+        if len(tuples) > 0:
+            return list(lzip(*tuples)[0])
+        else:
+            return []
 
-    @property
-    def current_database(self) -> str:
-        # XXX The parent `Client` has a generic method that calls this same
-        # method in the backend. But for whatever reason calling this code from
-        # that method doesn't seem to work. Maybe `con` is a copy?
-        return self.con.database
+    def set_database(self, name):
+        """
+        Set the default database scope for client
+        """
+        self.con.set_database(name)
+
+    def exists_database(self, name):
+        """
+        Checks if a given database exists
+
+        Parameters
+        ----------
+        name : string
+          Database name
+
+        Returns
+        -------
+        if_exists : boolean
+        """
+        return len(self.list_databases(like=name)) > 0
 
     def create_database(self, name, path=None, force=False):
-        """Create a new Impala database.
+        """
+        Create a new Impala database
 
         Parameters
         ----------
-        name
-            Database name
-        path
-            HDFS path where to store the database data; otherwise uses Impala
-            default
-        force
-            Forcibly create the database
+        name : string
+          Database name
+        path : string, default None
+          HDFS path where to store the database data; otherwise uses Impala
+          default
         """
         if path:
             # explicit mkdir ensures the user own the dir rather than impala,
             # which is easier for manual cleanup, if necessary
             self.hdfs.mkdir(path)
-        statement = CreateDatabase(name, path=path, can_exist=force)
-        self._safe_exec_sql(statement)
+        statement = ddl.CreateDatabase(name, path=path, can_exist=force)
+        return self._execute(statement)
 
     def drop_database(self, name, force=False):
-        """Drop an Impala database.
+        """
+        Drop an Impala database
 
         Parameters
         ----------
-        name
-            Database name
-        force
-            If False and there are any tables in this database, raises an
-            IntegrityError
+        name : string
+          Database name
+        force : boolean, default False
+          If False and there are any tables in this database, raises an
+          IntegrityError
         """
-        if not force or name in self.list_databases():
+        if not force or self.exists_database(name):
             tables = self.list_tables(database=name)
             udfs = self.list_udfs(database=name)
             udas = self.list_udas(database=name)
         else:
             tables = []
             udfs = []
             udas = []
         if force:
             for table in tables:
-                util.log(f'Dropping {name}.{table}')
+                self.log('Dropping {0}'.format('{0}.{1}'.format(name, table)))
                 self.drop_table_or_view(table, database=name)
             for func in udfs:
-                util.log(f'Dropping function {func.name}({func.inputs})')
-                self.drop_udf(
-                    func.name,
-                    input_types=func.inputs,
-                    database=name,
-                    force=True,
-                )
+                self.log('Dropping function {0}({1})'.format(func.name,
+                                                             func.inputs))
+                self.drop_udf(func.name, input_types=func.inputs,
+                              database=name, force=True)
             for func in udas:
-                util.log(f'Dropping aggregate function {func.name}({func.inputs})')
-                self.drop_uda(
-                    func.name,
-                    input_types=func.inputs,
-                    database=name,
-                    force=True,
-                )
-        elif tables or udfs or udas:
-            raise com.IntegrityError(
-                f"Database {name} must be empty before "
-                "being dropped, or set force=True"
-            )
-        statement = DropDatabase(name, must_exist=not force)
-        self._safe_exec_sql(statement)
-
-    def get_schema(self, table_name: str, database: str | None = None) -> sch.Schema:
-        """Return a Schema object for the indicated table and database.
+                self.log('Dropping aggregate function {0}({1})'
+                         .format(func.name, func.inputs))
+                self.drop_uda(func.name, input_types=func.inputs,
+                              database=name, force=True)
+        else:
+            if len(tables) > 0 or len(udfs) > 0 or len(udas) > 0:
+                raise com.IntegrityError('Database {0} must be empty before '
+                                         'being dropped, or set '
+                                         'force=True'.format(name))
+        statement = ddl.DropDatabase(name, must_exist=not force)
+        return self._execute(statement)
+
+    def list_databases(self, like=None):
+        """
+        List databases in the Impala cluster. Like the SHOW DATABASES command
+        in the impala-shell.
+
+        Parameters
+        ----------
+        like : string, default None
+          e.g. 'foo*' to match all tables starting with 'foo'
+
+        Returns
+        -------
+        databases : list of strings
+        """
+        statement = 'SHOW DATABASES'
+        if like:
+            statement += " LIKE '{0}'".format(like)
+
+        with self._execute(statement, results=True) as cur:
+            results = self._get_list(cur)
+
+        return results
+
+    def get_schema(self, table_name, database=None):
+        """
+        Return a Schema object for the indicated table and database
 
         Parameters
         ----------
-        table_name
-            Table name
-        database
-            Database name
+        table_name : string
+          May be fully qualified
+        database : string, default None
 
         Returns
         -------
-        Schema
-            Ibis schema
+        schema : ibis Schema
         """
         qualified_name = self._fully_qualified_name(table_name, database)
-        query = f"DESCRIBE {qualified_name}"
+        query = 'DESCRIBE {0}'.format(qualified_name)
+        tuples = self.con.fetchall(query)
 
-        # only pull out the first two columns which are names and types
-        pairs = [row[:2] for row in self.con.fetchall(query)]
-        names, types = zip(*pairs)
-
-        ibis_types = [udf.parse_type(type.lower()) for type in types]
-        ibis_fields = dict(zip(names, ibis_types))
-        return sch.Schema(ibis_fields)
+        names, types, comments = zip(*tuples)
+
+        ibis_types = []
+        for t in types:
+            t = t.lower()
+            t = udf._impala_to_ibis_type.get(t, t)
+            ibis_types.append(t)
+
+        names = [x.lower() for x in names]
+
+        return dt.Schema(names, ibis_types)
 
     @property
     def client_options(self):
         return self.con.options
 
-    def get_options(self) -> dict[str, str]:
-        """Return current query options for the Impala session."""
-        return {key: value for key, value, *_ in self.con.fetchall("SET")}
+    def get_options(self):
+        """
+        Return current query options for the Impala session
+        """
+        query = 'SET'
+        tuples = self.con.fetchall(query)
+        return dict(tuples)
 
     def set_options(self, options):
         self.con.set_options(options)
 
     def reset_options(self):
         # Must nuke all cursors
         raise NotImplementedError
 
     def set_compression_codec(self, codec):
+        """
+        Parameters
+        """
         if codec is None:
             codec = 'none'
         else:
             codec = codec.lower()
 
         if codec not in ('none', 'gzip', 'snappy'):
-            raise ValueError(f'Unknown codec: {codec}')
+            raise ValueError('Unknown codec: {0}'.format(codec))
 
         self.set_options({'COMPRESSION_CODEC': codec})
 
-    def create_view(
-        self,
-        name: str,
-        obj: ir.Table,
-        *,
-        database: str | None = None,
-        overwrite: bool = False,
-    ) -> ir.Table:
-        ast = self.compiler.to_ast(obj)
-        select = ast.queries[0]
-        statement = CreateView(name, select, database=database, can_exist=overwrite)
-        self._safe_exec_sql(statement)
-        return self.table(name, database=database)
+    def exists_table(self, name, database=None):
+        """
+        Determine if the indicated table or view exists
 
-    def drop_view(self, name, database=None, force=False):
-        stmt = DropView(name, database=database, must_exist=not force)
-        self._safe_exec_sql(stmt)
+        Parameters
+        ----------
+        name : string
+        database : string, default None
 
-    @contextlib.contextmanager
-    def _setup_insert(self, obj):
-        import pandas as pd
+        Returns
+        -------
+        if_exists : boolean
+        """
+        return len(self.list_tables(like=name, database=database)) > 0
 
-        if isinstance(obj, pd.DataFrame):
-            with DataFrameWriter(self, obj) as writer:
-                yield writer.delimited_table(writer.write_temp_csv())
-        else:
-            yield obj
+    def create_view(self, name, expr, database=None):
+        """
+        Create an Impala view from a table expression
 
-    def table(self, name: str, database: str | None = None, **kwargs: Any) -> ir.Table:
-        expr = super().table(name, database=database, **kwargs)
-        return ImpalaTable(expr.op())
-
-    def create_table(
-        self,
-        name: str,
-        obj: ir.Table | None = None,
-        *,
-        schema=None,
-        database=None,
-        temp: bool | None = None,
-        overwrite: bool = False,
-        external: bool = False,
-        # HDFS options
-        format='parquet',
-        location=None,
-        partition=None,
-        like_parquet=None,
-    ) -> ir.Table:
-        """Create a new table in Impala using an Ibis table expression.
-
-        This is currently designed for tables whose data is stored in HDFS.
+        Parameters
+        ----------
+        name : string
+        expr : ibis TableExpr
+        database : string, default None
+        """
+        ast = self._build_ast(expr)
+        select = ast.queries[0]
+        statement = ddl.CreateView(name, select, database=database)
+        return self._execute(statement)
+
+    def drop_view(self, name, database=None, force=False):
+        """
+        Drop an Impala view
 
         Parameters
         ----------
-        name
-            Table name
-        obj
-            If passed, creates table from select statement results
-        schema
-            Mutually exclusive with obj, creates an empty table with a
-            particular schema
-        database
-            Database name
-        temp
-            Whether a table is temporary
-        overwrite
-            Do not create table if table with indicated name already exists
-        external
-            Create an external table; Impala will not delete the underlying
-            data when the table is dropped
-        format
-            File format
-        location
-            Specify the directory location where Impala reads and writes files
-            for the table
-        partition
-            Must pass a schema to use this. Cannot partition from an
-            expression.
-        like_parquet
-            Can specify instead of a schema
-        """
-        if obj is None and schema is None:
-            raise com.IbisError("The schema or obj parameter is required")
-
-        if temp is not None:
-            raise NotImplementedError(
-                "Impala backend does not yet support temporary tables"
-            )
+        name : string
+        database : string, default None
+        force : boolean, default False
+          Database may throw exception if table does not exist
+        """
+        statement = ddl.DropView(name, database=database,
+                                 must_exist=not force)
+        return self._execute(statement)
+
+    def create_table(self, table_name, obj=None, schema=None, database=None,
+                     format='parquet', force=False, external=False,
+                     location=None, partition=None, like_parquet=None,
+                     path=None):
+        """
+        Create a new table in Impala using an Ibis table expression
+
+        Parameters
+        ----------
+        table_name : string
+        obj : TableExpr or pandas.DataFrame, optional
+          If passed, creates table from select statement results
+        schema : ibis.Schema, optional
+          Mutually exclusive with expr, creates an empty table with a
+          particular schema
+        database : string, default None (optional)
+        format : {'parquet'}
+        force : boolean, default False
+          Do not create table if table with indicated name already exists
+        external : boolean, default False
+          Create an external table; Impala will not delete the underlying data
+          when the table is dropped
+        location : string, default None
+          Specify the directory location where Impala reads and writes files
+          for the table
+        partition : list of strings
+          Must pass a schema to use this. Cannot partition from an expression
+          (create-table-as-select)
+        like_parquet : string (HDFS path), optional
+          Can specify in lieu of a schema
+
+        Examples
+        --------
+        con.create_table('new_table_name', table_expr)
+        """
         if like_parquet is not None:
             raise NotImplementedError
 
+        # TODO: deprecation warning
+        if path is not None:
+            location = path
+
         if obj is not None:
-            with self._setup_insert(obj) as to_insert:
-                ast = self.compiler.to_ast(to_insert)
-                select = ast.queries[0]
-
-                if overwrite:
-                    self.drop_table(name, force=True)
-                self._safe_exec_sql(
-                    CTAS(
-                        name,
-                        select,
-                        database=database,
-                        format=format,
-                        external=external,
-                        partition=partition,
-                        path=location,
-                    )
-                )
-        else:  # schema is not None
-            if overwrite:
-                self.drop_table(name, force=True)
-            self._safe_exec_sql(
-                CreateTableWithSchema(
-                    name,
-                    schema,
-                    database=database,
-                    format=format,
-                    external=external,
-                    path=location,
-                    partition=partition,
-                )
-            )
-        return self.table(name, database=database)
-
-    def avro_file(
-        self,
-        hdfs_dir,
-        avro_schema,
-        name=None,
-        database=None,
-        external=True,
-        persist=False,
-    ):
-        """Create a table to read a collection of Avro data.
-
-        Parameters
-        ----------
-        hdfs_dir
-            Absolute HDFS path to directory containing avro files
-        avro_schema
-            The Avro schema for the data as a Python dict
-        name
-            Table name
-        database
-            Database name
-        external
-            Whether the table is external
-        persist
-            Persist the table
-
-        Returns
-        -------
-        ImpalaTable
-            Impala table expression
-        """
-        name, database = self._get_concrete_table_path(name, database, persist=persist)
-
-        stmt = ddl.CreateTableAvro(
-            name, hdfs_dir, avro_schema, database=database, external=external
-        )
-        self._safe_exec_sql(stmt)
+            if isinstance(obj, pd.DataFrame):
+                writer, to_insert = _write_temp_dataframe(self, obj)
+            else:
+                to_insert = obj
+            ast = self._build_ast(to_insert)
+            select = ast.queries[0]
+
+            if partition is not None:
+                # Fairly certain this is currently the case
+                raise ValueError('partition not supported with '
+                                 'create-table-as-select. Create an '
+                                 'empty partitioned table instead '
+                                 'and insert into those partitions.')
+
+            statement = ddl.CTAS(table_name, select,
+                                 database=database,
+                                 can_exist=force,
+                                 format=format,
+                                 external=external,
+                                 path=location)
+        elif schema is not None:
+            statement = ddl.CreateTableWithSchema(
+                table_name, schema, ddl.NoFormat(),
+                database=database,
+                format=format,
+                can_exist=force,
+                external=external,
+                path=location, partition=partition)
+        else:
+            raise com.IbisError('Must pass expr or schema')
+
+        return self._execute(statement)
+
+    def avro_file(self, hdfs_dir, avro_schema, name=None, database=None,
+                  external=True, persist=False):
+        """
+        Create a (possibly temporary) table to read a collection of Avro data.
+
+        Parameters
+        ----------
+        hdfs_dir : string
+          Absolute HDFS path to directory containing avro files
+        avro_schema : dict
+          The Avro schema for the data as a Python dict
+        name : string, default None
+        database : string, default None
+        external : boolean, default True
+        persist : boolean, default False
+
+        Returns
+        -------
+        avro_table : ImpalaTable
+        """
+        name, database = self._get_concrete_table_path(name, database,
+                                                       persist=persist)
+
+        stmt = ddl.CreateTableAvro(name, hdfs_dir, avro_schema,
+                                   database=database,
+                                   external=external)
+        self._execute(stmt)
         return self._wrap_new_table(name, database, persist)
 
-    def delimited_file(
-        self,
-        hdfs_dir,
-        schema,
-        name=None,
-        database=None,
-        delimiter=',',
-        na_rep=None,
-        escapechar=None,
-        lineterminator=None,
-        external=True,
-        persist=False,
-    ):
-        """Interpret delimited text files as an Ibis table expression.
-
-        See the `parquet_file` method for more details on what happens under
-        the hood.
-
-        Parameters
-        ----------
-        hdfs_dir
-            HDFS directory containing delimited text files
-        schema
-            Ibis schema
-        name
-            Name for temporary or persistent table; otherwise random names are
-            generated
-        database
-            Database to create the table in
-        delimiter
-            Character used to delimit columns
-        na_rep
-            Character used to represent NULL values
-        escapechar
-            Character used to escape special characters
-        lineterminator
-            Character used to delimit lines
-        external
-            Create table as EXTERNAL (data will not be deleted on drop). Not
-            that if persist=False and external=False, whatever data you
-            reference will be deleted
-        persist
-            If True, do not delete the table upon garbage collection of ibis
-            table object
-
-        Returns
-        -------
-        ImpalaTable
-            Impala table expression
-        """
-        name, database = self._get_concrete_table_path(name, database, persist=persist)
-
-        stmt = ddl.CreateTableDelimited(
-            name,
-            hdfs_dir,
-            schema,
-            database=database,
-            delimiter=delimiter,
-            external=external,
-            na_rep=na_rep,
-            lineterminator=lineterminator,
-            escapechar=escapechar,
-        )
-        self._safe_exec_sql(stmt)
+    def delimited_file(self, hdfs_dir, schema, name=None, database=None,
+                       delimiter=',',
+                       na_rep=None, escapechar=None, lineterminator=None,
+                       external=True, persist=False):
+        """
+        Interpret delimited text files (CSV / TSV / etc.) as an Ibis table. See
+        `parquet_file` for more exposition on what happens under the hood.
+
+        Parameters
+        ----------
+        hdfs_dir : string
+          HDFS directory name containing delimited text files
+        schema : ibis Schema
+        name : string, default None
+          Name for temporary or persistent table; otherwise random one
+          generated
+        database : string
+          Database to create the (possibly temporary) table in
+        delimiter : length-1 string, default ','
+          Pass None if there is no delimiter
+        escapechar : length-1 string
+          Character used to escape special characters
+        lineterminator : length-1 string
+          Character used to delimit lines
+        external : boolean, default True
+          Create table as EXTERNAL (data will not be deleted on drop). Not that
+          if persist=False and external=False, whatever data you reference will
+          be deleted
+        persist : boolean, default False
+          If True, do not delete the table upon garbage collection of ibis
+          table object
+
+        Returns
+        -------
+        delimited_table : ImpalaTable
+        """
+        name, database = self._get_concrete_table_path(name, database,
+                                                       persist=persist)
+
+        stmt = ddl.CreateTableDelimited(name, hdfs_dir, schema,
+                                        database=database,
+                                        delimiter=delimiter,
+                                        external=external,
+                                        na_rep=na_rep,
+                                        lineterminator=lineterminator,
+                                        escapechar=escapechar)
+        self._execute(stmt)
         return self._wrap_new_table(name, database, persist)
 
-    def parquet_file(
-        self,
-        hdfs_dir,
-        schema=None,
-        name=None,
-        database=None,
-        external=True,
-        like_file=None,
-        like_table=None,
-        persist=False,
-    ):
-        """Make indicated parquet file in HDFS available as an Ibis table.
+    def parquet_file(self, hdfs_dir, schema=None, name=None, database=None,
+                     external=True, like_file=None, like_table=None,
+                     persist=False):
+        """
+        Make indicated parquet file in HDFS available as an Ibis table.
 
         The table created can be optionally named and persisted, otherwise a
         unique name will be generated. Temporarily, for any non-persistent
         external table created by Ibis we will attempt to drop it when the
         underlying object is garbage collected (or the Python interpreter shuts
         down normally).
 
         Parameters
         ----------
-        hdfs_dir
-            Path in HDFS
-        schema
-            If no schema provided, and neither of the like_* argument is
-            passed, one will be inferred from one of the parquet files in the
-            directory.
-        like_file
-            Absolute path to Parquet file in HDFS to use for schema
-            definitions. An alternative to having to supply an explicit schema
-        like_table
-            Fully scoped and escaped string to an Impala table whose schema we
-            will use for the newly created table.
-        name
-            Random unique name generated otherwise
-        database
-            Database to create the (possibly temporary) table in
-        external
-            If a table is external, the referenced data will not be deleted
-            when the table is dropped in Impala. Otherwise (external=False)
-            Impala takes ownership of the Parquet file.
-        persist
-            Do not drop the table during garbage collection
+        hdfs_dir : string
+          Path in HDFS
+        schema : ibis Schema
+          If no schema provided, and neither of the like_* argument is passed,
+          one will be inferred from one of the parquet files in the directory.
+        like_file : string
+          Absolute path to Parquet file in HDFS to use for schema
+          definitions. An alternative to having to supply an explicit schema
+        like_table : string
+          Fully scoped and escaped string to an Impala table whose schema we
+          will use for the newly created table.
+        name : string, optional
+          random unique name generated otherwise
+        database : string, optional
+          Database to create the (possibly temporary) table in
+        external : boolean, default True
+          If a table is external, the referenced data will not be deleted when
+          the table is dropped in Impala. Otherwise (external=False) Impala
+          takes ownership of the Parquet file.
+        persist : boolean, default False
+          Do not drop the table upon Ibis garbage collection / interpreter
+          shutdown
 
         Returns
         -------
-        ImpalaTable
-            Impala table expression
+        parquet_table : ImpalaTable
         """
-        name, database = self._get_concrete_table_path(name, database, persist=persist)
+        name, database = self._get_concrete_table_path(name, database,
+                                                       persist=persist)
 
         # If no schema provided, need to find some absolute path to a file in
         # the HDFS directory
         if like_file is None and like_table is None and schema is None:
-            try:
-                file_name = next(
-                    fn
-                    for fn in (
-                        os.path.basename(f["name"])
-                        for f in self.hdfs.ls(hdfs_dir, detail=True)
-                        if f["type"].lower() == "file"
-                    )
-                    if not fn.startswith(("_", "."))
-                    if not fn.endswith((".tmp", ".copying"))
-                )
-            except StopIteration:
-                raise com.IbisError("No files found in the passed directory")
-            else:
-                like_file = pjoin(hdfs_dir, file_name)
+            file_name = self.hdfs._find_any_file(hdfs_dir)
+            like_file = pjoin(hdfs_dir, file_name)
 
-        stmt = ddl.CreateTableParquet(
-            name,
-            hdfs_dir,
-            schema=schema,
-            database=database,
-            example_file=like_file,
-            example_table=like_table,
-            external=external,
-            can_exist=False,
-        )
-        self._safe_exec_sql(stmt)
+        stmt = ddl.CreateTableParquet(name, hdfs_dir,
+                                      schema=schema,
+                                      database=database,
+                                      example_file=like_file,
+                                      example_table=like_table,
+                                      external=external,
+                                      can_exist=False)
+        self._execute(stmt)
         return self._wrap_new_table(name, database, persist)
 
-    def _get_concrete_table_path(
-        self, name: str, database: str | None, persist: bool = False
-    ) -> tuple[str, str]:
+    def _get_concrete_table_path(self, name, database, persist=False):
         if not persist:
             if name is None:
-                name = f'__ibis_tmp_{util.guid()}'
+                name = util.guid()
 
             if database is None:
                 self._ensure_temp_db_exists()
                 database = options.impala.temp_db
             return name, database
         else:
             if name is None:
                 raise com.IbisError('Must pass table name if persist=True')
             return name, database
 
     def _ensure_temp_db_exists(self):
         # TODO: session memoize to avoid unnecessary `SHOW DATABASES` calls
         name, path = options.impala.temp_db, options.impala.temp_hdfs_path
-        if name not in self.list_databases():
-            if self._hdfs is not None:
-                self.create_database(name, path=path, force=True)
-
-    def _drop_table(self, name: str) -> None:
-        # database might have been dropped, so we suppress the
-        # corresponding Exception
-        with contextlib.suppress(ImpylaError):
-            self.drop_table(name)
+        if not self.exists_database(name):
+            self.create_database(name, path=path, force=True)
 
     def _wrap_new_table(self, name, database, persist):
         qualified_name = self._fully_qualified_name(name, database)
-        t = self.table(name, database=database)
-        if not persist:
-            self._temp_objects.add(
-                # weakref the op instead of the expression because the table is
-                # potentially collected after subsequent use when `_erase_expr`
-                # unwraps the Expr layer
-                weakref.finalize(t.op(), self._drop_table, qualified_name)
-            )
+
+        if persist:
+            t = self.table(qualified_name)
+        else:
+            schema = self._get_table_schema(qualified_name)
+            node = ImpalaTemporaryTable(qualified_name, schema, self)
+            t = self._table_expr_klass(node)
 
         # Compute number of rows in table for better default query planning
         cardinality = t.count().execute()
-        self._safe_exec_sql(
-            f"ALTER TABLE {qualified_name} SET tblproperties('numRows'='{cardinality:d}', "
-            "'STATS_GENERATED_VIA_STATS_TASK' = 'true')"
-        )
+        set_card = ("alter table {0} set tblproperties('numRows'='{1}', "
+                    "'STATS_GENERATED_VIA_STATS_TASK' = 'true')"
+                    .format(qualified_name, cardinality))
+        self._execute(set_card)
+
+        self._temp_objects[id(t)] = t
 
         return t
 
     def text_file(self, hdfs_path, column_name='value'):
-        """Interpret text data as a table with a single string column."""
+        """
+        Interpret text data as a table with a single string column.
 
-    def insert(
-        self,
-        table_name,
-        obj=None,
-        database=None,
-        overwrite=False,
-        partition=None,
-        values=None,
-        validate=True,
-    ):
-        """Insert data into an existing table.
-
-        See
-        [`ImpalaTable.insert`][ibis.backends.impala.client.ImpalaTable.insert]
-        for parameters.
+        Parameters
+        ----------
+
+        Returns
+        -------
+        text_table : TableExpr
+        """
+        pass
+
+    def insert(self, table_name, obj=None, database=None, overwrite=False,
+               partition=None, values=None, validate=True):
+        """
+        Insert into existing table.
+
+        See ImpalaTable.insert for other parameters.
+
+        Parameters
+        ----------
+        table_name : string
+        database : string, default None
 
         Examples
         --------
-        >>> table = 'my_table'
-        >>> con.insert(table, table_expr)  # doctest: +SKIP
+        con.insert('my_table', table_expr)
+
+        # Completely overwrite contents
+        con.insert('my_table', table_expr, overwrite=True)
+        """
+        table = self.table(table_name, database=database)
+        return table.insert(obj=obj, overwrite=overwrite, partition=partition,
+                            values=values, validate=validate)
 
-        Completely overwrite contents
-        >>> con.insert(table, table_expr, overwrite=True)  # doctest: +SKIP
+    def load_data(self, table_name, path, database=None, overwrite=False,
+                  partition=None):
+        """
+        Wraps the LOAD DATA DDL statement. Loads data into an Impala table by
+        physically moving data files.
+
+        Parameters
+        ----------
+        table_name : string
+        database : string, default None (optional)
         """
         table = self.table(table_name, database=database)
-        return table.insert(
-            obj=obj,
-            overwrite=overwrite,
-            partition=partition,
-            values=values,
-            validate=validate,
-        )
-
-    def drop_table(
-        self, name: str, *, database: str | None = None, force: bool = False
-    ) -> None:
-        """Drop an Impala table.
+        return table.load_data(path, overwrite=overwrite,
+                               partition=partition)
+
+    def drop_table(self, table_name, database=None, force=False):
+        """
+        Drop an Impala table
 
         Parameters
         ----------
-        name
-            Table name
-        database
-            Database name
-        force
-            Database may throw exception if table does not exist
+        table_name : string
+        database : string, default None (optional)
+        force : boolean, default False
+          Database may throw exception if table does not exist
 
         Examples
         --------
-        >>> table = 'my_table'
-        >>> db = 'operations'
-        >>> con.drop_table(table, database=db, force=True)  # doctest: +SKIP
+        con.drop_table('my_table', database='operations', force=True)
         """
-        statement = DropTable(name, database=database, must_exist=not force)
-        self._safe_exec_sql(statement)
+        statement = ddl.DropTable(table_name, database=database,
+                                  must_exist=not force)
+        self._execute(statement)
 
-    def truncate_table(self, name: str, database: str | None = None) -> None:
-        """Delete all rows from an existing table.
+    def truncate_table(self, table_name, database=None):
+        """
+        Delete all rows from, but do not drop, an existing table
 
         Parameters
         ----------
-        name
-            Table name
-        database
-            Database name
+        table_name : string
+        database : string, default None (optional)
         """
-        statement = TruncateTable(name, database=database)
-        self._safe_exec_sql(statement)
+        statement = ddl.TruncateTable(table_name, database=database)
+        self._execute(statement)
 
-    def drop_table_or_view(self, name, *, database=None, force=False):
-        """Drop view or table."""
+    def drop_table_or_view(self, name, database=None, force=False):
+        """
+        Attempt to drop a relation that may be a view or table
+        """
         try:
             self.drop_table(name, database=database)
-        except Exception as e:  # noqa: BLE001
+        except Exception as e:
             try:
                 self.drop_view(name, database=database)
-            except Exception:  # noqa: BLE001
+            except:
                 raise e
 
-    def cache_table(self, table_name, *, database=None, pool='default'):
-        """Caches a table in cluster memory in the given pool.
+    def cache_table(self, table_name, database=None, pool='default'):
+        """
+        Caches a table in cluster memory in the given pool.
 
         Parameters
         ----------
-        table_name
-            Table name
-        database
-            Database name
-        pool
-            The name of the pool in which to cache the table
+        table_name : string
+        database : string default None (optional)
+        pool : string, default 'default'
+           The name of the pool in which to cache the table
 
         Examples
         --------
-        >>> table = 'my_table'
-        >>> db = 'operations'
-        >>> pool = 'op_4GB_pool'
-        >>> con.cache_table('my_table', database=db, pool=pool)  # doctest: +SKIP
+        con.cache_table('my_table', database='operations', pool='op_4GB_pool')
         """
         statement = ddl.CacheTable(table_name, database=database, pool=pool)
-        self._safe_exec_sql(statement)
+        self._execute(statement)
+
+    def _get_table_schema(self, tname):
+        return self.get_schema(tname)
 
     def _get_schema_using_query(self, query):
-        with self._safe_raw_sql(f"SELECT * FROM ({query}) t0 LIMIT 0") as cur:
+        with self._execute(query, results=True) as cur:
             # resets the state of the cursor and closes operation
             cur.fetchall()
-            ibis_fields = self._adapt_types(cur.description)
+            names, ibis_types = self._adapt_types(cur.description)
+
+        # per #321; most Impala tables will be lower case already, but Avro
+        # data, depending on the version of Impala, might have field names in
+        # the metastore cased according to the explicit case in the declared
+        # avro schema. This is very annoying, so it's easier to just conform on
+        # all lowercase fields from Impala.
+        names = [x.lower() for x in names]
 
-        return sch.Schema(ibis_fields)
+        return dt.Schema(names, ibis_types)
 
     def create_function(self, func, name=None, database=None):
-        """Create a function within Impala.
+        """
+        Creates a function within Impala
 
         Parameters
         ----------
-        func
-            UDF or UDAF
-        name
-            Function name
-        database
-            Database name
+        func : ImpalaUDF or ImpalaUDA
+          Created with wrap_udf or wrap_uda
+        name : string (optional)
+        database : string (optional)
         """
         if name is None:
             name = func.name
         database = database or self.current_database
 
         if isinstance(func, udf.ImpalaUDF):
-            stmt = ddl.CreateUDF(func, name=name, database=database)
+            stmt = ddl.CreateFunction(func.lib_path, func.so_symbol,
+                                      func.input_type,
+                                      func.output,
+                                      name, database)
         elif isinstance(func, udf.ImpalaUDA):
-            stmt = ddl.CreateUDA(func, name=name, database=database)
+            stmt = ddl.CreateAggregateFunction(func.lib_path,
+                                               func.input_type,
+                                               func.output,
+                                               func.update_fn,
+                                               func.init_fn,
+                                               func.merge_fn,
+                                               func.serialize_fn,
+                                               func.finalize_fn,
+                                               name, database)
         else:
             raise TypeError(func)
-        self._safe_exec_sql(stmt)
+        self._execute(stmt)
 
-    def drop_udf(
-        self,
-        name,
-        input_types=None,
-        database=None,
-        force=False,
-        aggregate=False,
-    ):
-        """Drop a UDF.
-
-        If only name is given, this will search for the relevant UDF and drop
-        it. To delete an overloaded UDF, give only a name and force=True
+    def drop_udf(self, name, input_types=None, database=None, force=False,
+                 aggregate=False):
+        """
+        Drops a UDF
+        If only name is given, this will search
+        for the relevant UDF and drop it.
+        To delete an overloaded UDF, give only a name and force=True
 
         Parameters
         ----------
-        name
-            Function name
-        input_types
-            Input types
-        force
-            Must be set to `True` to drop overloaded UDFs
-        database
-            Database name
-        aggregate
-            Whether the function is an aggregate
+        name : string
+        input_types : list of strings (optional)
+        force : boolean, default False Must be set to true to
+                drop overloaded UDFs
+        database : string, default None
+        aggregate : boolean, default False
         """
         if not input_types:
             if not database:
                 database = self.current_database
             result = self.list_udfs(database=database, like=name)
             if len(result) > 1:
                 if force:
                     for func in result:
-                        self._drop_single_function(
-                            func.name,
-                            func.inputs,
-                            database=database,
-                            aggregate=aggregate,
-                        )
+                        self._drop_single_function(func.name, func.inputs,
+                                                   database=database,
+                                                   aggregate=aggregate)
                     return
                 else:
-                    raise com.DuplicateUDFError(name)
+                    raise Exception("More than one function " +
+                                    "with {0} found.".format(name) +
+                                    "Please specify force=True")
             elif len(result) == 1:
                 func = result.pop()
-                self._drop_single_function(
-                    func.name,
-                    func.inputs,
-                    database=database,
-                    aggregate=aggregate,
-                )
+                self._drop_single_function(func.name, func.inputs,
+                                           database=database,
+                                           aggregate=aggregate)
                 return
             else:
-                raise com.MissingUDFError(name)
-        self._drop_single_function(
-            name, input_types, database=database, aggregate=aggregate
-        )
+                raise Exception("No function found with name {0}"
+                                .format(name))
+        self._drop_single_function(name, input_types, database=database,
+                                   aggregate=aggregate)
 
     def drop_uda(self, name, input_types=None, database=None, force=False):
-        """Drop an aggregate function."""
-        return self.drop_udf(
-            name, input_types=input_types, database=database, force=force
-        )
-
-    def _drop_single_function(self, name, input_types, database=None, aggregate=False):
-        stmt = ddl.DropFunction(
-            name,
-            input_types,
-            must_exist=False,
-            aggregate=aggregate,
-            database=database,
-        )
-        self._safe_exec_sql(stmt)
+        """
+        Drop aggregate function. See drop_udf for more information on the
+        parameters.
+        """
+        return self.drop_udf(name, input_types=input_types, database=database,
+                             force=force)
+
+    def _drop_single_function(self, name, input_types, database=None,
+                              aggregate=False):
+        stmt = ddl.DropFunction(name, input_types, must_exist=False,
+                                aggregate=aggregate, database=database)
+        self._execute(stmt)
 
     def _drop_all_functions(self, database):
         udfs = self.list_udfs(database=database)
         for fnct in udfs:
-            stmt = ddl.DropFunction(
-                fnct.name,
-                fnct.inputs,
-                must_exist=False,
-                aggregate=False,
-                database=database,
-            )
-            self._safe_exec_sql(stmt)
+            stmt = ddl.DropFunction(fnct.name, fnct.inputs, must_exist=False,
+                                    aggregate=False, database=database)
+            self._execute(stmt)
         udafs = self.list_udas(database=database)
         for udaf in udafs:
-            stmt = ddl.DropFunction(
-                udaf.name,
-                udaf.inputs,
-                must_exist=False,
-                aggregate=True,
-                database=database,
-            )
-            self._safe_exec_sql(stmt)
+            stmt = ddl.DropFunction(udaf.name, udaf.inputs, must_exist=False,
+                                    aggregate=True, database=database)
+            self._execute(stmt)
 
     def list_udfs(self, database=None, like=None):
-        """Lists all UDFs associated with given database."""
+        """
+        Lists all UDFs associated with given database
+
+        Parameters
+        ----------
+        database : string
+        like : string for searching (optional)
+        """
         if not database:
             database = self.current_database
         statement = ddl.ListFunction(database, like=like, aggregate=False)
-        with self._safe_raw_sql(statement) as cur:
-            return self._get_udfs(cur, udf.ImpalaUDF)
+        with self._execute(statement, results=True) as cur:
+            result = self._get_udfs(cur, udf.ImpalaUDF)
+        return result
 
     def list_udas(self, database=None, like=None):
-        """Lists all UDAFs associated with a given database."""
+        """
+        Lists all UDAFs associated with a given database
+
+        Parameters
+        ----------
+        database : string
+        like : string for searching (optional)
+        """
         if not database:
             database = self.current_database
         statement = ddl.ListFunction(database, like=like, aggregate=True)
-        with self._safe_raw_sql(statement) as cur:
-            return self._get_udfs(cur, udf.ImpalaUDA)
+        with self._execute(statement, results=True) as cur:
+            result = self._get_udfs(cur, udf.ImpalaUDA)
+
+        return result
 
     def _get_udfs(self, cur, klass):
+        from ibis.expr.rules import varargs
+        from ibis.expr.datatypes import validate_type
+
         def _to_type(x):
             ibis_type = udf._impala_type_to_ibis(x.lower())
-            return dt.dtype(ibis_type)
+            return validate_type(ibis_type)
 
         tuples = cur.fetchall()
         if len(tuples) > 0:
             result = []
-            for tup in tuples:
-                out_type, sig = tup[:2]
+            for out_type, sig in tuples:
                 name, types = _split_signature(sig)
                 types = _type_parser(types).types
 
                 inputs = []
                 for arg in types:
                     argm = _arg_type.match(arg)
                     var, simple = argm.groups()
                     if simple:
                         t = _to_type(simple)
                         inputs.append(t)
                     else:
                         t = _to_type(var)
-                        inputs = rlz.listof(t)
+                        inputs = varargs(t)
                         # TODO
                         # inputs.append(varargs(t))
                         break
 
                 output = udf._impala_type_to_ibis(out_type.lower())
                 result.append(klass(inputs, output, name=name))
             return result
         else:
             return []
 
-    def exists_udf(self, name: str, database: str | None = None) -> bool:
-        """Checks if a given UDF exists within a specified database."""
-        return bool(self.list_udfs(database=database, like=name))
-
-    def exists_uda(self, name: str, database: str | None = None) -> bool:
-        """Checks if a given UDAF exists within a specified database."""
-        return bool(self.list_udas(database=database, like=name))
-
-    def compute_stats(
-        self,
-        name: str,
-        database: str | None = None,
-        incremental: bool = False,
-    ) -> None:
-        """Issue a `COMPUTE STATS` command for a given table.
+    def exists_udf(self, name, database=None):
+        """
+        Checks if a given UDF exists within a specified database
+
+        Parameters
+        ----------
+        name : string, UDF name
+        database : string, database name
+
+        Returns
+        -------
+        if_exists : boolean
+        """
+        return len(self.list_udfs(database=database, like=name)) > 0
+
+    def exists_uda(self, name, database=None):
+        """
+        Checks if a given UDAF exists within a specified database
 
         Parameters
         ----------
-        name
-            Can be fully qualified (with database name)
-        database
-            Database name
-        incremental
-            If True, issue COMPUTE INCREMENTAL STATS
+        name : string, UDAF name
+        database : string, database name
+
+        Returns
+        -------
+        if_exists : boolean
         """
+        return len(self.list_udas(database=database, like=name)) > 0
+
+    def compute_stats(self, name, database=None, incremental=False,
+                      async=False):
+        """
+        Issue COMPUTE STATS command for a given table
+
+        Parameters
+        ----------
+        name : string
+          Can be fully qualified (with database name)
+        database : string, optional
+        incremental : boolean, default False
+          If True, issue COMPUTE INCREMENTAL STATS
+        """
+        # TODO async + cancellation
+        if async:
+            raise NotImplementedError
+
         maybe_inc = 'INCREMENTAL ' if incremental else ''
-        cmd = f'COMPUTE {maybe_inc}STATS'
+        cmd = 'COMPUTE {0}STATS'.format(maybe_inc)
 
         stmt = self._table_command(cmd, name, database=database)
-        self._safe_exec_sql(stmt)
+        self._execute(stmt)
 
-    def invalidate_metadata(
-        self,
-        name: str | None = None,
-        database: str | None = None,
-    ) -> None:
-        """Issue an `INVALIDATE METADATA` command.
-
-        Optionally this applies to a specific table. See Impala documentation.
+    def invalidate_metadata(self, name=None, database=None):
+        """
+        Issue INVALIDATE METADATA command, optionally only applying to a
+        particular table. See Impala documentation.
 
         Parameters
         ----------
-        name
-            Table name. Can be fully qualified (with database)
-        database
-            Database name
+        name : string, optional
+          Table name. Can be fully qualified (with database)
+        database : string, optional
         """
         stmt = 'INVALIDATE METADATA'
         if name is not None:
             stmt = self._table_command(stmt, name, database=database)
-        self._safe_exec_sql(stmt)
-
-    def refresh(self, name: str, database: str | None = None) -> None:
-        """Reload HDFS block location metadata for a table.
-
-        This can be useful after ingesting data as part of an ETL pipeline, for
-        example.
+        self._execute(stmt)
 
-        Related to `INVALIDATE METADATA`. See Impala documentation for more.
+    def refresh(self, name, database=None):
+        """
+        Reload HDFS block location metadata for a table, for example after
+        ingesting data as part of an ETL pipeline. Related to INVALIDATE
+        METADATA. See Impala documentation for more.
 
         Parameters
         ----------
-        name
-            Table name. Can be fully qualified (with database)
-        database
-            Database name
+        name : string
+          Table name. Can be fully qualified (with database)
+        database : string, optional
         """
         # TODO(wesm): can this statement be cancelled?
         stmt = self._table_command('REFRESH', name, database=database)
-        self._safe_exec_sql(stmt)
-
-    def describe_formatted(
-        self,
-        name: str,
-        database: str | None = None,
-    ) -> pd.DataFrame:
-        """Retrieve the results of a `DESCRIBE FORMATTED` command.
+        self._execute(stmt)
 
-        See Impala documentation for more.
+    def describe_formatted(self, name, database=None):
+        """
+        Retrieve results of DESCRIBE FORMATTED command. See Impala
+        documentation for more.
 
         Parameters
         ----------
-        name
-            Table name. Can be fully qualified (with database)
-        database
-            Database name
+        name : string
+          Table name. Can be fully qualified (with database)
+        database : string, optional
         """
-        from ibis.backends.impala.metadata import parse_metadata
+        from ibis.impala.metadata import parse_metadata
 
-        stmt = self._table_command('DESCRIBE FORMATTED', name, database=database)
-        result = self._exec_statement(stmt)
+        stmt = self._table_command('DESCRIBE FORMATTED',
+                                   name, database=database)
+        query = ImpalaQuery(self, stmt)
+        result = query.execute()
 
         # Leave formatting to pandas
         for c in result.columns:
             result[c] = result[c].str.strip()
 
         return parse_metadata(result)
 
-    def show_files(
-        self,
-        name: str,
-        database: str | None = None,
-    ) -> pd.DataFrame:
-        """Retrieve results of a `SHOW FILES` command for a table.
-
-        See Impala documentation for more.
+    def show_files(self, name, database=None):
+        """
+        Retrieve results of SHOW FILES command for a table. See Impala
+        documentation for more.
 
         Parameters
         ----------
-        name
-            Table name. Can be fully qualified (with database)
-        database
-            Database name
+        name : string
+          Table name. Can be fully qualified (with database)
+        database : string, optional
         """
         stmt = self._table_command('SHOW FILES IN', name, database=database)
         return self._exec_statement(stmt)
 
     def list_partitions(self, name, database=None):
         stmt = self._table_command('SHOW PARTITIONS', name, database=database)
         return self._exec_statement(stmt)
 
     def table_stats(self, name, database=None):
-        """Return results of `SHOW TABLE STATS` for the table `name`."""
+        """
+        Return results of SHOW TABLE STATS for indicated table. See also
+        ImpalaTable.stats
+        """
         stmt = self._table_command('SHOW TABLE STATS', name, database=database)
         return self._exec_statement(stmt)
 
     def column_stats(self, name, database=None):
-        """Return results of `SHOW COLUMN STATS` for the table `name`."""
-        stmt = self._table_command('SHOW COLUMN STATS', name, database=database)
+        """
+        Return results of SHOW COLUMN STATS for indicated table. See also
+        ImpalaTable.column_stats
+        """
+        stmt = self._table_command('SHOW COLUMN STATS', name,
+                                   database=database)
         return self._exec_statement(stmt)
 
-    def _exec_statement(self, stmt):
-        with self._safe_raw_sql(stmt) as cur:
-            return self.fetch_from_cursor(cur, schema=None)
+    def _exec_statement(self, stmt, adapter=None):
+        query = ImpalaQuery(self, stmt)
+        result = query.execute()
+        if adapter is not None:
+            result = adapter(result)
+        return result
 
     def _table_command(self, cmd, name, database=None):
         qualified_name = self._fully_qualified_name(name, database)
-        return f'{cmd} {qualified_name}'
+        return '{0} {1}'.format(cmd, qualified_name)
 
     def _adapt_types(self, descr):
         names = []
         adapted_types = []
         for col in descr:
             names.append(col[0])
             impala_typename = col[1]
             typename = udf._impala_to_ibis_type[impala_typename.lower()]
 
             if typename == 'decimal':
                 precision, scale = col[4:6]
                 adapted_types.append(dt.Decimal(precision, scale))
             else:
                 adapted_types.append(typename)
-        return dict(zip(names, adapted_types))
+        return names, adapted_types
 
-    def write_dataframe(
-        self,
-        df: pd.DataFrame,
-        path: str,
-        format: Literal['csv'] = 'csv',
-    ) -> Any:
-        """Write a pandas DataFrame to indicated file path.
+    def write_dataframe(self, df, path, format='csv', async=False):
+        """
+        Write a pandas DataFrame to indicated file path (default: HDFS) in the
+        indicated format
 
         Parameters
         ----------
-        df
-            Pandas DataFrame
-        path
-            Absolute file path
-        format
-            File format
+        df : DataFrame
+        path : string
+          Absolute output path
+        format : {'csv'}, default 'csv'
+        async : boolean, default False
+          Not yet supported
+
+        Returns
+        -------
+        None (for now)
         """
+        from ibis.impala.pandas_interop import DataFrameWriter
+
+        if async:
+            raise NotImplementedError
+
         writer = DataFrameWriter(self, df)
         return writer.write_csv(path)
+
+
+# ----------------------------------------------------------------------
+# ORM-ish usability layer
+
+
+class ScalarFunction(DatabaseEntity):
+
+    def drop(self):
+        pass
+
+
+class AggregateFunction(DatabaseEntity):
+
+    def drop(self):
+        pass
+
+
+class ImpalaTable(ir.TableExpr, DatabaseEntity):
+
+    """
+    References a physical table in the Impala-Hive metastore
+    """
+
+    @property
+    def _qualified_name(self):
+        return self.op().args[0]
+
+    @property
+    def _unqualified_name(self):
+        return self._match_name()[1]
+
+    @property
+    def _client(self):
+        return self.op().args[2]
+
+    def _match_name(self):
+        m = ddl.fully_qualified_re.match(self._qualified_name)
+        if not m:
+            raise com.IbisError('Cannot determine database name from {0}'
+                                .format(self._qualified_name))
+        db, quoted, unquoted = m.groups()
+        return db, quoted or unquoted
+
+    @property
+    def _database(self):
+        return self._match_name()[0]
+
+    def compute_stats(self, incremental=False, async=False):
+        """
+        Invoke Impala COMPUTE STATS command to compute column, table, and
+        partition statistics.
+
+        See also ImpalaClient.compute_stats
+        """
+        return self._client.compute_stats(self._qualified_name,
+                                          incremental=incremental,
+                                          async=async)
+
+    def invalidate_metadata(self):
+        self._client.invalidate_metadata(self._qualified_name)
+
+    def refresh(self):
+        self._client.refresh(self._qualified_name)
+
+    def metadata(self):
+        """
+        Return parsed results of DESCRIBE FORMATTED statement
+
+        Returns
+        -------
+        meta : TableMetadata
+        """
+        return self._client.describe_formatted(self._qualified_name)
+
+    describe_formatted = metadata
+
+    def files(self):
+        """
+        Return results of SHOW FILES statement
+        """
+        return self._client.show_files(self._qualified_name)
+
+    def drop(self):
+        """
+        Drop the table from the database
+        """
+        self._client.drop_table_or_view(self._qualified_name)
+
+    def insert(self, obj=None, overwrite=False, partition=None,
+               values=None, validate=True):
+        """
+        Insert into Impala table. Wraps ImpalaClient.insert
+
+        Parameters
+        ----------
+        obj : TableExpr or pandas DataFrame
+        overwrite : boolean, default False
+          If True, will replace existing contents of table
+        partition : list or dict, optional
+          For partitioned tables, indicate the partition that's being inserted
+          into, either with an ordered list of partition keys or a dict of
+          partition field name to value. For example for the partition
+          (year=2007, month=7), this can be either (2007, 7) or {'year': 2007,
+          'month': 7}.
+        validate : boolean, default True
+          If True, do more rigorous validation that schema of table being
+          inserted is compatible with the existing table
+
+        Examples
+        --------
+        t.insert(table_expr)
+
+        # Completely overwrite contents
+        t.insert(table_expr, overwrite=True)
+        """
+        if isinstance(obj, pd.DataFrame):
+            writer, expr = _write_temp_dataframe(self._client, obj)
+        else:
+            expr = obj
+
+        if values is not None:
+            raise NotImplementedError
+
+        if validate:
+            existing_schema = self.schema()
+            insert_schema = expr.schema()
+            if not insert_schema.equals(existing_schema):
+                _validate_compatible(insert_schema, existing_schema)
+
+        if partition is not None:
+            partition_schema = self.partition_schema()
+            expr = expr.drop(partition_schema.names)
+        else:
+            partition_schema = None
+
+        ast = build_ast(expr)
+        select = ast.queries[0]
+        statement = ddl.InsertSelect(self._qualified_name,
+                                     select,
+                                     partition=partition,
+                                     partition_schema=partition_schema,
+                                     overwrite=overwrite)
+        return self._execute(statement)
+
+    def load_data(self, path, overwrite=False, partition=None):
+        """
+        Wraps the LOAD DATA DDL statement. Loads data into an Impala table by
+        physically moving data files.
+
+        Parameters
+        ----------
+        path : string
+        overwrite : boolean, default False
+          Overwrite the existing data in the entire table or indicated
+          partition
+        partition : dict, optional
+          If specified, the partition must already exist
+
+        Returns
+        -------
+        query : ImpalaQuery
+        """
+        if partition is not None:
+            partition_schema = self.partition_schema()
+        else:
+            partition_schema = None
+
+        stmt = ddl.LoadData(self._qualified_name, path,
+                            partition=partition,
+                            partition_schema=partition_schema)
+
+        return self._execute(stmt)
+
+    @property
+    def name(self):
+        return self.op().name
+
+    def rename(self, new_name, database=None):
+        """
+        Rename table inside Impala. References to the old table are no longer
+        valid.
+
+        Parameters
+        ----------
+        new_name : string
+        database : string
+
+        Returns
+        -------
+        renamed : ImpalaTable
+        """
+        m = ddl.fully_qualified_re.match(new_name)
+        if not m and database is None:
+            database = self._database
+        statement = ddl.RenameTable(self._qualified_name, new_name,
+                                    new_database=database)
+        self._client._execute(statement)
+
+        op = self.op().change_name(statement.new_qualified_name)
+        return ImpalaTable(op)
+
+    def _execute(self, stmt):
+        return self._client._execute(stmt)
+
+    @property
+    def is_partitioned(self):
+        """
+        True if the table is partitioned
+        """
+        return self.metadata().is_partitioned
+
+    def partition_schema(self):
+        """
+        For partitioned tables, return the schema (names and types) for the
+        partition columns
+
+        Returns
+        -------
+        partition_schema : ibis Schema
+        """
+        schema = self.schema()
+        name_to_type = dict(zip(schema.names, schema.types))
+
+        result = self.partitions()
+
+        partition_fields = []
+        for x in result.columns:
+            if x not in name_to_type:
+                break
+            partition_fields.append((x, name_to_type[x]))
+
+        pnames, ptypes = zip(*partition_fields)
+        return dt.Schema(pnames, ptypes)
+
+    def add_partition(self, spec, location=None):
+        """
+        Add a new table partition, creating any new directories in HDFS if
+        necessary.
+
+        Partition parameters can be set in a single DDL statement, or you can
+        use alter_partition to set them after the fact.
+
+        Returns
+        -------
+        None (for now)
+        """
+        part_schema = self.partition_schema()
+        stmt = ddl.AddPartition(self._qualified_name, spec, part_schema,
+                                location=location)
+        return self._execute(stmt)
+
+    def alter(self, location=None, format=None, tbl_properties=None,
+              serde_properties=None):
+        """
+        Change setting and parameters of the table.
+
+        Parameters
+        ----------
+        location : string, optional
+          For partitioned tables, you may want the alter_partition function
+        format : string, optional
+        tbl_properties : dict, optional
+        serde_properties : dict, optional
+
+        Returns
+        -------
+        None (for now)
+        """
+        def _run_ddl(**kwds):
+            stmt = ddl.AlterTable(self._qualified_name, **kwds)
+            return self._execute(stmt)
+
+        return self._alter_table_helper(_run_ddl, location=location,
+                                        format=format,
+                                        tbl_properties=tbl_properties,
+                                        serde_properties=serde_properties)
+
+    def alter_partition(self, spec, location=None, format=None,
+                        tbl_properties=None,
+                        serde_properties=None):
+        """
+        Change setting and parameters of an existing partition
+
+        Parameters
+        ----------
+        spec : dict or list
+          The partition keys for the partition being modified
+        location : string, optional
+        format : string, optional
+        tbl_properties : dict, optional
+        serde_properties : dict, optional
+
+        Returns
+        -------
+        None (for now)
+        """
+        part_schema = self.partition_schema()
+
+        def _run_ddl(**kwds):
+            stmt = ddl.AlterPartition(self._qualified_name, spec,
+                                      part_schema, **kwds)
+            return self._execute(stmt)
+
+        return self._alter_table_helper(_run_ddl, location=location,
+                                        format=format,
+                                        tbl_properties=tbl_properties,
+                                        serde_properties=serde_properties)
+
+    def _alter_table_helper(self, f, **alterations):
+        results = []
+        for k, v in alterations.items():
+            if v is None:
+                continue
+            result = f(**{k: v})
+            results.append(result)
+        return results
+
+    def drop_partition(self, spec):
+        """
+        Drop an existing table partition
+        """
+        part_schema = self.partition_schema()
+        stmt = ddl.DropPartition(self._qualified_name, spec, part_schema)
+        return self._execute(stmt)
+
+    def partitions(self):
+        """
+        Return a pandas.DataFrame giving information about this table's
+        partitions. Raises an exception if the table is not partitioned.
+
+        Returns
+        -------
+        partitions : pandas.DataFrame
+        """
+        return self._client.list_partitions(self._qualified_name)
+
+    def stats(self):
+        """
+        Return results of SHOW TABLE STATS as a DataFrame. If not partitioned,
+        contains only one row
+
+        Returns
+        -------
+        stats : pandas.DataFrame
+        """
+        return self._client.table_stats(self._qualified_name)
+
+    def column_stats(self):
+        """
+        Return results of SHOW COLUMN STATS as a pandas DataFrame
+
+        Returns
+        -------
+        column_stats : pandas.DataFrame
+        """
+        return self._client.column_stats(self._qualified_name)
+
+
+class ImpalaTemporaryTable(ops.DatabaseTable):
+
+    def __del__(self):
+        try:
+            self.drop()
+        except com.IbisError:
+            pass
+
+    def drop(self):
+        try:
+            self.source.drop_table(self.name)
+        except ImpylaError:
+            # database might have been dropped
+            pass
+
+
+def _write_temp_dataframe(client, df):
+    from ibis.impala.pandas_interop import DataFrameWriter
+    writer = DataFrameWriter(client, df)
+    path = writer.write_temp_csv()
+    return writer, writer.delimited_table(path)
+
+
+def _validate_compatible(from_schema, to_schema):
+    if set(from_schema.names) != set(to_schema.names):
+        raise com.IbisInputError('Schemas have different names')
+
+    for name in from_schema:
+        lt = from_schema[name]
+        rt = to_schema[name]
+        if not rt.can_implicit_cast(lt):
+            raise com.IbisInputError('Cannot safely cast {0!r} to {1!r}'
+                                     .format(lt, rt))
+
+
+def _split_signature(x):
+    name, rest = x.split('(', 1)
+    return name, rest[:-1]
+
+_arg_type = re.compile('(.*)\.\.\.|([^\.]*)')
+
+
+class _type_parser(object):
+
+    NORMAL, IN_PAREN = 0, 1
+
+    def __init__(self, value):
+        self.value = value
+        self.state = self.NORMAL
+        self.buf = six.StringIO()
+        self.types = []
+        for c in value:
+            self._step(c)
+        self._push()
+
+    def _push(self):
+        val = self.buf.getvalue().strip()
+        if val:
+            self.types.append(val)
+        self.buf = six.StringIO()
+
+    def _step(self, c):
+        if self.state == self.NORMAL:
+            if c == '(':
+                self.state = self.IN_PAREN
+            elif c == ',':
+                self._push()
+                return
+        elif self.state == self.IN_PAREN:
+            if c == ')':
+                self.state = self.NORMAL
+        self.buf.write(c)
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/impala/compat.py` & `ibis-framework-v0.6.0/ibis/impala/compat.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,24 +1,17 @@
-from __future__ import annotations
-
 # Copyright 2015 Cloudera Inc.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 # http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import impala.dbapi as impyla
-from impala.error import Error as ImpylaError
-from impala.error import HiveServer2Error as HS2Error
 
-__all__ = (
-    "impyla",
-    "ImpylaError",
-    "HS2Error",
-)
+from impala.error import Error as ImpylaError  # noqa
+from impala.error import HiveServer2Error as HS2Error  # noqa
+import impala.dbapi as impyla  # noqa
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/impala/ddl.py` & `ibis-framework-v0.6.0/ibis/expr/format.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,334 +1,254 @@
-from __future__ import annotations
-
 # Copyright 2014 Cloudera Inc.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 # http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import json
 
-from ibis.backends.base.sql.ddl import (
-    AlterTable,
-    BaseDDL,
-    CreateTable,
-    CreateTableWithSchema,
-    DropFunction,
-    format_partition,
-    format_schema,
-    format_tblproperties,
-)
-from ibis.backends.base.sql.registry import type_to_sql_string
-
-
-class CreateTableParquet(CreateTable):
-    def __init__(
-        self,
-        table_name,
-        path,
-        example_file=None,
-        example_table=None,
-        schema=None,
-        external=True,
-        **kwargs,
-    ):
-        super().__init__(
-            table_name,
-            external=external,
-            format='parquet',
-            path=path,
-            **kwargs,
-        )
-        self.example_file = example_file
-        self.example_table = example_table
-        self.schema = schema
-
-    @property
-    def _pieces(self):
-        if self.example_file is not None:
-            yield f"LIKE PARQUET '{self.example_file}'"
-        elif self.example_table is not None:
-            yield f"LIKE {self.example_table}"
-        elif self.schema is not None:
-            yield format_schema(self.schema)
-        else:
-            raise NotImplementedError
+import ibis.util as util
 
-        yield self._storage()
-        yield self._location()
+import ibis.expr.types as ir
+import ibis.expr.operations as ops
 
 
-class DelimitedFormat:
-    def __init__(
-        self,
-        path,
-        delimiter=None,
-        escapechar=None,
-        na_rep=None,
-        lineterminator=None,
-    ):
-        self.path = path
-        self.delimiter = delimiter
-        self.escapechar = escapechar
-        self.lineterminator = lineterminator
-        self.na_rep = na_rep
-
-    def to_ddl(self):
-        yield 'ROW FORMAT DELIMITED'
-
-        if self.delimiter is not None:
-            yield f"FIELDS TERMINATED BY '{self.delimiter}'"
-
-        if self.escapechar is not None:
-            yield f"ESCAPED BY '{self.escapechar}'"
-
-        if self.lineterminator is not None:
-            yield f"LINES TERMINATED BY '{self.lineterminator}'"
-
-        yield 'STORED AS TEXTFILE'
-        yield f"LOCATION '{self.path}'"
-
-        if self.na_rep is not None:
-            props = {'serialization.null.format': self.na_rep}
-            yield format_tblproperties(props)
-
-
-class AvroFormat:
-    def __init__(self, path, avro_schema):
-        self.path = path
-        self.avro_schema = avro_schema
-
-    def to_ddl(self):
-        yield 'STORED AS AVRO'
-        yield f"LOCATION '{self.path}'"
-
-        schema = json.dumps(self.avro_schema, indent=2, sort_keys=True)
-        schema = '\n'.join(x.rstrip() for x in schema.splitlines())
-
-        props = {'avro.schema.literal': schema}
-        yield format_tblproperties(props)
-
-
-class ParquetFormat:
-    def __init__(self, path):
-        self.path = path
-
-    def to_ddl(self):
-        yield 'STORED AS PARQUET'
-        yield f"LOCATION '{self.path}'"
-
-
-class CreateTableDelimited(CreateTableWithSchema):
-    def __init__(
-        self,
-        table_name,
-        path,
-        schema,
-        delimiter=None,
-        escapechar=None,
-        lineterminator=None,
-        na_rep=None,
-        external=True,
-        **kwargs,
-    ):
-        table_format = DelimitedFormat(
-            path,
-            delimiter=delimiter,
-            escapechar=escapechar,
-            lineterminator=lineterminator,
-            na_rep=na_rep,
-        )
-        super().__init__(table_name, schema, table_format, external=external, **kwargs)
-
-
-class CreateTableAvro(CreateTable):
-    def __init__(self, table_name, path, avro_schema, external=True, **kwargs):
-        super().__init__(table_name, external=external, **kwargs)
-        self.table_format = AvroFormat(path, avro_schema)
-
-    @property
-    def _pieces(self):
-        yield '\n'.join(self.table_format.to_ddl())
+class FormatMemo(object):
+    # A little sanity hack to simplify the below
 
+    def __init__(self):
+        from collections import defaultdict
+        self.formatted = {}
+        self.aliases = {}
+        self.ops = {}
+        self.counts = defaultdict(lambda: 0)
+        self._repr_memo = {}
 
-class LoadData(BaseDDL):
-    """Generate DDL for LOAD DATA command.
+    def __contains__(self, obj):
+        return self._key(obj) in self.formatted
 
-    Cannot be cancelled
-    """
+    def _key(self, obj):
+        memo_key = id(obj)
+        if memo_key in self._repr_memo:
+            return self._repr_memo[memo_key]
+        result = self._format(obj)
+        self._repr_memo[memo_key] = result
 
-    def __init__(
-        self,
-        table_name,
-        path,
-        database=None,
-        partition=None,
-        partition_schema=None,
-        overwrite=False,
-    ):
-        self.table_name = table_name
-        self.database = database
-        self.path = path
-
-        self.partition = partition
-        self.partition_schema = partition_schema
+        return result
 
-        self.overwrite = overwrite
+    def _format(self, obj):
+        return obj._repr(memo=self._repr_memo)
 
-    def compile(self):
-        overwrite = 'OVERWRITE ' if self.overwrite else ''
+    def observe(self, obj, formatter=lambda x: x._repr()):
+        key = self._key(obj)
+        if key not in self.formatted:
+            self.aliases[key] = 'ref_%d' % len(self.formatted)
+            self.formatted[key] = formatter(obj)
+            self.ops[key] = obj
+
+        self.counts[key] += 1
+
+    def count(self, obj):
+        return self.counts[self._key(obj)]
+
+    def get_alias(self, obj):
+        return self.aliases[self._key(obj)]
+
+    def get_formatted(self, obj):
+        return self.formatted[self._key(obj)]
 
-        if self.partition is not None:
-            partition = '\n' + format_partition(self.partition, self.partition_schema)
-        else:
-            partition = ''
 
-        scoped_name = self._get_scoped_name(self.table_name, self.database)
-        return "LOAD DATA INPATH '{}' {}INTO TABLE {}{}".format(
-            self.path, overwrite, scoped_name, partition
-        )
-
-
-class PartitionProperties(AlterTable):
-    def __init__(
-        self,
-        table,
-        partition,
-        partition_schema,
-        location=None,
-        format=None,
-        tbl_properties=None,
-        serde_properties=None,
-    ):
-        super().__init__(
-            table,
-            location=location,
-            format=format,
-            tbl_properties=tbl_properties,
-            serde_properties=serde_properties,
-        )
-        self.partition = partition
-        self.partition_schema = partition_schema
-
-    def _compile(self, cmd, property_prefix=''):
-        part = format_partition(self.partition, self.partition_schema)
-        if cmd:
-            part = f'{cmd} {part}'
-
-        props = self._format_properties(property_prefix)
-        action = f'{self.table} {part}{props}'
-        return self._wrap_command(action)
-
-
-class AddPartition(PartitionProperties):
-    def __init__(self, table, partition, partition_schema, location=None):
-        super().__init__(table, partition, partition_schema, location=location)
-
-    def compile(self):
-        return self._compile('ADD')
-
-
-class AlterPartition(PartitionProperties):
-    def compile(self):
-        return self._compile('', 'SET ')
-
-
-class DropPartition(PartitionProperties):
-    def __init__(self, table, partition, partition_schema):
-        super().__init__(table, partition, partition_schema)
-
-    def compile(self):
-        return self._compile('DROP')
-
-
-class CacheTable(BaseDDL):
-    def __init__(self, table_name, database=None, pool='default'):
-        self.table_name = table_name
-        self.database = database
-        self.pool = pool
-
-    def compile(self):
-        scoped_name = self._get_scoped_name(self.table_name, self.database)
-        return f"ALTER TABLE {scoped_name} SET CACHED IN '{self.pool}'"
-
-
-class CreateFunction(BaseDDL):
-    _object_type = 'FUNCTION'
-
-    def __init__(self, func, name=None, database=None):
-        self.func = func
-        self.name = name or func.name
-        self.database = database
-
-    def _impala_signature(self):
-        scoped_name = self._get_scoped_name(self.name, self.database)
-        input_sig = _impala_input_signature(self.func.inputs)
-        output_sig = type_to_sql_string(self.func.output)
-
-        return f'{scoped_name}({input_sig}) returns {output_sig}'
-
-
-class CreateUDF(CreateFunction):
-    def compile(self):
-        create_decl = 'CREATE FUNCTION'
-        impala_sig = self._impala_signature()
-        param_line = f"location '{self.func.lib_path}' symbol='{self.func.so_symbol}'"
-        return f'{create_decl} {impala_sig} {param_line}'
-
-
-class CreateUDA(CreateFunction):
-    def compile(self):
-        create_decl = 'CREATE AGGREGATE FUNCTION'
-        impala_sig = self._impala_signature()
-        tokens = [f"location '{self.func.lib_path}'"]
-
-        fn_names = (
-            'init_fn',
-            'update_fn',
-            'merge_fn',
-            'serialize_fn',
-            'finalize_fn',
-        )
-
-        for fn in fn_names:
-            value = getattr(self.func, fn)
-            if value is not None:
-                tokens.append(f"{fn}='{value}'")
-
-        joined_tokens = '\n'.join(tokens)
-        return f"{create_decl} {impala_sig} {joined_tokens}"
-
-
-class DropFunction(DropFunction):
-    def _impala_signature(self):
-        full_name = self._get_scoped_name(self.name, self.database)
-        input_sig = _impala_input_signature(self.inputs)
-        return f'{full_name}({input_sig})'
-
-
-class ListFunction(BaseDDL):
-    def __init__(self, database, like=None, aggregate=False):
-        self.database = database
-        self.like = like
-        self.aggregate = aggregate
-
-    def compile(self):
-        statement = 'SHOW '
-        if self.aggregate:
-            statement += 'AGGREGATE '
-        statement += f'FUNCTIONS IN {self.database}'
-        if self.like:
-            statement += f" LIKE '{self.like}'"
-        return statement
-
-
-def _impala_input_signature(inputs):
-    # TODO: varargs '{}...'.format(val)
-    return ', '.join(map(type_to_sql_string, inputs))
+class ExprFormatter(object):
+
+    """
+    For creating a nice tree-like representation of an expression graph for
+    displaying in the console.
+
+    TODO: detect reused DAG nodes and do not display redundant information
+    """
+
+    def __init__(self, expr, indent_size=2, base_level=0, memo=None,
+                 memoize=True):
+        self.expr = expr
+        self.indent_size = indent_size
+        self.base_level = base_level
+
+        self.memoize = memoize
+
+        # For tracking "extracted" objects, like tables, that we don't want to
+        # print out more than once, and simply alias in the expression tree
+        self.memo = memo or FormatMemo()
+
+    def get_result(self):
+        what = self.expr.op()
+
+        if self.memoize:
+            self._memoize_tables()
+
+        if isinstance(what, ir.TableNode) and what.has_schema():
+            # This should also catch aggregations
+            if not self.memoize and what in self.memo:
+                text = 'Table: %s' % self.memo.get_alias(what)
+            elif isinstance(what, ops.PhysicalTable):
+                text = self._format_table(what)
+            else:
+                # Any other node type
+                text = self._format_node(what)
+        elif isinstance(what, ops.TableColumn):
+            text = self._format_column(self.expr)
+        elif isinstance(what, ir.Node):
+            text = self._format_node(what)
+        elif isinstance(what, ops.Literal):
+            text = 'Literal[%s] %s' % (self._get_type_display(),
+                                       str(what.value))
+
+        if isinstance(self.expr, ir.ValueExpr) and self.expr._name is not None:
+            text = '{0} = {1}'.format(self.expr.get_name(), text)
+
+        if self.memoize:
+            alias_to_text = [(self.memo.aliases[x],
+                              self.memo.formatted[x],
+                              self.memo.ops[x])
+                             for x in self.memo.formatted]
+            alias_to_text.sort()
+
+            # A hack to suppress printing out of a ref that is the result of
+            # the top level expression
+            refs = [x + '\n' + y
+                    for x, y, op in alias_to_text
+                    if not op.equals(what)]
+
+            text = '\n\n'.join(refs + [text])
+
+        return self._indent(text, self.base_level)
+
+    def _memoize_tables(self):
+        table_memo_ops = (ops.Aggregation, ops.Filter,
+                          ops.Projection, ops.SelfReference)
+
+        def walk(expr):
+            op = expr.op()
+
+            def visit(arg):
+                if isinstance(arg, list):
+                    [visit(x) for x in arg]
+                elif isinstance(arg, ir.Expr):
+                    walk(arg)
+
+            if isinstance(op, ops.PhysicalTable):
+                self.memo.observe(op, self._format_table)
+            elif isinstance(op, ir.Node):
+                visit(op.args)
+                if isinstance(op, table_memo_ops):
+                    self.memo.observe(op, self._format_node)
+            elif isinstance(op, ir.TableNode) and op.has_schema():
+                self.memo.observe(op, self._format_table)
+
+        walk(self.expr)
+
+    def _indent(self, text, indents=1):
+        return util.indent(text, self.indent_size * indents)
+
+    def _format_table(self, table):
+        # format the schema
+        rows = ['name: {0!s}\nschema:'.format(table.name)]
+        rows.extend(['  %s : %s' % tup for tup in
+                     zip(table.schema.names, table.schema.types)])
+        opname = type(table).__name__
+        type_display = self._get_type_display(table)
+        opline = '%s[%s]' % (opname, type_display)
+        return '{0}\n{1}'.format(opline, self._indent('\n'.join(rows)))
+
+    def _format_column(self, expr):
+        # HACK: if column is pulled from a Filter of another table, this parent
+        # will not be found in the memo
+        col = expr.op()
+        parent_op = col.parent().op()
+        if parent_op in self.memo:
+            table_formatted = self.memo.get_alias(parent_op)
+        else:
+            table_formatted = '\n' + self._indent(self._format_node(parent_op))
+        type_display = self._get_type_display(self.expr)
+        return ("Column[{0}] '{1}' from table {2}"
+                .format(type_display, col.name, table_formatted))
+
+    def _format_node(self, op):
+        formatted_args = []
+
+        def visit(what, extra_indents=0):
+            if isinstance(what, ir.Expr):
+                result = self._format_subexpr(what)
+            else:
+                result = self._indent(str(what))
+
+            if extra_indents > 0:
+                result = util.indent(result, self.indent_size)
+
+            formatted_args.append(result)
+
+        arg_names = getattr(op, '_arg_names', None)
+
+        if arg_names is None:
+            for arg in op.args:
+                if isinstance(arg, list):
+                    for x in arg:
+                        visit(x)
+                else:
+                    visit(arg)
+        else:
+            for arg, name in zip(op.args, arg_names):
+                if name is not None:
+                    name = self._indent('{0}:'.format(name))
+                if isinstance(arg, list):
+                    if name is not None and len(arg) > 0:
+                        formatted_args.append(name)
+                        indents = 1
+                    else:
+                        indents = 0
+                    for x in arg:
+                        visit(x, extra_indents=indents)
+                else:
+                    if name is not None:
+                        formatted_args.append(name)
+                        indents = 1
+                    else:
+                        indents = 0
+                    visit(arg, extra_indents=indents)
+
+        opname = type(op).__name__
+        type_display = self._get_type_display(op)
+        opline = '%s[%s]' % (opname, type_display)
+
+        return '\n'.join([opline] + formatted_args)
+
+    def _format_subexpr(self, expr):
+        formatter = ExprFormatter(expr, base_level=1, memo=self.memo,
+                                  memoize=False)
+        return formatter.get_result()
+
+    def _get_type_display(self, expr=None):
+        if expr is None:
+            expr = self.expr
+
+        if isinstance(expr, ir.Node):
+            expr = expr.to_expr()
+
+        if isinstance(expr, ir.TableExpr):
+            return 'table'
+        elif isinstance(expr, ir.ArrayExpr):
+            return 'array(%s)' % expr.type()
+        elif isinstance(expr, ir.SortExpr):
+            return 'array-sort'
+        elif isinstance(expr, (ir.ScalarExpr, ir.AnalyticExpr)):
+            return '%s' % expr.type()
+        elif isinstance(expr, ir.ExprList):
+            list_args = [self._get_type_display(arg)
+                         for arg in expr.op().args]
+            return ', '.join(list_args)
+        else:
+            raise NotImplementedError
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/impala/metadata.py` & `ibis-framework-v0.6.0/ibis/impala/metadata.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,23 +1,23 @@
-from __future__ import annotations
-
 # Copyright 2014 Cloudera Inc.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 # http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from io import StringIO
+
+from six import StringIO
+import pandas as pd
 
 
 def parse_metadata(descr_table):
     parser = MetadataParser(descr_table)
     return parser.parse()
 
 
@@ -33,42 +33,30 @@
                 result = converter(result)
             return result
 
         return _converter
 
     return _get_item
 
-
 _get_type = _item_converter(1)
 _get_comment = _item_converter(2)
 
 
 def _try_timestamp(x):
-    import pandas as pd
-
     try:
-        ts = pd.Timestamp(x, tz='UTC')
-        return ts.to_pydatetime().replace(tzinfo=None)
+        return pd.Timestamp(x)
     except (ValueError, TypeError):
         return x
 
 
 def _try_unix_timestamp(x):
     try:
-        value = int(x)
+        return pd.Timestamp.fromtimestamp(int(x))
     except (ValueError, TypeError):
         return x
-    else:
-        import pandas as pd
-
-        return (
-            pd.Timestamp.fromtimestamp(value, tz="UTC")
-            .tz_localize(None)
-            .to_pydatetime()
-        )
 
 
 def _try_boolean(x):
     try:
         x = x.lower()
         if x in ('true', 'yes'):
             return True
@@ -82,16 +70,19 @@
 def _try_int(x):
     try:
         return int(x)
     except (ValueError, TypeError):
         return x
 
 
-class MetadataParser:
-    """A simple state machine to parse the results of `DESCRIBE FORMATTED`."""
+class MetadataParser(object):
+
+    """
+    A simple state-ish machine to parse the results of DESCRIBE FORMATTED
+    """
 
     def __init__(self, table):
         self.table = table
         self.tuples = list(self.table.itertuples(index=False))
 
     def _reset(self):
         self.pos = 0
@@ -108,17 +99,16 @@
         self.pos += 1
         return result
 
     def parse(self):
         self._reset()
         self._parse()
 
-        return TableMetadata(
-            self.schema, self.info, self.storage, partitions=self.partitions
-        )
+        return TableMetadata(self.schema, self.info, self.storage,
+                             partitions=self.partitions)
 
     def _parse(self):
         self.schema = self._parse_schema()
 
         next_section = self._next_tuple()
         if 'partition' in next_section[0].lower():
             self._parse_partitions()
@@ -133,38 +123,37 @@
             raise ValueError('Table information not present')
 
         self._parse_info()
 
     def _parse_schema(self):
         tup = self._next_tuple()
         if 'col_name' not in tup[0]:
-            raise ValueError(
-                'DESCRIBE FORMATTED did not return '
-                'the expected results: {}'.format(tup)
-            )
+            raise ValueError('DESCRIBE FORMATTED did not return '
+                             'the expected results: {0}'
+                             .format(tup))
         self._next_tuple()
 
         # Use for both main schema and partition schema (if any)
         schema = []
         while True:
             tup = self._next_tuple()
-            if not tup[0].strip():
+            if tup[0].strip() == '':
                 break
             schema.append((tup[0], tup[1]))
 
         return schema
 
     def _parse_info(self):
         self.info = {}
         while True:
             tup = self._next_tuple()
             orig_key = tup[0].strip(':')
             key = _clean_param_name(tup[0])
 
-            if not key or key.startswith('#'):
+            if key == '' or key.startswith('#'):
                 # section is done
                 break
 
             if key == 'table parameters':
                 self._parse_table_parameters()
             elif key in self._info_cleaners:
                 result = self._info_cleaners[key](tup)
@@ -181,15 +170,15 @@
         'database': _get_type(),
         'owner': _get_type(),
         'createtime': _get_type(_try_timestamp),
         'lastaccesstime': _get_type(_try_timestamp),
         'protect mode': _get_type(),
         'retention': _get_type(_try_int),
         'location': _get_type(),
-        'table type': _get_type(),
+        'table type': _get_type()
     }
 
     def _parse_table_parameters(self):
         params = self._parse_nested_params(self._table_param_cleaners)
         self.info['Table Parameters'] = params
 
     _table_param_cleaners = {
@@ -210,15 +199,15 @@
                 tup = self._next_tuple()
             except StopIteration:
                 break
 
             orig_key = tup[0].strip(':')
             key = _clean_param_name(tup[0])
 
-            if not key or key.startswith('#'):
+            if key == '' or key.startswith('#'):
                 # section is done
                 break
 
             if key == 'storage desc params':
                 self._parse_storage_desc_params()
             elif key in self._storage_cleaners:
                 result = self._storage_cleaners[key](tup)
@@ -234,16 +223,14 @@
     def _parse_storage_desc_params(self):
         params = self._parse_nested_params(self._storage_param_cleaners)
         self.storage['Desc Params'] = params
 
     _storage_param_cleaners = {}
 
     def _parse_nested_params(self, cleaners):
-        import pandas as pd
-
         params = {}
         while True:
             try:
                 tup = self._next_tuple()
             except StopIteration:
                 break
             if pd.isnull(tup[1]):
@@ -272,21 +259,23 @@
             for k in key:
                 if k not in result:
                     raise KeyError(k)
                 result = result[k]
             return result
         else:
             return data[key]
-
     return f
 
 
-class TableMetadata:
-    """Container for the parsed and wrangled results of `DESCRIBE FORMATTED`."""
+class TableMetadata(object):
 
+    """
+    Container for the parsed and wrangled results of DESCRIBE FORMATTED for
+    easier Ibis use (and testing).
+    """
     def __init__(self, schema, info, storage, partitions=None):
         self.schema = schema
         self.info = info
         self.storage = storage
         self.partitions = partitions
 
     def __repr__(self):
@@ -296,20 +285,20 @@
         buf = StringIO()
         buf.write(str(type(self)))
         buf.write('\n')
 
         data = {
             'schema': self.schema,
             'info': self.info,
-            'storage info': self.storage,
+            'storage info': self.storage
         }
         if self.partitions is not None:
             data['partition schema'] = self.partitions
 
-        pprint.pprint(data, stream=buf)  # noqa: T203
+        pprint.pprint(data, stream=buf)
 
         return buf.getvalue()
 
     @property
     def is_partitioned(self):
         return self.partitions is not None
 
@@ -319,13 +308,13 @@
     num_rows = _get_meta('info', ['Table Parameters', 'numRows'])
     hive_format = _get_meta('storage', 'InputFormat')
 
     tbl_properties = _get_meta('info', 'Table Parameters')
     serde_properties = _get_meta('storage', 'Desc Params')
 
 
-class TableInfo:
+class TableInfo(object):
     pass
 
 
-class TableStorageInfo:
+class TableStorageInfo(object):
     pass
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_metadata.py` & `ibis-framework-v0.6.0/ibis/impala/tests/test_metadata.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,140 +1,129 @@
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import pandas as pd
-import pytest
-import toolz
-from numpy import nan
 
-from ibis.backends.impala.metadata import parse_metadata
+from numpy import nan
 
+from ibis.compat import unittest
+from ibis.impala.metadata import parse_metadata
 
-@pytest.fixture(scope="module")
-def spacer():
-    return ('', nan, nan)
-
-
-@pytest.fixture(scope="module")
-def schema(spacer):
-    return [
-        ('# col_name', 'data_type', 'comment'),
-        spacer,
-        ('foo', 'int', nan),
-        ('bar', 'tinyint', nan),
-        ('baz', 'bigint', nan),
-    ]
-
-
-@pytest.fixture(scope="module")
-def partitions(spacer):
-    return [
-        ('# Partition Information', nan, nan),
-        ('# col_name', 'data_type', 'comment'),
-        spacer,
-        ('qux', 'bigint', nan),
-    ]
-
-
-@pytest.fixture(scope="module")
-def info():
-    return [
-        ('# Detailed Table Information', nan, nan),
-        ('Database:', 'tpcds', nan),
-        ('Owner:', 'wesm', nan),
-        ('CreateTime:', '2015-11-08 01:09:42-08:00', nan),
-        ('LastAccessTime:', 'UNKNOWN', nan),
-        ('Protect Mode:', 'None', nan),
-        ('Retention:', '0', nan),
-        ('Location:', 'hdfs://host-name:20500/my.db/dbname.table_name', nan),
-        ('Table Type:', 'EXTERNAL_TABLE', nan),
-        ('Table Parameters:', nan, nan),
-        ('', 'EXTERNAL', 'TRUE'),
-        ('', 'STATS_GENERATED_VIA_STATS_TASK', 'true'),
-        ('', 'numRows', '183592'),
-        ('', 'transient_lastDdlTime', '1447340941'),
-    ]
-
-
-@pytest.fixture(scope="module")
-def storage_info():
-    return [
-        ('# Storage Information', nan, nan),
-        ('SerDe Library:', 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe', nan),
-        ('InputFormat:', 'org.apache.hadoop.mapred.TextInputFormat', nan),
-        (
-            'OutputFormat:',
-            'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',
-            nan,
-        ),
-        ('Compressed:', 'No', nan),
-        ('Num Buckets:', '0', nan),
-        ('Bucket Columns:', '[]', nan),
-        ('Sort Columns:', '[]', nan),
-        ('Storage Desc Params:', nan, nan),
-        ('', 'field.delim', '|'),
-        ('', 'serialization.format', '|'),
-    ]
-
-
-@pytest.fixture(scope="module")
-def part_metadata(spacer, schema, partitions, info, storage_info):
-    return pd.DataFrame.from_records(
-        list(
-            toolz.concat(
-                toolz.interpose([spacer], [schema, partitions, info, storage_info])
-            )
-        ),
-        columns=['name', 'type', 'comment'],
-    )
-
-
-@pytest.fixture(scope="module")
-def unpart_metadata(spacer, schema, info, storage_info):
-    return pd.DataFrame.from_records(
-        list(toolz.concat(toolz.interpose([spacer], [schema, info, storage_info]))),
-        columns=['name', 'type', 'comment'],
-    )
-
-
-@pytest.fixture(scope="module")
-def parsed_part(part_metadata):
-    return parse_metadata(part_metadata)
-
-
-@pytest.fixture(scope="module")
-def parsed_unpart(unpart_metadata):
-    return parse_metadata(unpart_metadata)
-
-
-def test_table_params(parsed_part):
-    params = parsed_part.info['Table Parameters']
-
-    assert params['EXTERNAL'] is True
-    assert params['STATS_GENERATED_VIA_STATS_TASK'] is True
-    assert params['numRows'] == 183592
-    assert params['transient_lastDdlTime'] == pd.Timestamp('2015-11-12 15:09:01')
-
-
-def test_partitions(parsed_unpart, parsed_part):
-    assert parsed_unpart.partitions is None
-    assert parsed_part.partitions == [('qux', 'bigint')]
-
-
-def test_schema(parsed_part):
-    assert parsed_part.schema == [
-        ('foo', 'int'),
-        ('bar', 'tinyint'),
-        ('baz', 'bigint'),
-    ]
-
-
-def test_storage_info(parsed_part):
-    storage = parsed_part.storage
-    assert storage['Compressed'] is False
-    assert storage['Num Buckets'] == 0
 
+def _glue_lists_spacer(spacer, lists):
+    result = list(lists[0])
+    for lst in lists[1:]:
+        result.append(spacer)
+        result.extend(lst)
+    return result
+
+
+class TestMetadataParser(unittest.TestCase):
+
+    @classmethod
+    def setUpClass(cls):
+        cls.spacer = ('', nan, nan)
+
+        cls.schema = [
+            ('# col_name', 'data_type', 'comment'),
+            cls.spacer,
+            ('foo', 'int', nan),
+            ('bar', 'tinyint', nan),
+            ('baz', 'bigint', nan)
+        ]
+
+        cls.partitions = [
+            ('# Partition Information', nan, nan),
+            ('# col_name', 'data_type', 'comment'),
+            cls.spacer,
+            ('qux', 'bigint', nan)
+        ]
+
+        cls.info = [
+            ('# Detailed Table Information', nan, nan),
+            ('Database:', 'tpcds', nan),
+            ('Owner:', 'wesm', nan),
+            ('CreateTime:', 'Sun Nov 08 01:09:42 PST 2015', nan),
+            ('LastAccessTime:', 'UNKNOWN', nan),
+            ('Protect Mode:', 'None', nan),
+            ('Retention:', '0', nan),
+            ('Location:', ('hdfs://host-name:20500/my.db'
+                           '/dbname.table_name'), nan),
+            ('Table Type:', 'EXTERNAL_TABLE', nan),
+            ('Table Parameters:', nan, nan),
+            ('', 'EXTERNAL', 'TRUE'),
+            ('', 'STATS_GENERATED_VIA_STATS_TASK', 'true'),
+            ('', 'numRows', '183592'),
+            ('', 'transient_lastDdlTime', '1447369741'),
+        ]
+
+        cls.storage_info = [
+            ('# Storage Information', nan, nan),
+            ('SerDe Library:', ('org.apache.hadoop'
+                                '.hive.serde2.lazy.LazySimpleSerDe'), nan),
+            ('InputFormat:', 'org.apache.hadoop.mapred.TextInputFormat', nan),
+            ('OutputFormat:',
+             'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',
+             nan),
+            ('Compressed:', 'No', nan),
+            ('Num Buckets:', '0', nan),
+            ('Bucket Columns:', '[]', nan),
+            ('Sort Columns:', '[]', nan),
+            ('Storage Desc Params:', nan, nan),
+            ('', 'field.delim', '|'),
+            ('', 'serialization.format', '|')
+        ]
+
+        cls.part_metadata = pd.DataFrame.from_records(
+            _glue_lists_spacer(cls.spacer, [cls.schema, cls.partitions,
+                                            cls.info, cls.storage_info]),
+            columns=['name', 'type', 'comment'])
+
+        cls.unpart_metadata = pd.DataFrame.from_records(
+            _glue_lists_spacer(cls.spacer, [cls.schema, cls.info,
+                                            cls.storage_info]),
+            columns=['name', 'type', 'comment'])
+
+        cls.parsed_part = parse_metadata(cls.part_metadata)
+        cls.parsed_unpart = parse_metadata(cls.unpart_metadata)
+
+    def test_table_params(self):
+        params = self.parsed_part.info['Table Parameters']
+
+        assert params['EXTERNAL'] is True
+        assert params['STATS_GENERATED_VIA_STATS_TASK'] is True
+        assert params['numRows'] == 183592
+        assert (params['transient_lastDdlTime'] ==
+                pd.Timestamp('2015-11-12 15:09:01'))
+
+    def test_partitions(self):
+        assert self.parsed_unpart.partitions is None
+        assert self.parsed_part.partitions == [('qux', 'bigint')]
+
+    def test_schema(self):
+        assert self.parsed_part.schema == [
+            ('foo', 'int'),
+            ('bar', 'tinyint'),
+            ('baz', 'bigint')
+        ]
+
+    def test_storage_info(self):
+        storage = self.parsed_part.storage
+        assert storage['Compressed'] is False
+        assert storage['Num Buckets'] == 0
 
-def test_storage_params(parsed_part):
-    params = parsed_part.storage['Desc Params']
+    def test_storage_params(self):
+        params = self.parsed_part.storage['Desc Params']
 
-    assert params['field.delim'] == '|'
-    assert params['serialization.format'] == '|'
+        assert params['field.delim'] == '|'
+        assert params['serialization.format'] == '|'
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_partition.py` & `ibis-framework-v0.6.0/ibis/impala/tests/test_partition.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,253 +1,253 @@
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-import os
 from posixpath import join as pjoin
+import pytest
 
+from pandas.util.testing import assert_frame_equal
 import pandas as pd
-import pandas.testing as tm
-import pytest
 
-import ibis
-from ibis import util
+from ibis.compat import unittest
+from ibis.impala.compat import ImpylaError
+from ibis.impala.tests.common import ImpalaE2E
 from ibis.tests.util import assert_equal
-
-pytest.importorskip("impala")
-
-from ibis.backends.impala.compat import ImpylaError  # noqa: E402
-
-
-@pytest.fixture
-def df():
-    df = pd.DataFrame(
-        {
-            'year': [2009] * 3 + [2010] * 3,
-            'month': list(map(str, [1, 2, 3] * 2)),
-            'value': list(range(1, 7)),
-        },
-        index=list(range(6)),
-    )
-    df = pd.concat([df] * 10, ignore_index=True)
-    df['id'] = df.index.values
-    return df
-
-
-@pytest.fixture
-def unpart_t(con, df, tmp_db):
-    pd_name = f'__ibis_test_partition_{util.guid()}'
-    con.create_table(pd_name, df, database=tmp_db)
-    assert pd_name in con.list_tables(database=tmp_db), pd_name
-    yield con.table(pd_name, database=tmp_db)
-    con.drop_table(pd_name, database=tmp_db)
-
-
-def test_is_partitioned(con, temp_table):
-    schema = ibis.schema([('foo', 'string'), ('year', 'int32'), ('month', 'string')])
-    con.create_table(temp_table, schema=schema, partition=['year', 'month'])
-    assert con.table(temp_table).is_partitioned
-
-
-def test_create_table_with_partition_column(con, temp_table_db):
-    schema = ibis.schema(
-        [
-            ('year', 'int32'),
-            ('month', 'string'),
-            ('day', 'int8'),
-            ('value', 'double'),
-        ]
-    )
-
-    tmp_db, name = temp_table_db
-    con.create_table(name, schema=schema, database=tmp_db, partition=['year', 'month'])
-
-    # the partition column get put at the end of the table
-    ex_schema = ibis.schema(
-        [
-            ('day', 'int8'),
-            ('value', 'double'),
-            ('year', 'int32'),
-            ('month', 'string'),
-        ]
-    )
-    table_schema = con.get_schema(name, database=tmp_db)
-    assert_equal(table_schema, ex_schema)
-
-    partition_schema = con.database(tmp_db).table(name).partition_schema()
-
-    expected = ibis.schema([('year', 'int32'), ('month', 'string')])
-    assert_equal(partition_schema, expected)
-
-
-def test_create_partitioned_separate_schema(con, temp_table):
-    schema = ibis.schema([('day', 'int8'), ('value', 'double')])
-    part_schema = ibis.schema([('year', 'int32'), ('month', 'string')])
-
-    con.create_table(temp_table, schema=schema, partition=part_schema)
-
-    # the partition column get put at the end of the table
-    ex_schema = ibis.schema(
-        [
-            ('day', 'int8'),
-            ('value', 'double'),
-            ('year', 'int32'),
-            ('month', 'string'),
-        ]
-    )
-    table_schema = con.get_schema(temp_table)
-    assert_equal(table_schema, ex_schema)
-
-    partition_schema = con.table(temp_table).partition_schema()
-    assert_equal(partition_schema, part_schema)
-
-
-def test_unpartitioned_table_get_schema(con):
-    tname = 'functional_alltypes'
-    with pytest.raises(ImpylaError):
-        con.table(tname).partition_schema()
-
-
-def test_insert_select_partitioned_table(con, df, temp_table, unpart_t):
-    part_keys = ['year', 'month']
-
-    con.create_table(temp_table, schema=unpart_t.schema(), partition=part_keys)
-    part_t = con.table(temp_table)
-    unique_keys = df[part_keys].drop_duplicates()
-
-    for i, (year, month) in enumerate(unique_keys.itertuples(index=False)):
-        select_stmt = unpart_t[(unpart_t.year == year) & (unpart_t.month == month)]
-
-        # test both styles of insert
-        if i:
-            part = {'year': year, 'month': month}
-        else:
-            part = [year, month]
-        part_t.insert(select_stmt, partition=part)
-
-    verify_partitioned_table(part_t, df, unique_keys)
-
-
-@pytest.fixture
-def tmp_parted(con):
-    name = f'tmppart_{util.guid()}'
-    yield name
-    con.drop_table(name, force=True)
-
-
-def test_create_partitioned_table_from_expr(con, alltypes, tmp_parted):
-    t = alltypes
-    expr = t[t.id <= 10][['id', 'double_col', 'month', 'year']]
-    name = tmp_parted
-    con.create_table(name, expr, partition=[t.year])
-    new = con.table(name)
-    expected = expr.execute().sort_values('id').reset_index(drop=True)
-    result = new.execute().sort_values('id').reset_index(drop=True)
-    tm.assert_frame_equal(result, expected)
-
-
-def test_add_drop_partition_no_location(con, temp_table):
-    schema = ibis.schema([('foo', 'string'), ('year', 'int32'), ('month', 'int16')])
-    con.create_table(temp_table, schema=schema, partition=['year', 'month'])
-    table = con.table(temp_table)
-
-    part = {'year': 2007, 'month': 4}
-
-    table.add_partition(part)
-
-    assert len(table.partitions()) == 2
-
-    table.drop_partition(part)
-
-    assert len(table.partitions()) == 1
-
-
-@pytest.mark.hdfs
-def test_add_drop_partition_owned_by_impala(hdfs, con, temp_table):
-    schema = ibis.schema([('foo', 'string'), ('year', 'int32'), ('month', 'int16')])
-    con.create_table(temp_table, schema=schema, partition=['year', 'month'])
-
-    table = con.table(temp_table)
-
-    part = {'year': 2007, 'month': 4}
-
-    subdir = util.guid()
-    basename = util.guid()
-    path = f'/tmp/{subdir}/{basename}'
-
-    parent = os.path.dirname(path)
-    hdfs.mkdir(parent, create_parents=True)
-    hdfs.chown(parent, owner='impala', group='supergroup')
-
-    table.add_partition(part, location=path)
-
-    assert len(table.partitions()) == 2
-
-    table.drop_partition(part)
-
-    assert len(table.partitions()) == 1
-
-
-def test_add_drop_partition_hive_bug(con, temp_table):
-    schema = ibis.schema([('foo', 'string'), ('year', 'int32'), ('month', 'int16')])
-    con.create_table(temp_table, schema=schema, partition=['year', 'month'])
-
-    table = con.table(temp_table)
-
-    part = {'year': 2007, 'month': 4}
-
-    path = f'/tmp/{util.guid()}'
-
-    table.add_partition(part, location=path)
-
-    assert len(table.partitions()) == 2
-
-    table.drop_partition(part)
-
-    assert len(table.partitions()) == 1
-
-
-@pytest.mark.hdfs
-def test_load_data_partition(con, hdfs, tmp_dir, unpart_t, df, temp_table):
-    part_keys = ['year', 'month']
-
-    con.create_table(temp_table, schema=unpart_t.schema(), partition=part_keys)
-    part_t = con.table(temp_table)
-
-    # trim the runtime of this test
-    df = df[df.month == '1'].reset_index(drop=True)
-
-    unique_keys = df[part_keys].drop_duplicates()
-
-    hdfs_dir = pjoin(tmp_dir, 'load-data-partition')
-
-    df2 = df.drop(['year', 'month'], axis='columns')
-
-    csv_props = {'serialization.format': ',', 'field.delim': ','}
-
-    for i, (year, month) in enumerate(unique_keys.itertuples(index=False)):
-        chunk = df2[(df.year == year) & (df.month == month)]
-        chunk_path = pjoin(hdfs_dir, f'{i}.csv')
-
-        con.write_dataframe(chunk, chunk_path)
-
-        # test both styles of insert
-        if i:
-            part = {'year': year, 'month': month}
-        else:
-            part = [year, month]
-
-        part_t.add_partition(part)
-        part_t.alter_partition(part, format='text', serde_properties=csv_props)
-        part_t.load_data(chunk_path, partition=part)
-
-    hdfs.rm(hdfs_dir, recursive=True)
-    verify_partitioned_table(part_t, df, unique_keys)
+import ibis
+import ibis.util as util
 
 
-def verify_partitioned_table(part_t, df, unique_keys):
-    result = part_t.execute().sort_values(by='id').reset_index(drop=True)[df.columns]
+def _tmp_name():
+    return 'tmp_partition_{0}'.format(util.guid())
 
-    tm.assert_frame_equal(result, df)
 
-    parts = part_t.partitions()
+class TestPartitioning(ImpalaE2E, unittest.TestCase):
 
-    # allow for the total line
-    assert len(parts) == len(unique_keys) + 1
+    @classmethod
+    def setUpClass(cls):
+        ImpalaE2E.setup_e2e(cls)
+
+        df = pd.DataFrame({'year': [2009, 2009, 2009, 2010, 2010, 2010],
+                           'month': [1, 2, 3, 1, 2, 3],
+                           'value': [1, 2, 3, 4, 5, 6]})
+        df = pd.concat([df] * 10, ignore_index=True)
+        df['id'] = df.index.values
+
+        cls.df = df
+        cls.db = cls.con.database(cls.tmp_db)
+        cls.pd_name = _tmp_name()
+        cls.db.create_table(cls.pd_name, df,
+                            location=cls._temp_location())
+
+    @classmethod
+    def _temp_location(cls):
+        return cls._create_777_tmp_dir()
+
+    def test_is_partitioned(self):
+        schema = ibis.schema([('foo', 'string'),
+                              ('year', 'int32'),
+                              ('month', 'int16')])
+        name = _tmp_name()
+        self.db.create_table(name, schema=schema,
+                             partition=['year', 'month'],
+                             location=self._temp_location())
+        assert self.db.table(name).is_partitioned
+
+    @pytest.mark.superuser
+    def test_create_table_with_partition_column(self):
+        schema = ibis.schema([('year', 'int32'),
+                              ('month', 'int8'),
+                              ('day', 'int8'),
+                              ('value', 'double')])
+
+        name = _tmp_name()
+        self.con.create_table(name, schema=schema,
+                              database=self.tmp_db,
+                              partition=['year', 'month'],
+                              location=self._temp_location())
+        self.temp_tables.append(name)
+
+        # the partition column get put at the end of the table
+        ex_schema = ibis.schema([('day', 'int8'),
+                                 ('value', 'double'),
+                                 ('year', 'int32'),
+                                 ('month', 'int8')])
+        table_schema = self.con.get_schema(name, database=self.tmp_db)
+        assert_equal(table_schema, ex_schema)
+
+        partition_schema = self.db.table(name).partition_schema()
+
+        expected = ibis.schema([('year', 'int32'),
+                                ('month', 'int8')])
+        assert_equal(partition_schema, expected)
+
+    @pytest.mark.superuser
+    def test_create_partitioned_separate_schema(self):
+        schema = ibis.schema([('day', 'int8'),
+                              ('value', 'double')])
+        part_schema = ibis.schema([('year', 'int32'),
+                                   ('month', 'int8')])
+
+        name = _tmp_name()
+        self.con.create_table(name, schema=schema, partition=part_schema,
+                              location=self._temp_location())
+        self.temp_tables.append(name)
+
+        # the partition column get put at the end of the table
+        ex_schema = ibis.schema([('day', 'int8'),
+                                 ('value', 'double'),
+                                 ('year', 'int32'),
+                                 ('month', 'int8')])
+        table_schema = self.con.get_schema(name)
+        assert_equal(table_schema, ex_schema)
+
+        partition_schema = self.con.table(name).partition_schema()
+        assert_equal(partition_schema, part_schema)
+
+    @pytest.mark.superuser
+    def test_unpartitioned_table_get_schema(self):
+        tname = 'functional_alltypes'
+        with self.assertRaises(ImpylaError):
+            self.con.table(tname).partition_schema()
+
+    @pytest.mark.superuser
+    def test_insert_select_partitioned_table(self):
+        df = self.df
+
+        unpart_t = self.db.table(self.pd_name)
+        part_keys = ['year', 'month']
+        part_t = self._create_partitioned_table(unpart_t.schema(),
+                                                part_keys)
+        unique_keys = df[part_keys].drop_duplicates()
+
+        for i, (year, month) in enumerate(unique_keys.itertuples(index=False)):
+            select_stmt = unpart_t[(unpart_t.year == year) &
+                                   (unpart_t.month == month)]
+
+            # test both styles of insert
+            if i:
+                part = {'year': year, 'month': month}
+            else:
+                part = [year, month]
+            part_t.insert(select_stmt, partition=part)
+
+        self._verify_partitioned_table(part_t, df, unique_keys)
+
+    @pytest.mark.superuser
+    def test_insert_overwrite_partition(self):
+        pass
+
+    @pytest.mark.superuser
+    def test_dynamic_partitioning(self):
+        pass
+
+    @pytest.mark.superuser
+    def test_add_drop_partition(self):
+        schema = ibis.schema([('foo', 'string'),
+                              ('year', 'int32'),
+                              ('month', 'int16')])
+        name = _tmp_name()
+        tmp_dir = self._temp_location()
+        self.db.create_table(name, schema=schema,
+                             partition=['year', 'month'],
+                             location=tmp_dir)
+
+        table = self.db.table(name)
+
+        part = {'year': 2007, 'month': 4}
+
+        path = '/tmp/tmp-{0}'.format(util.guid())
+        table.add_partition(part, location=path)
+
+        assert len(table.partitions()) == 2
+
+        table.drop_partition(part)
+
+        assert len(table.partitions()) == 1
+
+    @pytest.mark.superuser
+    def test_set_partition_location(self):
+        pass
+
+    @pytest.mark.superuser
+    def test_load_data_partition(self):
+        df = self.df
+
+        unpart_t = self.db.table(self.pd_name)
+        part_keys = ['year', 'month']
+        part_t = self._create_partitioned_table(unpart_t.schema(),
+                                                part_keys)
+
+        # trim the runtime of this test
+        df = df[df.month == 1].reset_index(drop=True)
+
+        unique_keys = df[part_keys].drop_duplicates()
+
+        hdfs_dir = pjoin(self.tmp_dir, 'load-data-partition')
+
+        df2 = df.drop(['year', 'month'], axis='columns')
+
+        csv_props = {
+            'serialization.format': ',',
+            'field.delim': ','
+        }
+
+        for i, (year, month) in enumerate(unique_keys.itertuples(index=False)):
+            chunk = df2[(df.year == year) & (df.month == month)]
+            chunk_path = pjoin(hdfs_dir, '{0}.csv'.format(i))
+
+            self.con.write_dataframe(chunk, chunk_path)
+
+            # test both styles of insert
+            if i:
+                part = {'year': year, 'month': month}
+            else:
+                part = [year, month]
+
+            part_t.add_partition(part)
+            part_t.alter_partition(part, format='text',
+                                   serde_properties=csv_props)
+            part_t.load_data(chunk_path, partition=part)
+
+        self.hdfs.rmdir(hdfs_dir)
+        self._verify_partitioned_table(part_t, df, unique_keys)
+
+    def _verify_partitioned_table(self, part_t, df, unique_keys):
+        result = (part_t.execute()
+                  .sort_index(by='id')
+                  .reset_index(drop=True)
+                  [df.columns])
+
+        assert_frame_equal(result, df)
+
+        parts = part_t.partitions()
+
+        # allow for the total line
+        assert len(parts) == (len(unique_keys) + 1)
+
+    def _create_partitioned_table(self, schema, part_keys):
+        part_name = _tmp_name()
+
+        self.db.create_table(part_name,
+                             schema=schema,
+                             partition=part_keys,
+                             location=self._temp_location())
+        self.temp_tables.append(part_name)
+        return self.db.table(part_name)
+
+    @pytest.mark.superuser
+    def test_drop_partition(self):
+        pass
+
+    @pytest.mark.superuser
+    def test_repartition_automated(self):
+        pass
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/impala/tests/test_udf.py` & `ibis-framework-v0.6.0/ibis/impala/tests/test_udf.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,664 +1,502 @@
-from __future__ import annotations
+# Copyright 2015 Cloudera Inc
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
-from decimal import Decimal
 from posixpath import join as pjoin
-
-import numpy as np
-import pandas as pd
 import pytest
 
 import ibis
-import ibis.backends.impala as api
-import ibis.common.exceptions as com
-import ibis.expr.datatypes as dt
-import ibis.expr.types as ir
-from ibis import util
-from ibis.backends.impala import ddl
-from ibis.common.exceptions import IbisTypeError
-from ibis.expr import rules
-
-pytest.importorskip("impala")
-
-
-@pytest.fixture(scope="module")
-def table(mockcon):
-    return mockcon.table("functional_alltypes")
-
-
-@pytest.fixture
-def i8(table):
-    return table.tinyint_col
-
-
-@pytest.fixture
-def i16(table):
-    return table.smallint_col
-
-
-@pytest.fixture
-def i32(table):
-    return table.int_col
-
-
-@pytest.fixture
-def i64(table):
-    return table.bigint_col
-
-
-@pytest.fixture
-def d(table):
-    return table.double_col
-
-
-@pytest.fixture
-def f(table):
-    return table.float_col
-
-
-@pytest.fixture
-def s(table):
-    return table.string_col
-
-
-@pytest.fixture
-def b(table):
-    return table.bool_col
-
-
-@pytest.fixture
-def t(table):
-    return table.timestamp_col
-
-
-@pytest.fixture
-def tpch_customer(con):
-    return con.table("customer")
-
-
-@pytest.fixture
-def dec(tpch_customer):
-    return tpch_customer.c_acctbal
-
-
-@pytest.fixture
-def all_cols(i8, i16, i32, i64, d, f, dec, s, b, t):
-    return [
-        i8,
-        i16,
-        i32,
-        i64,
-        d,
-        f,
-        dec,
-        s,
-        b,
-        t,
-    ]
-
-
-def test_sql_generation(snapshot):
-    func = api.scalar_function(['string'], 'string', name='Tester')
-    func.register('identity', 'udf_testing')
-
-    result = func('hello world')
-    snapshot.assert_match(ibis.impala.compile(result), "out.sql")
-
-
-def test_sql_generation_from_infoclass(snapshot):
-    func = api.wrap_udf('test.so', ['string'], 'string', 'info_test')
-    repr(func)
-
-    func.register('info_test', 'udf_testing')
-    result = func('hello world').name('tmp')
-    snapshot.assert_match(ibis.impala.compile(result), "out.sql")
-
-
-@pytest.mark.parametrize(
-    ("ty", "value", "column"),
-    [
-        pytest.param('boolean', True, "bool_col", id="boolean"),
-        pytest.param('int8', 1, "tinyint_col", id="int8"),
-        pytest.param('int16', 1, "smallint_col", id="int16"),
-        pytest.param('int32', 1, "int_col", id="int32"),
-        pytest.param('int64', 1, "bigint_col", id="int64"),
-        pytest.param('float', 1.0, "float_col", id="float"),
-        pytest.param('double', 1.0, "double_col", id="double"),
-        pytest.param('string', '1', "string_col", id="string"),
-        pytest.param(
-            'timestamp',
-            ibis.timestamp('1961-04-10'),
-            "timestamp_col",
-            id="timestamp",
-        ),
-    ],
-)
-def test_udf_primitive_output_types(ty, value, column, table):
-    func = _register_udf([ty], ty, 'test')
-
-    ibis_type = dt.validate_type(ty)
-
-    expr = func(value)
-    assert type(expr) == getattr(ir, ibis_type.scalar)
-
-    expr = func(table[column])
-    assert type(expr) == getattr(ir, ibis_type.column)
-
-
-@pytest.mark.parametrize(
-    ("ty", "value"),
-    [
-        pytest.param('boolean', True, id="boolean"),
-        pytest.param('int8', 1, id="int8"),
-        pytest.param('int16', 1, id="int16"),
-        pytest.param('int32', 1, id="int32"),
-        pytest.param('int64', 1, id="int64"),
-        pytest.param('float', 1.0, id="float"),
-        pytest.param('double', 1.0, id="double"),
-        pytest.param('string', '1', id="string"),
-        pytest.param(
-            'timestamp',
-            ibis.timestamp('1961-04-10'),
-            id="timestamp",
-        ),
-    ],
-)
-def test_uda_primitive_output_types(ty, value):
-    func = _register_uda([ty], ty, 'test')
-
-    ibis_type = dt.validate_type(ty)
-    scalar_type = getattr(ir, ibis_type.scalar)
-
-    expr1 = func(value)
-    assert isinstance(expr1, scalar_type)
-
-    expr2 = func(value)
-    assert isinstance(expr2, scalar_type)
-
-
-def test_decimal(dec):
-    func = _register_udf(['decimal(12, 2)'], 'decimal(12, 2)', 'test')
-    expr = func(1.0)
-    assert type(expr) == ir.DecimalScalar
-    expr = func(dec)
-    assert type(expr) == ir.DecimalColumn
-
-
-@pytest.mark.parametrize(
-    ("ty", "valid_cast_indexer"),
-    [
-        pytest.param("decimal(12, 2)", slice(7), id="decimal"),
-        pytest.param("double", slice(6), id="double"),
-        pytest.param("float", slice(6), id="float"),
-        pytest.param("int16", slice(2), id="int16"),
-        pytest.param("int32", slice(3), id="int32"),
-        pytest.param("int64", slice(4), id="int64"),
-        pytest.param("int8", slice(1), id="int8"),
-    ],
-)
-def test_udf_valid_typecasting(ty, valid_cast_indexer, all_cols):
-    func = _register_udf([ty], 'int32', 'typecast')
-
-    for expr in all_cols[valid_cast_indexer]:
-        func(expr)
-
-
-@pytest.mark.parametrize(
-    ("ty", "valid_cast_indexer"),
-    [
-        pytest.param("boolean", slice(8), id="boolean_first_8"),
-        pytest.param("boolean", slice(9, None), id="boolean_9_onwards"),
-        pytest.param("decimal", slice(7, None), id="decimal"),
-        pytest.param("double", slice(-3, None), id="double"),
-        pytest.param("float", slice(-3, None), id="float"),
-        pytest.param("int16", slice(2, None), id="int16"),
-        pytest.param("int32", slice(3, None), id="int32"),
-        pytest.param("int64", slice(4, None), id="int64"),
-        pytest.param("int8", slice(1, None), id="int8"),
-        pytest.param("string", slice(7), id="string_first_7"),
-        pytest.param("string", slice(8, None), id="string_8_onwards"),
-        pytest.param("timestamp", slice(-1), id="timestamp"),
-    ],
-)
-def test_udf_invalid_typecasting(ty, valid_cast_indexer, all_cols):
-    func = _register_udf([ty], 'int32', 'typecast')
-
-    for expr in all_cols[valid_cast_indexer]:
-        with pytest.raises(IbisTypeError):
-            func(expr)
-
-
-def test_mult_args(i32, d, s, b, t):
-    func = _register_udf(
-        ['int32', 'double', 'string', 'boolean', 'timestamp'],
-        'int64',
-        'mult_types',
-    )
-
-    expr = func(i32, d, s, b, t)
-    assert issubclass(type(expr), ir.Column)
-
-    expr = func(1, 1.0, 'a', True, ibis.timestamp('1961-04-10'))
-    assert issubclass(type(expr), ir.Scalar)
-
-
-def _register_udf(inputs, output, name):
-    func = api.scalar_function(inputs, output, name=name)
-    func.register(name, 'ibis_testing')
-    return func
-
-
-def _register_uda(inputs, output, name):
-    func = api.aggregate_function(inputs, output, name=name)
-    func.register(name, 'ibis_testing')
-    return func
-
-
-@pytest.fixture
-def udfcon(con, monkeypatch):
-    monkeypatch.setitem(con.con.options, "DISABLE_CODEGEN", "0")
-    return con
-
-
-@pytest.fixture
-def alltypes(udfcon):
-    return udfcon.table('functional_alltypes')
-
-
-@pytest.fixture
-def udf_ll(test_data_dir):
-    return pjoin(test_data_dir, 'udf/udf-sample.ll')
-
-
-@pytest.fixture
-def uda_ll(test_data_dir):
-    return pjoin(test_data_dir, 'udf/uda-sample.ll')
-
-
-@pytest.fixture
-def uda_so(test_data_dir):
-    return pjoin(test_data_dir, 'udf/libudasample.so')
-
-
-@pytest.mark.parametrize(
-    ('typ', 'lit_val', 'col_name'),
-    [
-        pytest.param('boolean', True, 'bool_col', id="boolean"),
-        pytest.param('int8', ibis.literal(5), 'tinyint_col', id="int8"),
-        pytest.param(
-            'int16',
-            ibis.literal(2**10),
-            'smallint_col',
-            id="int16",
-        ),
-        pytest.param('int32', ibis.literal(2**17), 'int_col', id="int16"),
-        pytest.param('int64', ibis.literal(2**33), 'bigint_col', id="int64"),
-        pytest.param('float', ibis.literal(3.14), 'float_col', id="float"),
-        pytest.param('double', ibis.literal(3.14), 'double_col', id="double"),
-        pytest.param(
-            'string',
-            ibis.literal('ibis'),
-            'string_col',
-            id="string",
-        ),
-        pytest.param(
-            'timestamp',
-            ibis.timestamp('1961-04-10'),
-            'timestamp_col',
-            id="timestamp",
-        ),
-    ],
-)
-@pytest.mark.xfail(
-    reason='Unknown reason. xfailing to restore the CI for udf tests. #2358'
-)
-def test_identity_primitive_types(
-    udfcon, alltypes, test_data_db, udf_ll, typ, lit_val, col_name
-):
-    col_val = alltypes[col_name]
-    identity_func_testing(udf_ll, udfcon, test_data_db, typ, lit_val, col_val)
-
-
-@pytest.mark.xfail(
-    reason='Unknown reason. xfailing to restore the CI for udf tests. #2358'
-)
-def test_decimal_fail(udfcon, test_data_db, udf_ll):
-    col = udfcon.table('customer').c_acctbal
-    literal = ibis.literal(1).cast('decimal(12,2)')
-    name = '__tmp_udf_' + util.guid()
-
-    func = udf_creation_to_op(
-        udf_ll,
-        udfcon,
-        test_data_db,
-        name,
-        'Identity',
-        ['decimal(12,2)'],
-        'decimal(12,2)',
-    )
-
-    expr = func(literal)
-    assert issubclass(type(expr), ir.Scalar)
-    result = udfcon.execute(expr)
-    assert result == Decimal(1)
-
-    expr = func(col)
-    assert issubclass(type(expr), ir.Column)
-    udfcon.execute(expr)
-
-
-@pytest.mark.xfail(
-    reason='Unknown reason. xfailing to restore the CI for udf tests. #2358'
-)
-def test_mixed_inputs(udfcon, alltypes, test_data_db, udf_ll):
-    name = 'two_args'
-    symbol = 'TwoArgs'
-    inputs = ['int32', 'int32']
-    output = 'int32'
-    func = udf_creation_to_op(
-        udf_ll, udfcon, test_data_db, name, symbol, inputs, output
-    )
-
-    expr = func(alltypes.int_col, 1)
-    assert issubclass(type(expr), ir.Column)
-    udfcon.execute(expr)
-
-    expr = func(1, alltypes.int_col)
-    assert issubclass(type(expr), ir.Column)
-    udfcon.execute(expr)
-
-    expr = func(alltypes.int_col, alltypes.tinyint_col)
-    udfcon.execute(expr)
-
-
-@pytest.mark.xfail(
-    reason='Unknown reason. xfailing to restore the CI for udf tests. #2358'
-)
-def test_implicit_typecasting(udfcon, alltypes, test_data_db, udf_ll):
-    col = alltypes.tinyint_col
-    literal = ibis.literal(1000)
-    identity_func_testing(udf_ll, udfcon, test_data_db, 'int32', literal, col)
-
-
-def identity_func_testing(udf_ll, udfcon, test_data_db, datatype, literal, column):
-    inputs = [datatype]
-    name = '__tmp_udf_' + util.guid()
-    func = udf_creation_to_op(
-        udf_ll, udfcon, test_data_db, name, 'Identity', inputs, datatype
-    )
-
-    expr = func(literal)
-    assert issubclass(type(expr), ir.Scalar)
-    result = udfcon.execute(expr)
-    # Hacky
-    if datatype == 'timestamp':
-        assert type(result) == pd.Timestamp
-    else:
-        lop = literal.op()
-        if isinstance(lop, ir.Literal):
-            np.testing.assert_allclose(lop.value, 5)
-        else:
-            np.testing.assert_allclose(result, udfcon.execute(literal), 5)
-
-    expr = func(column)
-    assert issubclass(type(expr), ir.Column)
-    udfcon.execute(expr)
-
-
-@pytest.mark.xfail(
-    reason='Unknown reason. xfailing to restore the CI for udf tests. #2358'
-)
-def test_mult_type_args(udfcon, alltypes, test_data_db, udf_ll):
-    symbol = 'AlmostAllTypes'
-    name = 'most_types'
-    inputs = [
-        'string',
-        'boolean',
-        'int8',
-        'int16',
-        'int32',
-        'int64',
-        'float',
-        'double',
-    ]
-    output = 'int32'
-
-    func = udf_creation_to_op(
-        udf_ll, udfcon, test_data_db, name, symbol, inputs, output
-    )
-
-    expr = func('a', True, 1, 1, 1, 1, 1.0, 1.0)
-    result = udfcon.execute(expr)
-    assert result == 8
-
-    table = alltypes
-    expr = func(
-        table.string_col,
-        table.bool_col,
-        table.tinyint_col,
-        table.tinyint_col,
-        table.smallint_col,
-        table.smallint_col,
-        1.0,
-        1.0,
-    )
-    udfcon.execute(expr)
-
-
-@pytest.mark.xfail(
-    reason='Unknown reason. xfailing to restore the CI for udf tests. #2358'
-)
-def test_udf_varargs(udfcon, alltypes, udf_ll, test_data_db):
-    t = alltypes
-
-    name = f'add_numbers_{util.guid()[:4]}'
-
-    input_sig = rules.varargs(rules.double)
-    func = api.wrap_udf(udf_ll, input_sig, 'double', 'AddNumbers', name=name)
-    func.register(name, test_data_db)
-    udfcon.create_function(func, database=test_data_db)
-
-    expr = func(t.double_col, t.double_col)
-    expr.execute()
-
-
-def test_drop_udf_not_exists(udfcon):
-    random_name = util.guid()
-    with pytest.raises(com.MissingUDFError, match=random_name):
-        udfcon.drop_udf(random_name)
-
-
-def test_drop_uda_not_exists(udfcon):
-    random_name = util.guid()
-    with pytest.raises(com.MissingUDFError, match=random_name):
-        udfcon.drop_uda(random_name)
-
-
-def udf_creation_to_op(udf_ll, udfcon, test_data_db, name, symbol, inputs, output):
-    func = api.wrap_udf(udf_ll, inputs, output, symbol, name)
-
-    udfcon.create_function(func, database=test_data_db)
-
-    func.register(name, test_data_db)
-
-    assert udfcon.exists_udf(name, test_data_db)
-    return func
-
-
-def test_ll_uda_not_supported(uda_ll):
-    # LLVM IR UDAs are not supported as of Impala 2.2
-    with pytest.raises(com.IbisError):
-        conforming_wrapper(uda_ll, ['double'], 'double', 'Variance')
-
-
-def conforming_wrapper(where, inputs, output, prefix, serialize=True, name=None):
-    kwds = {'name': name}
-    if serialize:
-        kwds['serialize_fn'] = f'{prefix}Serialize'
-    return api.wrap_uda(
-        where,
-        inputs,
-        output,
-        f'{prefix}Update',
-        init_fn=f'{prefix}Init',
-        merge_fn=f'{prefix}Merge',
-        finalize_fn=f'{prefix}Finalize',
-        **kwds,
-    )
-
-
-@pytest.fixture
-def wrapped_count_uda(uda_so):
-    name = f'user_count_{util.guid()}'
-    return api.wrap_uda(uda_so, ['int32'], 'int64', 'CountUpdate', name=name)
-
-
-def test_count_uda(udfcon, alltypes, test_data_db, wrapped_count_uda):
-    func = wrapped_count_uda
-    func.register(func.name, test_data_db)
-    udfcon.create_function(func, database=test_data_db)
-
-    # it works!
-    func(alltypes.int_col).execute()
-
-
-def test_list_udas(udfcon, temp_database, wrapped_count_uda):
-    func = wrapped_count_uda
-    db = temp_database
-    udfcon.create_function(func, database=db)
-
-    funcs = udfcon.list_udas(database=db)
 
-    f = funcs[0]
-    assert f.name == func.name
-    assert f.inputs == func.inputs
-    assert f.output == func.output
-
-
-@pytest.mark.xfail(
-    reason='Unknown reason. xfailing to restore the CI for udf tests. #2358'
-)
-def test_drop_database_with_udfs_and_udas(udfcon, temp_database, wrapped_count_uda):
-    uda1 = wrapped_count_uda
-
-    udf1 = api.wrap_udf(
-        udf_ll,
-        ['boolean'],
-        'boolean',
-        'Identity',
-        f'udf_{util.guid()}',
-    )
-
-    db = temp_database
-
-    udfcon.create_database(db)
-
-    udfcon.create_function(uda1, database=db)
-    udfcon.create_function(udf1, database=db)
-    # drop happens in test tear down
-
-
-@pytest.fixture
-def inputs():
-    return ["string", "string"]
-
-
-@pytest.fixture
-def output():
-    return "int64"
-
-
-@pytest.fixture
-def name():
-    return "test_name"
-
-
-def test_create_udf(inputs, output, name, snapshot):
-    func = api.wrap_udf(
-        '/foo/bar.so',
-        inputs,
-        output,
-        so_symbol='testFunc',
-        name=name,
-    )
-    stmt = ddl.CreateUDF(func)
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_create_udf_type_conversions(output, name, snapshot):
-    inputs = ['string', 'int8', 'int16', 'int32']
-    func = api.wrap_udf(
-        '/foo/bar.so',
-        inputs,
-        output,
-        so_symbol='testFunc',
-        name=name,
-    )
-    stmt = ddl.CreateUDF(func)
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_delete_udf_simple(name, inputs, snapshot):
-    stmt = ddl.DropFunction(name, inputs)
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_delete_udf_if_exists(name, inputs, snapshot):
-    stmt = ddl.DropFunction(name, inputs, must_exist=False)
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_delete_udf_aggregate(name, inputs, snapshot):
-    stmt = ddl.DropFunction(name, inputs, aggregate=True)
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_delete_udf_db(name, inputs, snapshot):
-    stmt = ddl.DropFunction(name, inputs, database='test')
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-@pytest.mark.parametrize("series", [True, False])
-def test_create_uda(name, inputs, output, series, snapshot):
-    func = api.wrap_uda(
-        '/foo/bar.so',
-        inputs,
-        output,
-        update_fn='Update',
-        init_fn='Init',
-        merge_fn='Merge',
-        finalize_fn='Finalize',
-        serialize_fn='Serialize' if series else None,
-    )
-    stmt = ddl.CreateUDA(func, name=name, database='bar')
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_list_udf(snapshot):
-    stmt = ddl.ListFunction('test')
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_list_udfs_like(snapshot):
-    stmt = ddl.ListFunction('test', like='identity')
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_list_udafs(snapshot):
-    stmt = ddl.ListFunction('test', aggregate=True)
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
+import ibis.expr.types as ir
 
+from ibis.impala import ddl
+import ibis.impala as api
 
-def test_list_udafs_like(snapshot):
-    stmt = ddl.ListFunction('test', like='identity', aggregate=True)
-    result = stmt.compile()
-    snapshot.assert_match(result, "out.sql")
+from ibis.common import IbisTypeError
+from ibis.compat import unittest, Decimal
+from ibis.expr.datatypes import validate_type
+from ibis.expr.tests.mocks import MockConnection
+from ibis.impala.tests.common import ImpalaE2E
+import ibis.expr.rules as rules
+import ibis.common as com
+import ibis.util as util
+
+
+class TestWrapping(unittest.TestCase):
+
+    def setUp(self):
+        self.con = MockConnection()
+        self.table = self.con.table('functional_alltypes')
+
+        self.i8 = self.table.tinyint_col
+        self.i16 = self.table.smallint_col
+        self.i32 = self.table.int_col
+        self.i64 = self.table.bigint_col
+        self.d = self.table.double_col
+        self.f = self.table.float_col
+        self.s = self.table.string_col
+        self.b = self.table.bool_col
+        self.t = self.table.timestamp_col
+        self.dec = self.con.table('tpch_customer').c_acctbal
+        self.all_cols = [self.i8, self.i16, self.i32, self.i64, self.d,
+                         self.f, self.dec, self.s, self.b, self.t]
+
+    def test_sql_generation(self):
+        func = api.scalar_function(['string'], 'string', name='Tester')
+        func.register('identity', 'udf_testing')
+
+        result = func('hello world')
+        assert result == "SELECT udf_testing.identity('hello world')"
+
+    def test_sql_generation_from_infoclass(self):
+        func = api.wrap_udf('test.so', ['string'], 'string', 'info_test')
+        repr(func)
+
+        func.register('info_test', 'udf_testing')
+        result = func('hello world')
+        assert result == "SELECT udf_testing.info_test('hello world')"
+
+    def test_udf_primitive_output_types(self):
+        types = [
+            ('boolean', True, self.b),
+            ('int8', 1, self.i8),
+            ('int16', 1, self.i16),
+            ('int32', 1, self.i32),
+            ('int64', 1, self.i64),
+            ('float', 1.0, self.f),
+            ('double', 1.0, self.d),
+            ('string', '1', self.s),
+            ('timestamp', ibis.timestamp('1961-04-10'), self.t)
+        ]
+        for t, sv, av in types:
+            func = self._register_udf([t], t, 'test')
+
+            ibis_type = validate_type(t)
+
+            expr = func(sv)
+            assert type(expr) == ibis_type.scalar_type()
+            expr = func(av)
+            assert type(expr) == ibis_type.array_type()
+
+    def test_uda_primitive_output_types(self):
+        types = [
+            ('boolean', True, self.b),
+            ('int8', 1, self.i8),
+            ('int16', 1, self.i16),
+            ('int32', 1, self.i32),
+            ('int64', 1, self.i64),
+            ('float', 1.0, self.f),
+            ('double', 1.0, self.d),
+            ('string', '1', self.s),
+            ('timestamp', ibis.timestamp('1961-04-10'), self.t)
+        ]
+        for t, sv, av in types:
+            func = self._register_uda([t], t, 'test')
+
+            ibis_type = validate_type(t)
+
+            expr1 = func(sv)
+            expr2 = func(sv)
+            assert isinstance(expr1, ibis_type.scalar_type())
+            assert isinstance(expr2, ibis_type.scalar_type())
+
+    def test_decimal(self):
+        func = self._register_udf(['decimal(9,0)'], 'decimal(9,0)', 'test')
+        expr = func(1.0)
+        assert type(expr) == ir.DecimalScalar
+        expr = func(self.dec)
+        assert type(expr) == ir.DecimalArray
+
+    def test_udf_invalid_typecasting(self):
+        cases = [
+            ('int8', self.all_cols[:1], self.all_cols[1:]),
+            ('int16', self.all_cols[:2], self.all_cols[2:]),
+            ('int32', self.all_cols[:3], self.all_cols[3:]),
+            ('int64', self.all_cols[:4], self.all_cols[4:]),
+            ('boolean', [], self.all_cols[:8] + self.all_cols[9:]),
+
+            # allowing double here for now
+            ('float', self.all_cols[:4], [self.s, self.b, self.t, self.dec]),
+
+            ('double', self.all_cols[:4], [self.s, self.b, self.t, self.dec]),
+            ('string', [], self.all_cols[:7] + self.all_cols[8:]),
+            ('timestamp', [], self.all_cols[:-1]),
+            ('decimal', [], self.all_cols[:4] + self.all_cols[7:])
+        ]
+
+        for t, valid_casts, invalid_casts in cases:
+            func = self._register_udf([t], 'int32', 'typecast')
+
+            for expr in valid_casts:
+                func(expr)
+
+            for expr in invalid_casts:
+                self.assertRaises(IbisTypeError, func, expr)
+
+    def test_mult_args(self):
+        func = self._register_udf(['int32', 'double', 'string',
+                                   'boolean', 'timestamp'],
+                                  'int64', 'mult_types')
+
+        expr = func(self.i32, self.d, self.s, self.b, self.t)
+        assert issubclass(type(expr), ir.ArrayExpr)
+
+        expr = func(1, 1.0, 'a', True, ibis.timestamp('1961-04-10'))
+        assert issubclass(type(expr), ir.ScalarExpr)
+
+    def _register_udf(self, inputs, output, name):
+        func = api.scalar_function(inputs, output, name=name)
+        func.register(name, 'ibis_testing')
+        return func
+
+    def _register_uda(self, inputs, output, name):
+        func = api.aggregate_function(inputs, output, name=name)
+        func.register(name, 'ibis_testing')
+        return func
+
+
+class TestUDFE2E(ImpalaE2E, unittest.TestCase):
+
+    def setUp(self):
+        super(TestUDFE2E, self).setUp()
+        self.udf_ll = pjoin(self.test_data_dir, 'udf/udf-sample.ll')
+        self.uda_ll = pjoin(self.test_data_dir, 'udf/uda-sample.ll')
+        self.uda_so = pjoin(self.test_data_dir, 'udf/libudasample.so')
+
+    @pytest.mark.udf
+    def test_identity_primitive_types(self):
+        cases = [
+            ('boolean', True, self.alltypes.bool_col),
+            ('int8', 5, self.alltypes.tinyint_col),
+            ('int16', 2**10, self.alltypes.smallint_col),
+            ('int32', 2**17, self.alltypes.int_col),
+            ('int64', 2**33, self.alltypes.bigint_col),
+            ('float', 3.14, self.alltypes.float_col),
+            ('double', 3.14, self.alltypes.double_col),
+            ('string', 'ibis', self.alltypes.string_col),
+            ('timestamp', ibis.timestamp('1961-04-10'),
+             self.alltypes.timestamp_col),
+        ]
+
+        for t, lit_val, array_val in cases:
+            if not isinstance(lit_val, ir.Expr):
+                lit_val = ibis.literal(lit_val)
+            self._identity_func_testing(t, lit_val, array_val)
+
+    @pytest.mark.udf
+    def test_decimal(self):
+        col = self.con.table('tpch_customer').c_acctbal
+        literal = ibis.literal(1).cast('decimal(12,2)')
+        name = '__tmp_udf_' + util.guid()
+        func = self._udf_creation_to_op(name, 'Identity',
+                                        ['decimal(12,2)'],
+                                        'decimal(12,2)')
+
+        expr = func(literal)
+        assert issubclass(type(expr), ir.ScalarExpr)
+        result = self.con.execute(expr)
+        assert result == Decimal(1)
+
+        expr = func(col)
+        assert issubclass(type(expr), ir.ArrayExpr)
+        self.con.execute(expr)
+
+    @pytest.mark.udf
+    def test_mixed_inputs(self):
+        name = 'two_args'
+        symbol = 'TwoArgs'
+        inputs = ['int32', 'int32']
+        output = 'int32'
+        func = self._udf_creation_to_op(name, symbol, inputs, output)
+
+        expr = func(self.alltypes.int_col, 1)
+        assert issubclass(type(expr), ir.ArrayExpr)
+        self.con.execute(expr)
+
+        expr = func(1, self.alltypes.int_col)
+        assert issubclass(type(expr), ir.ArrayExpr)
+        self.con.execute(expr)
+
+        expr = func(self.alltypes.int_col, self.alltypes.tinyint_col)
+        self.con.execute(expr)
+
+    @pytest.mark.udf
+    def test_implicit_typecasting(self):
+        col = self.alltypes.tinyint_col
+        literal = ibis.literal(1000)
+        self._identity_func_testing('int32', literal, col)
+
+    def _identity_func_testing(self, datatype, literal, column):
+        inputs = [datatype]
+        name = '__tmp_udf_' + util.guid()
+        func = self._udf_creation_to_op(name, 'Identity', inputs, datatype)
+
+        expr = func(literal)
+        assert issubclass(type(expr), ir.ScalarExpr)
+        result = self.con.execute(expr)
+        # Hacky
+        if datatype is 'timestamp':
+            import pandas as pd
+            assert type(result) == pd.tslib.Timestamp
+        else:
+            lop = literal.op()
+            if isinstance(lop, ir.Literal):
+                self.assertAlmostEqual(result, lop.value, 5)
+            else:
+                self.assertAlmostEqual(result, self.con.execute(literal), 5)
+
+        expr = func(column)
+        assert issubclass(type(expr), ir.ArrayExpr)
+        self.con.execute(expr)
+
+    @pytest.mark.udf
+    def test_mult_type_args(self):
+        symbol = 'AlmostAllTypes'
+        name = 'most_types'
+        inputs = ['string', 'boolean', 'int8', 'int16', 'int32',
+                  'int64', 'float', 'double']
+        output = 'int32'
+
+        func = self._udf_creation_to_op(name, symbol, inputs, output)
+
+        expr = func('a', True, 1, 1, 1, 1, 1.0, 1.0)
+        result = self.con.execute(expr)
+        assert result == 8
+
+        table = self.alltypes
+        expr = func(table.string_col, table.bool_col, table.tinyint_col,
+                    table.tinyint_col, table.smallint_col,
+                    table.smallint_col, 1.0, 1.0)
+        self.con.execute(expr)
+
+    def test_all_type_args(self):
+        pytest.skip('failing test, to be fixed later')
+
+        symbol = 'AllTypes'
+        name = 'all_types'
+        inputs = ['string', 'boolean', 'int8', 'int16', 'int32',
+                  'int64', 'float', 'double', 'decimal']
+        output = 'int32'
+
+        func = self._udf_creation_to_op(name, symbol, inputs, output)
+        expr = func('a', True, 1, 1, 1, 1, 1.0, 1.0, 1.0)
+        result = self.con.execute(expr)
+        assert result == 9
+
+    @pytest.mark.udf
+    def test_udf_varargs(self):
+        t = self.alltypes
+
+        name = 'add_numbers_{0}'.format(util.guid()[:4])
+
+        input_sig = rules.varargs(rules.double)
+        func = api.wrap_udf(self.udf_ll, input_sig, 'double', 'AddNumbers',
+                            name=name)
+        func.register(name, self.test_data_db)
+        self.con.create_function(func, database=self.test_data_db)
+
+        expr = func(t.double_col, t.double_col)
+        expr.execute()
+
+    def test_drop_udf_not_exists(self):
+        random_name = util.guid()
+        self.assertRaises(Exception, self.con.drop_udf, random_name)
+
+    def test_drop_uda_not_exists(self):
+        random_name = util.guid()
+        self.assertRaises(Exception, self.con.drop_uda, random_name)
+
+    def _udf_creation_to_op(self, name, symbol, inputs, output):
+        func = api.wrap_udf(self.udf_ll, inputs, output, symbol, name)
+
+        self.temp_udfs.append((name, inputs))
+
+        self.con.create_function(func, database=self.test_data_db)
+
+        func.register(name, self.test_data_db)
+
+        assert self.con.exists_udf(name, self.test_data_db)
+        return func
+
+    def test_ll_uda_not_supported(self):
+        # LLVM IR UDAs are not supported as of Impala 2.2
+        with self.assertRaises(com.IbisError):
+            self._conforming_wrapper(self.uda_ll, ['double'], 'double',
+                                     'Variance')
+
+    def _conforming_wrapper(self, where, inputs, output, prefix,
+                            serialize=True, name=None):
+        kwds = {
+            'name': name
+        }
+        if serialize:
+            kwds['serialize_fn'] = '{0}Serialize'.format(prefix)
+        return api.wrap_uda(where, inputs, output, '{0}Update'.format(prefix),
+                            init_fn='{0}Init'.format(prefix),
+                            merge_fn='{0}Merge'.format(prefix),
+                            finalize_fn='{0}Finalize'.format(prefix),
+                            **kwds)
+
+    @pytest.mark.udf
+    def test_count_uda(self):
+        func = self._wrap_count_uda()
+        func.register(func.name, self.test_data_db)
+        self.con.create_function(func, database=self.test_data_db)
+
+        # it works!
+        func(self.alltypes.int_col).execute()
+        self.temp_udas.append((func.name, ['int32']))
+
+    @pytest.mark.udf
+    def test_list_udas(self):
+        db = '__ibis_tmp_{0}'.format(util.guid())
+        self.con.create_database(db)
+        self.temp_databases.append(db)
+
+        func = self._wrap_count_uda()
+        self.con.create_function(func, database=db)
+
+        funcs = self.con.list_udas(database=db)
+
+        f = funcs[0]
+        assert f.name == func.name
+        assert f.inputs == func.inputs
+        assert f.output == func.output
+
+    @pytest.mark.udf
+    def test_drop_database_with_udfs_and_udas(self):
+        uda1 = self._wrap_count_uda()
+        uda2 = self._wrap_count_uda()
+
+        udf1 = api.wrap_udf(self.udf_ll, ['boolean'], 'boolean', 'Identity',
+                            'udf_{0}'.format(util.guid()))
+
+        db = '__ibis_tmp_{0}'.format(util.guid())
+
+        self.con.create_database(db)
+
+        self.con.create_function(uda1, database=db)
+        self.con.create_function(uda2, database=db)
+
+        self.con.create_function(udf1, database=db)
+
+        self.con.drop_database(db, force=True)
+
+        assert not self.con.exists_database(db)
+
+    def _wrap_count_uda(self, name=None):
+        if name is None:
+            name = 'user_count_{0}'.format(util.guid())
+        func = api.wrap_uda(self.uda_so, ['int32'], 'int64',
+                            'CountUpdate', name=name)
+        return func
+
+
+class TestUDFDDL(unittest.TestCase):
+
+    def setUp(self):
+        self.con = MockConnection()
+        self.name = 'test_name'
+        self.inputs = ['string', 'string']
+        self.output = 'int64'
+
+    def test_create_udf(self):
+        stmt = ddl.CreateFunction('/foo/bar.so', 'testFunc', self.inputs,
+                                  self.output, self.name)
+        result = stmt.compile()
+        expected = ("CREATE FUNCTION `test_name`(string, string) "
+                    "returns bigint "
+                    "location '/foo/bar.so' symbol='testFunc'")
+        assert result == expected
+
+    def test_create_udf_type_conversions(self):
+        stmt = ddl.CreateFunction('/foo/bar.so', 'testFunc',
+                                  ['string', 'int8', 'int16', 'int32'],
+                                  self.output, self.name)
+        result = stmt.compile()
+        expected = ("CREATE FUNCTION `test_name`(string, tinyint, "
+                    "smallint, int) returns bigint "
+                    "location '/foo/bar.so' symbol='testFunc'")
+        assert result == expected
+
+    def test_delete_udf_simple(self):
+        stmt = ddl.DropFunction(self.name, self.inputs)
+        result = stmt.compile()
+        expected = "DROP FUNCTION `test_name`(string, string)"
+        assert result == expected
+
+    def test_delete_udf_if_exists(self):
+        stmt = ddl.DropFunction(self.name, self.inputs, must_exist=False)
+        result = stmt.compile()
+        expected = "DROP FUNCTION IF EXISTS `test_name`(string, string)"
+        assert result == expected
+
+    def test_delete_udf_aggregate(self):
+        stmt = ddl.DropFunction(self.name, self.inputs, aggregate=True)
+        result = stmt.compile()
+        expected = "DROP AGGREGATE FUNCTION `test_name`(string, string)"
+        assert result == expected
+
+    def test_delete_udf_db(self):
+        stmt = ddl.DropFunction(self.name, self.inputs, database='test')
+        result = stmt.compile()
+        expected = "DROP FUNCTION test.`test_name`(string, string)"
+        assert result == expected
+
+    def test_create_uda(self):
+        def make_ex(serialize=False):
+            if serialize:
+                serialize = "\nserialize_fn='Serialize'"
+            else:
+                serialize = ""
+            return (("CREATE AGGREGATE FUNCTION "
+                     "bar.`test_name`(string, string)"
+                     " returns bigint location '/foo/bar.so'"
+                     "\ninit_fn='Init'"
+                     "\nupdate_fn='Update'"
+                     "\nmerge_fn='Merge'") +
+                    serialize +
+                    ("\nfinalize_fn='Finalize'"))
+
+        for ser in [True, False]:
+            stmt = ddl.CreateAggregateFunction('/foo/bar.so', self.inputs,
+                                               self.output, 'Update', 'Init',
+                                               'Merge',
+                                               'Serialize' if ser else None,
+                                               'Finalize', self.name, 'bar')
+            result = stmt.compile()
+            expected = make_ex(ser)
+            assert result == expected
+
+    def test_list_udf(self):
+        stmt = ddl.ListFunction('test')
+        result = stmt.compile()
+        expected = 'SHOW FUNCTIONS IN test'
+        assert result == expected
+
+    def test_list_udfs_like(self):
+        stmt = ddl.ListFunction('test', like='identity')
+        result = stmt.compile()
+        expected = "SHOW FUNCTIONS IN test LIKE 'identity'"
+        assert result == expected
+
+    def test_list_udafs(self):
+        stmt = ddl.ListFunction('test', aggregate=True)
+        result = stmt.compile()
+        expected = 'SHOW AGGREGATE FUNCTIONS IN test'
+        assert result == expected
+
+    def test_list_udafs_like(self):
+        stmt = ddl.ListFunction('test', like='identity', aggregate=True)
+        result = stmt.compile()
+        expected = "SHOW AGGREGATE FUNCTIONS IN test LIKE 'identity'"
+        assert result == expected
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/impala/udf.py` & `ibis-framework-v0.6.0/ibis/impala/udf.py`

 * *Files 11% similar despite different names*

```diff
@@ -7,84 +7,99 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from __future__ import annotations
+from ibis.expr.datatypes import validate_type
+import ibis.expr.datatypes as _dt
+import ibis.expr.operations as _ops
+import ibis.expr.rules as rules
+import ibis.impala.compiler as comp
+import ibis.common as com
+import ibis.util as util
 
-import abc
-import re
 
-import ibis.common.exceptions as com
-import ibis.expr.datatypes as dt
-import ibis.expr.operations as ops
-import ibis.expr.rules as rlz
-import ibis.legacy.udf.validate as v
-from ibis import util
-from ibis.backends.base.sql.registry import fixed_arity, sql_type_names
-from ibis.backends.impala.compiler import ImpalaExprTranslator
-
-__all__ = [
-    'add_operation',
-    'scalar_function',
-    'aggregate_function',
-    'wrap_udf',
-    'wrap_uda',
-]
+__all__ = ['add_operation', 'scalar_function', 'aggregate_function',
+           'wrap_udf', 'wrap_uda']
 
 
-class Function(metaclass=abc.ABCMeta):
-    def __init__(self, inputs, output, name):
-        self.inputs = tuple(map(dt.dtype, inputs))
-        self.output = dt.dtype(output)
-        self.name = name or util.guid()
-        self._klass = self._create_operation_class()
+class Function(object):
 
-    @abc.abstractmethod
-    def _create_operation_class(self):
-        pass
+    def __init__(self, inputs, output, name):
+        self.inputs = inputs
+        self.output = output
+        self.name = name
+
+        (self.input_type,
+         self.output_type) = self._type_signature(inputs, output)
+        self._klass = self._create_operation(name)
+
+    def _create_operation(self, name):
+        class_name = self._get_class_name(name)
+        return _create_operation_class(class_name, self.input_type,
+                                       self.output_type)
 
     def __repr__(self):
         klass = type(self).__name__
-        return f'{klass}({self.name}, {self.inputs!r}, {self.output!r})'
+        return ('{0}({1}, {2!r}, {3!r})'
+                .format(klass, self.name, self.inputs, self.output))
 
     def __call__(self, *args):
         return self._klass(*args).to_expr()
 
-    def register(self, name: str, database: str) -> None:
-        """Register the given operation.
+    def register(self, name, database):
+        """
+        Registers the given operation within the Ibis SQL translation
+        toolchain. Can also use add_operation API
 
         Parameters
         ----------
-        name
-            Used in issuing statements to SQL engine
-        database
-            Database the relevant operator is registered to
+        name: used in issuing statements to SQL engine
+        database: database the relevant operator is registered to
         """
         add_operation(self._klass, name, database)
 
 
 class ScalarFunction(Function):
-    def _create_operation_class(self):
-        fields = {f'_{i}': rlz.value(dtype) for i, dtype in enumerate(self.inputs)}
-        fields['output_dtype'] = self.output
-        fields['output_shape'] = rlz.shape_like('args')
-        return type(f"UDF_{self.name}", (ops.Value,), fields)
+
+    def _get_class_name(self, name):
+        if name is None:
+            name = util.guid()
+        return 'UDF_{0}'.format(name)
+
+    def _type_signature(self, inputs, output):
+        input_type = _to_input_sig(inputs)
+        output = validate_type(output)
+        output_type = rules.shape_like_flatargs(output)
+        return input_type, output_type
 
 
 class AggregateFunction(Function):
-    def _create_operation_class(self):
-        fields = {f'_{i}': rlz.value(dtype) for i, dtype in enumerate(self.inputs)}
-        fields['output_dtype'] = self.output
-        return type(f"UDA_{self.name}", (ops.Reduction,), fields)
+
+    def _create_operation(self, name):
+        klass = Function._create_operation(self, name)
+        klass._reduction = True
+        return klass
+
+    def _get_class_name(self, name):
+        if name is None:
+            name = util.guid()
+        return 'UDA_{0}'.format(name)
+
+    def _type_signature(self, inputs, output):
+        input_type = _to_input_sig(inputs)
+        output = validate_type(output)
+        output_type = rules.scalar_output(output)
+        return input_type, output_type
 
 
-class ImpalaFunction:
+class ImpalaFunction(object):
+
     def __init__(self, name=None, lib_path=None):
         self.lib_path = lib_path
         self.name = name or util.guid()
 
         if lib_path is not None:
             self._check_library()
 
@@ -94,18 +109,19 @@
             raise ValueError('Invalid file type. Must be .so or .ll ')
 
     def hash(self):
         raise NotImplementedError
 
 
 class ImpalaUDF(ScalarFunction, ImpalaFunction):
-    """Feel free to customize my __doc__ or wrap in a nicer user API."""
-
-    def __init__(self, inputs, output, so_symbol=None, lib_path=None, name=None):
-        v.validate_output_type(output)
+    """
+    Feel free to customize my __doc__ or wrap in a nicer user API
+    """
+    def __init__(self, inputs, output, so_symbol=None, lib_path=None,
+                 name=None):
         self.so_symbol = so_symbol
         ImpalaFunction.__init__(self, name=name, lib_path=lib_path)
         ScalarFunction.__init__(self, inputs, output, name=self.name)
 
     def hash(self):
         # TODO: revisit this later
         # from hashlib import sha1
@@ -114,222 +130,196 @@
         #     val += in_type.name()
 
         # return sha1(val).hexdigest()
         pass
 
 
 class ImpalaUDA(AggregateFunction, ImpalaFunction):
-    def __init__(
-        self,
-        inputs,
-        output,
-        update_fn=None,
-        init_fn=None,
-        merge_fn=None,
-        finalize_fn=None,
-        serialize_fn=None,
-        lib_path=None,
-        name=None,
-    ):
+
+    def __init__(self, inputs, output, update_fn=None, init_fn=None,
+                 merge_fn=None, finalize_fn=None, serialize_fn=None,
+                 lib_path=None, name=None):
         self.init_fn = init_fn
         self.update_fn = update_fn
         self.merge_fn = merge_fn
         self.finalize_fn = finalize_fn
         self.serialize_fn = serialize_fn
 
-        v.validate_output_type(output)
-
         ImpalaFunction.__init__(self, name=name, lib_path=lib_path)
         AggregateFunction.__init__(self, inputs, output, name=self.name)
 
     def _check_library(self):
         suffix = self.lib_path[-3:]
         if suffix == '.ll':
             raise com.IbisInputError('LLVM IR UDAs are not yet supported')
         elif suffix != '.so':
             raise ValueError('Invalid file type. Must be .so')
 
 
-def wrap_uda(
-    hdfs_file: str,
-    inputs: str,
-    output: str,
-    update_fn: str,
-    init_fn: str | None = None,
-    merge_fn: str | None = None,
-    finalize_fn: str | None = None,
-    serialize_fn: str | None = None,
-    name: str | None = None,
-):
-    """Creates a callable aggregation function object.
-
-    Must be created in Impala to be used.
+def wrap_uda(hdfs_file, inputs, output, update_fn, init_fn=None,
+             merge_fn=None, finalize_fn=None, serialize_fn=None,
+             close_fn=None, name=None):
+    """
+    Creates a callable aggregation function object. Must be created in Impala
+    to be used
 
     Parameters
     ----------
-    hdfs_file
-        .so file that contains relevant UDA
-    inputs
-        list of strings denoting ibis datatypes
-    output
-        string denoting ibis datatype
-    update_fn
-        Library symbol name for update function
-    init_fn
-        Library symbol name for initialization function
-    merge_fn
-        Library symbol name for merge function
-    finalize_fn
-        Library symbol name for finalize function
-    serialize_fn
-        Library symbol name for serialize UDA API function. Not required for all
-        UDAs.
-    name
-        Used internally to track function
+    hdfs_file: .so file that contains relevant UDA
+    inputs: list of strings denoting ibis datatypes
+    output: string denoting ibis datatype
+    update_fn: string
+      Library symbol name for update function
+    init_fn: string, optional
+      Library symbol name for initialization function
+    merge_fn: string, optional
+      Library symbol name for merge function
+    finalize_fn: string, optional
+      Library symbol name for finalize function
+    serialize_fn : string, optional
+      Library symbol name for serialize UDA API function. Not required for all
+      UDAs; see documentation for more.
+    close_fn : string, optional
+    name: string, optional
+      Used internally to track function
 
     Returns
     -------
     container : UDA object
     """
-    return ImpalaUDA(
-        inputs,
-        output,
-        update_fn,
-        init_fn,
-        merge_fn,
-        finalize_fn,
-        serialize_fn=serialize_fn,
-        name=name,
-        lib_path=hdfs_file,
-    )
+    func = ImpalaUDA(inputs, output, update_fn, init_fn,
+                     merge_fn, finalize_fn,
+                     serialize_fn=serialize_fn,
+                     name=name, lib_path=hdfs_file)
+    return func
 
 
 def wrap_udf(hdfs_file, inputs, output, so_symbol, name=None):
-    """Creates a callable scalar function object.
-
-    Must be created in Impala to be used.
+    """
+    Creates a callable scalar function object. Must be created in Impala to be
+    used
 
     Parameters
     ----------
-    hdfs_file
-        .so file that contains relevant UDF
-    inputs
-        Input types to UDF
-    output
-        Ibis data type
-    so_symbol
-        C++ function name for relevant UDF
-    name
-        Used internally to track function
+    hdfs_file: .so file that contains relevant UDF
+    inputs: list of strings or TypeSignature
+      Input types to UDF
+    output: string
+      Ibis data type
+    so_symbol: string, C++ function name for relevant UDF
+    name: string (optional). Used internally to track function
+
+    Returns
+    -------
+    container : UDF object
     """
-    func = ImpalaUDF(inputs, output, so_symbol, name=name, lib_path=hdfs_file)
+    func = ImpalaUDF(inputs, output, so_symbol, name=name,
+                     lib_path=hdfs_file)
     return func
 
 
 def scalar_function(inputs, output, name=None):
-    """Creates an operator class that can be passed to add_operation().
+    """
+    Creates an operator class that can be passed to add_operation()
 
-    Parameters
-    ----------
-    inputs
-        Ibis data type names
-    output
-        Ibis data type
-    name
-        Used internally to track function
+    Parameters:
+    inputs: list of strings
+      Ibis data type names
+    output: string
+      Ibis data type
+    name: string, optional
+      Used internally to track function
+
+    Returns
+    -------
+    klass, user_api : class, function
     """
     return ScalarFunction(inputs, output, name=name)
 
 
 def aggregate_function(inputs, output, name=None):
-    """Creates an operator class that can be passed to add_operation().
+    """
+    Creates an operator class that can be passed to add_operation()
 
-    Parameters
-    ----------
+    Parameters:
     inputs: list of strings
       Ibis data type names
     output: string
       Ibis data type
     name: string, optional
         Used internally to track function
+
+    Returns
+    -------
+    klass, user_api : class, function
     """
     return AggregateFunction(inputs, output, name=name)
 
 
+def _to_input_sig(inputs):
+    if isinstance(inputs, rules.TypeSignature):
+        return inputs
+    else:
+        in_type = [validate_type(x) for x in inputs]
+        return rules.TypeSignature([rules.value_typed_as(x)
+                                    for x in in_type])
+
+
+def _create_operation_class(name, input_type, output_type):
+    func_dict = {
+        'input_type': input_type,
+        'output_type': output_type,
+    }
+    klass = type(name, (_ops.ValueOp,), func_dict)
+    return klass
+
+
 def add_operation(op, func_name, db):
-    """Registers the given operation within the Ibis SQL translation toolchain.
+    """
+    Registers the given operation within the Ibis SQL translation toolchain
 
     Parameters
     ----------
-    op
-        operator class
-    func_name
-        used in issuing statements to SQL engine
-    db
-        database the relevant operator is registered to
-    """
-    full_name = f'{db}.{func_name}'
-    # TODO
-    # if op.input_type is rlz.listof:
-    #     translator = comp.varargs(full_name)
-    # else:
-    arity = len(op.__signature__.parameters)
-    translator = fixed_arity(full_name, arity)
-
-    ImpalaExprTranslator._registry[op] = translator
-
-
-def parse_type(t):
-    t = t.lower()
-    if t in _impala_to_ibis_type:
-        return _impala_to_ibis_type[t]
-    elif 'varchar' in t or 'char' in t:
-        return 'string'
-    elif 'decimal' in t:
-        result = dt.dtype(t)
-        if result:
-            return t
-        else:
-            return ValueError(t)
+    op: operator class
+    name: used in issuing statements to SQL engine
+    database: database the relevant operator is registered to
+    """
+    full_name = '{0}.{1}'.format(db, func_name)
+    if isinstance(op.input_type, rules.VarArgs):
+        translator = comp.varargs(full_name)
     else:
-        raise Exception(t)
-
+        arity = len(op.input_type.types)
+        translator = comp.fixed_arity(full_name, arity)
 
-_VARCHAR_RE = re.compile(r'varchar\((\d+)\)')
-
-
-def _parse_varchar(t):
-    m = _VARCHAR_RE.match(t)
-    if m:
-        return 'string'
-    return None
+    comp._operation_registry[op] = translator
 
 
 def _impala_type_to_ibis(tval):
     if tval in _impala_to_ibis_type:
         return _impala_to_ibis_type[tval]
     return tval
 
 
 def _ibis_string_to_impala(tval):
-    if tval in sql_type_names:
-        return sql_type_names[tval]
-    result = dt.validate_type(tval)
-    return repr(result) if result else None
+    from ibis.impala.compiler import _sql_type_names
+
+    if tval in _sql_type_names:
+        return _sql_type_names[tval]
+    result = _dt._parse_decimal(tval)
+    if result:
+        return repr(result)
 
 
 _impala_to_ibis_type = {
     'boolean': 'boolean',
     'tinyint': 'int8',
     'smallint': 'int16',
     'int': 'int32',
     'bigint': 'int64',
-    'float': 'float32',
-    'double': 'float64',
+    'float': 'float',
+    'double': 'double',
     'string': 'string',
     'varchar': 'string',
     'char': 'string',
     'timestamp': 'timestamp',
-    'decimal': 'decimal',
-    'date': 'date',
-    'void': 'null',
+    'decimal': 'decimal'
 }
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/backends/sqlite/tests/test_functions.py` & `ibis-framework-v0.6.0/ibis/sql/tests/test_sqlalchemy.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,715 +1,580 @@
-from __future__ import annotations
+# Copyright 2015 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import operator
+
+from ibis.compat import unittest
+from ibis.expr.tests.mocks import MockConnection
+from ibis.sql.tests.test_compiler import ExprTestCases
+from ibis.tests.util import assert_equal
+import ibis.expr.datatypes as dt
+import ibis.expr.types as ir
+import ibis.sql.alchemy as alch
+import ibis
 
-import math
-import sqlite3
-import uuid
-
-import numpy as np
-import pandas as pd
-import pandas.testing as tm
-import pytest
-from packaging.version import parse
-from pytest import param
+from sqlalchemy import types as sat, func as F
+import sqlalchemy.sql as sql
+import sqlalchemy as sa
 
-import ibis
-import ibis.expr.datatypes as dt
-from ibis import config
-from ibis import literal as L
+L = sa.literal
+
+# SQL engine-independent unit tests
+
+
+class MockAlchemyConnection(MockConnection):
+
+    def __init__(self):
+        self.meta = sa.MetaData()
+        MockConnection.__init__(self)
+
+    def table(self, name):
+        schema = self._get_table_schema(name)
+        return self._inject_table(name, schema)
+
+    def _inject_table(self, name, schema):
+        if name in self.meta.tables:
+            table = self.meta.tables[name]
+        else:
+            table = alch.table_from_schema(name, self.meta, schema)
+
+        node = alch.AlchemyTable(table, self)
+        return ir.TableExpr(node)
+
+
+def _table_wrapper(name, tname=None):
+    @property
+    def f(self):
+        t = self._table_from_schema(name, tname)
+        return t
+    return f
+
+
+class TestSQLAlchemySelect(unittest.TestCase, ExprTestCases):
+
+    def setUp(self):
+        self.con = MockAlchemyConnection()
+        self.alltypes = self.con.table('functional_alltypes')
+        self.sa_alltypes = self.con.meta.tables['functional_alltypes']
+        self.meta = sa.MetaData()
+
+        self.sa_star1 = self._get_sqla('star1')
+
+    foo = _table_wrapper('foo')
+    bar = _table_wrapper('bar')
+    t1 = _table_wrapper('t1', 'foo')
+    t2 = _table_wrapper('t2', 'bar')
+
+    def _table_from_schema(self, name, tname=None):
+        tname = tname or name
+        schema = ibis.schema(self._schemas[name])
+        return self.con._inject_table(tname, schema)
+
+    def _get_sqla(self, name):
+        return self._to_sqla(self.con.table(name))
+
+    def _check_expr_cases(self, cases, named=False):
+        for expr, expected in cases:
+            result = self._translate(expr, named=named)
+            assert str(result) == str(expected)
+            if named:
+                assert result.name == expected.name
+
+    def _translate(self, expr, named=False, context=None):
+        translator = alch.AlchemyExprTranslator(expr, context=context,
+                                                named=named)
+        return translator.get_result()
+
+    def test_sqla_schema_conversion(self):
+        typespec = [
+            # name, type, nullable
+            ('smallint', sat.SmallInteger, False, dt.int16),
+            ('int', sat.Integer, True, dt.int32),
+            ('integer', sat.INTEGER(), True, dt.int64),
+            ('bigint', sat.BigInteger, False, dt.int64),
+            ('real', sat.REAL, True, dt.double),
+            ('bool', sat.Boolean, True, dt.boolean),
+            ('timestamp', sat.DateTime, True, dt.timestamp),
+        ]
+
+        sqla_types = []
+        ibis_types = []
+        for name, t, nullable, ibis_type in typespec:
+            sqla_type = sa.Column(name, t, nullable=nullable)
+            sqla_types.append(sqla_type)
+            ibis_types.append((name, ibis_type(nullable)))
+
+        table = sa.Table('tname', self.meta, *sqla_types)
+
+        schema = alch.schema_from_table(table)
+        expected = ibis.schema(ibis_types)
+
+        assert_equal(schema, expected)
+
+    def test_ibis_to_sqla_conversion(self):
+        pass
+
+    def test_comparisons(self):
+        sat = self.sa_alltypes
+
+        ops = ['ge', 'gt', 'lt', 'le', 'eq', 'ne']
+
+        cases = []
+
+        for op in ops:
+            f = getattr(operator, op)
+            case = f(self.alltypes.double_col, 5), f(sat.c.double_col, L(5))
+            cases.append(case)
+
+        self._check_expr_cases(cases)
+
+    def test_boolean_conjunction(self):
+        sat = self.sa_alltypes
+        sd = sat.c.double_col
+
+        d = self.alltypes.double_col
+        cases = [
+            ((d > 0) & (d < 5), sql.and_(sd > L(0), sd < L(5))),
+            ((d < 0) | (d > 5), sql.or_(sd < L(0), sd > L(5)))
+        ]
+
+        self._check_expr_cases(cases)
+
+    def test_between(self):
+        sat = self.sa_alltypes
+        sd = sat.c.double_col
+        d = self.alltypes.double_col
+
+        cases = [
+            (d.between(5, 10), sd.between(L(5), L(10))),
+        ]
+        self._check_expr_cases(cases)
+
+    def test_isnull_notnull(self):
+        sat = self.sa_alltypes
+        sd = sat.c.double_col
+        d = self.alltypes.double_col
+
+        cases = [
+            (d.isnull(), sd.is_(sa.null())),
+            (d.notnull(), sd.isnot(sa.null())),
+        ]
+        self._check_expr_cases(cases)
+
+    def test_negate(self):
+        sat = self.sa_alltypes
+        sd = sat.c.double_col
+        d = self.alltypes.double_col
+        cases = [
+            (-(d > 0), sql.not_(sd > L(0)))
+        ]
+
+        self._check_expr_cases(cases)
+
+    def test_coalesce(self):
+        sat = self.sa_alltypes
+        sd = sat.c.double_col
+        sf = sat.c.float_col
+
+        d = self.alltypes.double_col
+        f = self.alltypes.float_col
+        null = sa.null()
+
+        v1 = ibis.NA
+        v2 = (d > 30).ifelse(d, ibis.NA)
+        v3 = f
+
+        cases = [
+            (ibis.coalesce(v2, v1, v3),
+             sa.func.coalesce(sa.case([(sd > L(30), sd)], else_=null),
+                              null, sf))
+        ]
+        self._check_expr_cases(cases)
+
+    def test_named_expr(self):
+        sat = self.sa_alltypes
+        d = self.alltypes.double_col
+
+        cases = [
+            ((d * 2).name('foo'), (sat.c.double_col * L(2)).label('foo'))
+        ]
+        self._check_expr_cases(cases, named=True)
+
+    def test_joins(self):
+        region = self.con.table('tpch_region')
+        nation = self.con.table('tpch_nation')
+
+        rt = self._to_sqla(region).alias('t0')
+        nt = self._to_sqla(nation).alias('t1')
+
+        ipred = region.r_regionkey == nation.n_regionkey
+        spred = rt.c.r_regionkey == nt.c.n_regionkey
+
+        fully_mat_joins = [
+            (region.inner_join(nation, ipred),
+             rt.join(nt, spred)),
+
+            (region.left_join(nation, ipred),
+             rt.join(nt, spred, isouter=True)),
+
+            (region.outer_join(nation, ipred),
+             rt.outerjoin(nt, spred)),
+        ]
+        for ibis_joined, joined_sqla in fully_mat_joins:
+            expected = sa.select(['*']).select_from(joined_sqla)
+            self._compare_sqla(ibis_joined, expected)
+
+        subselect_joins = [
+            (region.inner_join(nation, ipred).projection(nation),
+             rt.join(nt, spred)),
+
+            (region.left_join(nation, ipred).projection(nation),
+             rt.join(nt, spred, isouter=True)),
+
+            (region.outer_join(nation, ipred).projection(nation),
+             rt.outerjoin(nt, spred)),
+        ]
+        for ibis_joined, joined_sqla in subselect_joins:
+            expected = sa.select([nt]).select_from(joined_sqla)
+            self._compare_sqla(ibis_joined, expected)
+
+    def test_join_just_materialized(self):
+        joined = self._case_join_just_materialized()
+
+        rt, nt, ct = self._sqla_tables(['tpch_region', 'tpch_nation',
+                                        'tpch_customer'])
+        nt = nt.alias('t0')
+        rt = rt.alias('t1')
+        ct = ct.alias('t2')
+
+        sqla_joined = (nt.join(rt, nt.c.n_regionkey == rt.c.r_regionkey)
+                       .join(ct, nt.c.n_nationkey == ct.c.c_nationkey))
+
+        expected = sa.select(['*']).select_from(sqla_joined)
+
+        self._compare_sqla(joined, expected)
+
+    def _sqla_tables(self, tables):
+        result = []
+        for t in tables:
+            ibis_table = self.con.table(t)
+            result.append(self._to_sqla(ibis_table))
+        return result
+
+    def test_simple_case(self):
+        self.con.table('alltypes')
+        st = self.con.meta.tables['alltypes']
+
+        expr = self._case_simple_case()
+
+        cases = [
+            (expr, sa.case([(st.c.g == L('foo'), L('bar')),
+                            (st.c.g == L('baz'), L('qux'))],
+                           else_='default')),
+        ]
+        self._check_expr_cases(cases)
+
+    def test_searched_case(self):
+        self.con.table('alltypes')
+        st = self.con.meta.tables['alltypes']
+
+        expr = self._case_search_case()
+        cases = [
+            (expr, sa.case([(st.c.f > L(0), st.c.d * L(2)),
+                            (st.c.c < L(0), st.c.a * L(2))],
+                           else_=sa.null())),
+        ]
+        self._check_expr_cases(cases)
+
+    def test_where_simple_comparisons(self):
+        expr = self._case_where_simple_comparisons()
+
+        st = self.sa_star1.alias('t0')
+
+        clause = sql.and_(st.c.f > L(0), st.c.c < (st.c.f * L(2)))
+        expected = sa.select([st]).where(clause)
+
+        self._compare_sqla(expr, expected)
+
+    def test_simple_aggregate_query(self):
+        st = self.sa_star1.alias('t0')
+
+        cases = self._case_simple_aggregate_query()
+
+        metric = F.sum(st.c.f).label('total')
+        k1 = st.c.foo_id
+        k2 = st.c.bar_id
+        expected = [
+            sa.select([k1, metric]).group_by(k1),
+            sa.select([k1, k2, metric]).group_by(k1, k2)
+        ]
+
+        for case, ex_sqla in zip(cases, expected):
+            self._compare_sqla(case, ex_sqla)
+
+    def test_aggregate_having(self):
+        st = self.sa_star1.alias('t0')
+
+        cases = self._case_aggregate_having()
+
+        metric = F.sum(st.c.f)
+        k1 = st.c.foo_id
+        expected = [
+            sa.select([k1, metric.label('total')]).group_by(k1)
+            .having(metric > L(10)),
+            sa.select([k1, metric.label('total')]).group_by(k1)
+            .having(F.count('*') > L(100))
+        ]
+
+        for case, ex_sqla in zip(cases, expected):
+            self._compare_sqla(case, ex_sqla)
+
+    def test_sort_by(self):
+        st = self.sa_star1.alias('t0')
+        cases = self._case_sort_by()
+
+        base = sa.select([st])
+        expected = [
+            base.order_by(st.c.f),
+            base.order_by(st.c.f.desc()),
+            base.order_by(st.c.c, st.c.f.desc()),
+        ]
+        for case, ex_sqla in zip(cases, expected):
+            self._compare_sqla(case, ex_sqla)
+
+    def test_limit(self):
+        cases = self._case_limit()
+
+        st = self.sa_star1.alias('t0')
+        base = sa.select([st])
+
+        expected = [
+            base.limit(10),
+            base.limit(10).offset(5),
+            base.where(st.c.f > L(0)).limit(10),
+        ]
+
+        st = self.sa_star1.alias('t1')
+        base = sa.select([st])
+        aliased = base.limit(10).alias('t0')
+        case4 = sa.select([aliased]).where(aliased.c.f > L(0))
+        expected.append(case4)
+
+        for case, ex in zip(cases, expected):
+            self._compare_sqla(case, ex)
+
+    def test_cte_factor_distinct_but_equal(self):
+        expr = self._case_cte_factor_distinct_but_equal()
+
+        alltypes = self._get_sqla('alltypes')
+
+        t2 = alltypes.alias('t2')
+        t0 = (sa.select([t2.c.g, F.sum(t2.c.f).label('metric')])
+              .group_by(t2.c.g)
+              .cte('t0'))
+
+        t1 = t0.alias('t1')
+        table_set = t0.join(t1, t0.c.g == t1.c.g)
+        stmt = sa.select([t0]).select_from(table_set)
+
+        self._compare_sqla(expr, stmt)
+
+    def test_self_reference_join(self):
+        t0 = self.sa_star1.alias('t0')
+        t1 = self.sa_star1.alias('t1')
+
+        case = self._case_self_reference_join()
+
+        table_set = t0.join(t1, t0.c.foo_id == t1.c.bar_id)
+        expected = sa.select([t0]).select_from(table_set)
+        self._compare_sqla(case, expected)
+
+    def test_self_reference_in_not_exists(self):
+        semi, anti = self._case_self_reference_in_exists()
+
+        s1 = self.sa_alltypes.alias('t0')
+        s2 = self.sa_alltypes.alias('t1')
+
+        cond = (sa.exists([L(1)]).select_from(s1)
+                .where(s1.c.string_col == s2.c.string_col))
+
+        ex_semi = sa.select([s1]).where(cond)
+        ex_anti = sa.select([s1]).where(~cond)
+
+        self._compare_sqla(semi, ex_semi)
+        self._compare_sqla(anti, ex_anti)
+
+    def test_where_uncorrelated_subquery(self):
+        expr = self._case_where_uncorrelated_subquery()
+
+        foo = self._to_sqla(self.foo).alias('t0')
+        bar = self._to_sqla(self.bar)
+
+        subq = sa.select([bar.c.job])
+        stmt = sa.select([foo]).where(foo.c.job.in_(subq))
+        self._compare_sqla(expr, stmt)
+
+    def test_where_correlated_subquery(self):
+        expr = self._case_where_correlated_subquery()
+
+        foo = self._to_sqla(self.foo)
+        t0 = foo.alias('t0')
+        t1 = foo.alias('t1')
+        subq = (sa.select([F.avg(t1.c.y).label('mean')])
+                .where(t0.c.dept_id == t1.c.dept_id))
+        stmt = sa.select([t0]).where(t0.c.y > subq)
+        self._compare_sqla(expr, stmt)
+
+    def test_subquery_aliased(self):
+        expr = self._case_subquery_aliased()
+
+        s1 = self._get_sqla('star1').alias('t2')
+        s2 = self._get_sqla('star2').alias('t1')
+
+        agged = (sa.select([s1.c.foo_id, F.sum(s1.c.f).label('total')])
+                 .group_by(s1.c.foo_id)
+                 .alias('t0'))
+
+        joined = agged.join(s2, agged.c.foo_id == s2.c.foo_id)
+        expected = sa.select([agged, s2.c.value1]).select_from(joined)
+
+        self._compare_sqla(expr, expected)
+
+    def test_lower_projection_sort_key(self):
+        expr = self._case_subquery_aliased()
+
+        s1 = self._get_sqla('star1').alias('t2')
+        s2 = self._get_sqla('star2').alias('t1')
+
+        expr2 = (expr
+                 [expr.total > 100]
+                 .sort_by(ibis.desc('total')))
+
+        agged = (sa.select([s1.c.foo_id, F.sum(s1.c.f).label('total')])
+                 .group_by(s1.c.foo_id)
+                 .alias('t3'))
+
+        joined = agged.join(s2, agged.c.foo_id == s2.c.foo_id)
+        expected = sa.select([agged, s2.c.value1]).select_from(joined)
+
+        joined = agged.join(s2, agged.c.foo_id == s2.c.foo_id)
+        expected = sa.select([agged, s2.c.value1]).select_from(joined)
+
+        ex = expected.alias('t0')
+
+        expected2 = (sa.select([ex])
+                     .where(ex.c.total > L(100))
+                     .order_by(ex.c.total.desc()))
+
+        self._compare_sqla(expr2, expected2)
+
+    def test_exists(self):
+        e1, e2 = self._case_exists()
+
+        t1 = self._to_sqla(self.t1).alias('t0')
+        t2 = self._to_sqla(self.t2).alias('t1')
+
+        cond1 = sa.exists([L(1)]).where(t1.c.key1 == t2.c.key1)
+        ex1 = sa.select([t1]).where(cond1)
+
+        cond2 = sa.exists([L(1)]).where(
+            sql.and_(t1.c.key1 == t2.c.key1, t2.c.key2 == L('foo')))
+        ex2 = sa.select([t1]).where(cond2)
+
+        # pytest.skip('not yet implemented')
+
+        self._compare_sqla(e1, ex1)
+        self._compare_sqla(e2, ex2)
+
+    def test_not_exists(self):
+        expr = self._case_not_exists()
+
+        t1 = self._to_sqla(self.t1).alias('t0')
+        t2 = self._to_sqla(self.t2).alias('t1')
+
+        cond1 = sa.exists([L(1)]).where(t1.c.key1 == t2.c.key1)
+        expected = sa.select([t1]).where(sa.not_(cond1))
+
+        self._compare_sqla(expr, expected)
+
+    def test_general_sql_function(self):
+        pass
+
+    def test_union(self):
+        pass
+
+    def test_table_distinct(self):
+        t = self.alltypes
+        sat = self.sa_alltypes.alias('t0')
+
+        cases = [
+            (t.distinct(), sa.select([sat]).distinct()),
+            (t['string_col', 'int_col'].distinct(),
+             sa.select([sat.c.string_col, sat.c.int_col]).distinct())
+        ]
+        for case, ex in cases:
+            self._compare_sqla(case, ex)
+
+    def test_array_distinct(self):
+        t = self.alltypes
+        sat = self.sa_alltypes.alias('t0')
+
+        cases = [
+            (t.string_col.distinct(),
+             sa.select([sat.c.string_col.distinct()]))
+        ]
+        for case, ex in cases:
+            self._compare_sqla(case, ex)
+
+    def test_count_distinct(self):
+        t = self.alltypes
+        sat = self.sa_alltypes.alias('t0')
+
+        cases = [
+            (t.int_col.nunique().name('nunique'),
+             sa.select([F.count(sat.c.int_col.distinct())
+                        .label('nunique')])),
+            (t.group_by('string_col')
+             .aggregate(t.int_col.nunique().name('nunique')),
+             sa.select([sat.c.string_col,
+                        F.count(sat.c.int_col.distinct())
+                        .label('nunique')])
+             .group_by(sat.c.string_col)),
+        ]
+        for case, ex in cases:
+            self._compare_sqla(case, ex)
+
+    def test_sort_aggregation_translation_failure(self):
+        # This works around a nuance with our choice to hackishly fuse SortBy
+        # after Aggregate to produce a single select statement rather than an
+        # inline view.
+        t = self.alltypes
+        sat = self.sa_alltypes.alias('t0')
+
+        agg = (t.group_by('string_col')
+               .aggregate(t.double_col.max().name('foo')))
+        expr = agg.sort_by(ibis.desc('foo'))
+
+        ex = (sa.select([sat.c.string_col,
+                         F.max(sat.c.double_col).label('foo')])
+              .group_by(sat.c.string_col)
+              .order_by(sa.desc('foo')))
 
-sa = pytest.importorskip("sqlalchemy")
+        self._compare_sqla(expr, ex)
 
+    def _compare_sqla(self, expr, sqla):
+        result = alch.to_sqlalchemy(expr)
+        assert str(result.compile()) == str(sqla.compile())
 
-@pytest.mark.parametrize(
-    ('func', 'expected'),
-    [
-        (
-            lambda t: t.double_col.cast(dt.int8),
-            lambda at: sa.cast(at.c.double_col, sa.SMALLINT),
-        ),
-        (
-            lambda t: t.string_col.cast(dt.float64),
-            lambda at: sa.cast(at.c.string_col, sa.REAL),
-        ),
-        (
-            lambda t: t.string_col.cast(dt.float32),
-            lambda at: sa.cast(at.c.string_col, sa.REAL),
-        ),
-    ],
-)
-def test_cast(alltypes, alltypes_sqla, translate, func, expected):
-    expr = func(alltypes)
-    assert translate(expr.op()) == str(expected(alltypes_sqla.alias("t0")))
-
-
-@pytest.mark.parametrize(
-    ('func', 'expected_func'),
-    [
-        param(
-            lambda t: t.timestamp_col.cast(dt.timestamp),
-            lambda at: at.c.timestamp_col,
-            id="timestamp_col",
-        ),
-        param(
-            lambda t: t.int_col.cast(dt.timestamp),
-            lambda at: sa.func.datetime(at.c.int_col, 'unixepoch'),
-            id="cast_integer_to_timestamp",
-        ),
-    ],
-)
-def test_timestamp_cast_noop(
-    alltypes, func, translate, alltypes_sqla, expected_func, sqla_compile
-):
-    # See GH #592
-    result = func(alltypes)
-    expected = expected_func(alltypes_sqla.alias("t0"))
-    assert translate(result.op()) == sqla_compile(expected)
-
-
-def test_timestamp_functions(con):
-    value = ibis.timestamp('2015-09-01 14:48:05.359')
-    expr = value.strftime('%Y%m%d')
-    expected = '20150901'
-    assert con.execute(expr) == expected
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'),
-    [
-        (L(3) + L(4), 7),
-        (L(3) - L(4), -1),
-        (L(3) * L(4), 12),
-        (L(12) / L(4), 3),
-        (L(12) ** L(2), 144),
-        (L(12) % L(5), 2),
-    ],
-)
-def test_binary_arithmetic(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'),
-    [
-        (L(7) / L(2), 3.5),
-        (L(7) // L(2), 3),
-        (L(7).floordiv(2), 3),
-        (L(2).rfloordiv(7), 3),
-    ],
-)
-def test_div_floordiv(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'),
-    [(L(0).nullifzero(), None), (L(5.5).nullifzero(), 5.5)],
-)
-def test_nullifzero(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'), [(L('foo_bar').length(), 7), (L('').length(), 0)]
-)
-def test_string_length(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'),
-    [
-        (L('foo_bar').left(3), 'foo'),
-        (L('foo_bar').right(3), 'bar'),
-        (L('foo_bar').substr(0, 3), 'foo'),
-        (L('foo_bar').substr(4, 3), 'bar'),
-        (L('foo_bar').substr(1), 'oo_bar'),
-    ],
-)
-def test_string_substring(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'),
-    [
-        (L('   foo   ').lstrip(), 'foo   '),
-        (L('   foo   ').rstrip(), '   foo'),
-        (L('   foo   ').strip(), 'foo'),
-    ],
-)
-def test_string_strip(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'),
-    [(L('foo').upper(), 'FOO'), (L('FOO').lower(), 'foo')],
-)
-def test_string_upper_lower(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'),
-    [
-        (L('foobar').contains('bar'), True),
-        (L('foobar').contains('foo'), True),
-        (L('foobar').contains('baz'), False),
-    ],
-)
-def test_string_contains(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'),
-    [(L('foobar').find('bar'), 3), (L('foobar').find('baz'), -1)],
-)
-def test_string_find(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'),
-    [
-        (L('foobar').like('%bar'), True),
-        (L('foobar').like('foo%'), True),
-        (L('foobar').like('%baz%'), False),
-        (L('foobar').like(['%bar']), True),
-        (L('foobar').like(['foo%']), True),
-        (L('foobar').like(['%baz%']), False),
-        (L('foobar').like(['%bar', 'foo%']), True),
-    ],
-)
-def test_string_like(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-def test_str_replace(con):
-    expr = L('foobarfoo').replace('foo', 'H')
-    expected = 'HbarH'
-    assert con.execute(expr) == expected
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'),
-    [
-        (L(-5).abs(), 5),
-        (L(5).abs(), 5),
-        (ibis.least(L(5), L(10), L(1)), 1),
-        (ibis.greatest(L(5), L(10), L(1)), 10),
-        (L(5.5).round(), 6.0),
-        (L(5.556).round(2), 5.56),
-        (L(5.556).sqrt(), math.sqrt(5.556)),
-        (L(5.556).ceil(), 6.0),
-        (L(5.556).floor(), 5.0),
-        (L(5.556).exp(), math.exp(5.556)),
-        (L(5.556).sign(), 1),
-        (L(-5.556).sign(), -1),
-        (L(0).sign(), 0),
-        (L(5.556).log(2), math.log(5.556, 2)),
-        (L(5.556).ln(), math.log(5.556)),
-        (L(5.556).log2(), math.log(5.556, 2)),
-        (L(5.556).log10(), math.log10(5.556)),
-    ],
-)
-def test_math_functions(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-NULL_STRING = L(None).cast(dt.string)
-NULL_INT64 = L(None).cast(dt.int64)
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'),
-    [
-        (L('abcd').re_search('[a-z]'), True),
-        (L('abcd').re_search(r'[\d]+'), False),
-        (L('1222').re_search(r'[\d]+'), True),
-        (L('abcd').re_search(None), None),
-        (NULL_STRING.re_search('[a-z]'), None),
-        (NULL_STRING.re_search(NULL_STRING), None),
-    ],
-)
-def test_regexp_search(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'),
-    [
-        (L('abcd').re_replace('[ab]', ''), 'cd'),
-        (L(None).cast(dt.string).re_replace(NULL_STRING, NULL_STRING), None),
-        (L('abcd').re_replace(NULL_STRING, NULL_STRING), None),
-        (L('abcd').re_replace('a', NULL_STRING), None),
-        (L('abcd').re_replace(NULL_STRING, 'a'), None),
-        (NULL_STRING.re_replace('a', NULL_STRING), None),
-        (NULL_STRING.re_replace(NULL_STRING, 'a'), None),
-    ],
-)
-def test_regexp_replace(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'),
-    [
-        (L('1222').re_extract(r'1(22)\d+', 1).cast('int64'), 22),
-        (L('abcd').re_extract(r'(\d+)', 1), None),
-        (L('1222').re_extract('([a-z]+)', 1), None),
-        (L('1222').re_extract(r'1(22)\d+', 2), None),
-        # extract nulls
-        (NULL_STRING.re_extract(NULL_STRING, NULL_INT64), None),
-        (L('abcd').re_extract(NULL_STRING, NULL_INT64), None),
-        (L('abcd').re_extract('a', NULL_INT64), None),
-        (L('abcd').re_extract(NULL_STRING, 1), None),
-        (NULL_STRING.re_extract('a', NULL_INT64), None),
-        (NULL_STRING.re_extract(NULL_STRING, 1), None),
-    ],
-)
-def test_regexp_extract(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-@pytest.mark.parametrize(
-    ('expr', 'expected'),
-    [
-        (ibis.NA.fillna(5), 5),
-        (L(5).fillna(10), 5),
-        (L(5).nullif(5), None),
-        (L(10).nullif(5), 10),
-    ],
-)
-def test_fillna_nullif(con, expr, expected):
-    assert con.execute(expr) == expected
-
-
-def test_numeric_builtins_work(alltypes, df):
-    expr = alltypes.double_col.fillna(0).name('tmp')
-    result = expr.execute()
-    expected = df.double_col.fillna(0)
-    expected.name = 'tmp'
-    tm.assert_series_equal(result, expected)
-
-
-@pytest.mark.parametrize(
-    ('func', 'expected_func'),
-    [
-        (
-            lambda t: (t.double_col > 20).ifelse(10, -20),
-            lambda df: pd.Series(
-                np.where(df.double_col > 20, 10, -20), name='tmp', dtype='int8'
-            ),
-        ),
-        (
-            lambda t: (t.double_col > 20).ifelse(10, -20).abs(),
-            lambda df: pd.Series(
-                np.where(df.double_col > 20, 10, -20), name='tmp', dtype='int8'
-            ).abs(),
-        ),
-    ],
-)
-def test_ifelse(alltypes, df, func, expected_func):
-    expr = func(alltypes).name('tmp')
-    result = expr.execute()
-    expected = expected_func(df)
-    tm.assert_series_equal(result, expected)
-
-
-@pytest.mark.parametrize(
-    ('func', 'expected_func'),
-    [
-        # tier and histogram
-        param(
-            lambda d: d.bucket([0, 10, 25, 50, 100]),
-            lambda s: pd.cut(s, [0, 10, 25, 50, 100], right=False, labels=False).astype(
-                "int8"
-            ),
-            id="default",
-        ),
-        param(
-            lambda d: d.bucket([0, 10, 25, 50], include_over=True),
-            lambda s: pd.cut(
-                s, [0, 10, 25, 50, np.inf], right=False, labels=False
-            ).astype("int8"),
-            id="include_over",
-        ),
-        param(
-            lambda d: d.bucket([0, 10, 25, 50], close_extreme=False),
-            lambda s: pd.cut(s, [0, 10, 25, 50], right=False, labels=False),
-            id="no_close_extreme",
-        ),
-        param(
-            lambda d: d.bucket([0, 10, 25, 50], closed='right', close_extreme=False),
-            lambda s: pd.cut(
-                s,
-                [0, 10, 25, 50],
-                include_lowest=False,
-                right=True,
-                labels=False,
-            ),
-            id="closed_right_no_close_extreme",
-        ),
-        param(
-            lambda d: d.bucket([10, 25, 50, 100], include_under=True),
-            lambda s: pd.cut(s, [0, 10, 25, 50, 100], right=False, labels=False).astype(
-                "int8"
-            ),
-            id="include_under",
-        ),
-    ],
-)
-def test_bucket(alltypes, df, func, expected_func):
-    expr = func(alltypes.double_col)
-    result = expr.execute()
-    expected = expected_func(df.double_col)
-
-    tm.assert_series_equal(result, expected, check_names=False)
-
-
-def test_category_label(alltypes, df):
-    bins = [0, 10, 25, 50, 100]
-    labels = ['a', 'b', 'c', 'd']
-    expr = alltypes.double_col.bucket(bins).label(labels)
-    result = expr.execute()
-    result = pd.Series(pd.Categorical(result, ordered=True))
-
-    result.name = 'double_col'
-
-    expected = pd.cut(df.double_col, bins, labels=labels, right=False)
-    tm.assert_series_equal(result, expected)
-
-
-@pytest.mark.xfail(
-    parse(sqlite3.sqlite_version) < parse('3.8.3'),
-    raises=sa.exc.OperationalError,
-    reason='SQLite versions < 3.8.3 do not support the WITH statement',
-)
-def test_union(alltypes):
-    t = alltypes
-
-    expr = (
-        t.group_by('string_col')
-        .aggregate(t.double_col.sum().name('foo'))
-        .order_by('string_col')
-    )
-
-    t1 = expr.limit(4)
-    t2 = expr.limit(4, offset=4)
-    t3 = expr.limit(8)
-
-    result = t1.union(t2).execute()
-    expected = t3.execute()
-
-    assert (result.string_col == expected.string_col).all()
-
-
-@pytest.mark.parametrize(
-    ('func', 'expected_func'),
-    [
-        (
-            lambda t, cond: t.bool_col.count(),
-            lambda df, cond: df.bool_col.count(),
-        ),
-        (lambda t, cond: t.bool_col.any(), lambda df, cond: df.bool_col.any()),
-        (lambda t, cond: t.bool_col.all(), lambda df, cond: df.bool_col.all()),
-        (
-            lambda t, cond: t.bool_col.notany(),
-            lambda df, cond: ~df.bool_col.any(),
-        ),
-        (
-            lambda t, cond: t.bool_col.notall(),
-            lambda df, cond: ~df.bool_col.all(),
-        ),
-        (
-            lambda t, cond: t.double_col.sum(),
-            lambda df, cond: df.double_col.sum(),
-        ),
-        (
-            lambda t, cond: t.double_col.mean(),
-            lambda df, cond: df.double_col.mean(),
-        ),
-        (
-            lambda t, cond: t.double_col.min(),
-            lambda df, cond: df.double_col.min(),
-        ),
-        (
-            lambda t, cond: t.double_col.max(),
-            lambda df, cond: df.double_col.max(),
-        ),
-        (
-            lambda t, cond: t.double_col.var(),
-            lambda df, cond: df.double_col.var(),
-        ),
-        (
-            lambda t, cond: t.double_col.std(),
-            lambda df, cond: df.double_col.std(),
-        ),
-        (
-            lambda t, cond: t.double_col.var(how='sample'),
-            lambda df, cond: df.double_col.var(ddof=1),
-        ),
-        (
-            lambda t, cond: t.double_col.std(how='pop'),
-            lambda df, cond: df.double_col.std(ddof=0),
-        ),
-        (
-            lambda t, cond: t.bool_col.count(where=cond),
-            lambda df, cond: df.bool_col[cond].count(),
-        ),
-        (
-            lambda t, cond: t.double_col.sum(where=cond),
-            lambda df, cond: df.double_col[cond].sum(),
-        ),
-        (
-            lambda t, cond: t.double_col.mean(where=cond),
-            lambda df, cond: df.double_col[cond].mean(),
-        ),
-        (
-            lambda t, cond: t.double_col.min(where=cond),
-            lambda df, cond: df.double_col[cond].min(),
-        ),
-        (
-            lambda t, cond: t.double_col.max(where=cond),
-            lambda df, cond: df.double_col[cond].max(),
-        ),
-        (
-            lambda t, cond: t.double_col.var(where=cond),
-            lambda df, cond: df.double_col[cond].var(),
-        ),
-        (
-            lambda t, cond: t.double_col.std(where=cond),
-            lambda df, cond: df.double_col[cond].std(),
-        ),
-        (
-            lambda t, cond: t.double_col.var(where=cond, how='sample'),
-            lambda df, cond: df.double_col[cond].var(),
-        ),
-        (
-            lambda t, cond: t.double_col.std(where=cond, how='pop'),
-            lambda df, cond: df.double_col[cond].std(ddof=0),
-        ),
-    ],
-)
-def test_aggregations_execute(alltypes, func, df, expected_func):
-    cond = alltypes.string_col.isin(['1', '7'])
-    expr = func(alltypes, cond)
-    result = expr.execute()
-    expected = expected_func(df, df.string_col.isin(['1', '7']))
-
-    np.testing.assert_allclose(result, expected)
-
-
-def test_not_contains(alltypes, df):
-    n = 100
-    table = alltypes.limit(n)
-    expr = table.string_col.notin(['1', '7'])
-    result = expr.execute()
-    expected = ~df.head(n).string_col.isin(['1', '7'])
-    tm.assert_series_equal(result, expected, check_names=False)
-
-
-def test_distinct_aggregates(alltypes, df):
-    expr = alltypes.double_col.nunique()
-    result = expr.execute()
-    expected = df.double_col.nunique()
-    assert result == expected
-
-
-def test_not_exists_works(alltypes):
-    t = alltypes
-    t2 = t.view()
-
-    expr = t[-((t.string_col == t2.string_col).any())]
-    expr.execute()
-
-
-def test_interactive_repr_shows_error(alltypes):
-    # #591. Doing this in SQLite because so many built-in functions are not
-    # available
-
-    expr = alltypes.double_col.approx_median()
-
-    with config.option_context('interactive', True):
-        result = repr(expr)
-        assert 'no translation rule' in result.lower()
-
-
-def test_subquery(alltypes, df):
-    t = alltypes
-
-    expr = t.mutate(d=t.double_col.fillna(0)).limit(1000).group_by('string_col').size()
-    result = expr.execute()
-    expected = (
-        df.assign(d=df.double_col.fillna(0))
-        .head(1000)
-        .groupby('string_col')
-        .size()
-        .reset_index()
-        .rename(columns={0: 'CountStar()'})
-    )
-    tm.assert_frame_equal(result, expected)
-
-
-def test_filter(alltypes, df):
-    expr = alltypes.filter(alltypes.year == 2010).float_col
-    result = expr.execute().squeeze().reset_index(drop=True)
-    expected = df.query('year == 2010').float_col
-    assert len(result) == len(expected)
-
-
-@pytest.mark.parametrize('column', [lambda t: 'float_col', lambda t: t['float_col']])
-def test_column_access_after_sort(alltypes, df, column):
-    expr = alltypes.order_by(column(alltypes)).head(10).string_col
-    result = expr.execute()
-    expected = df.sort_values('float_col').string_col.head(10).reset_index(drop=True)
-    tm.assert_series_equal(result, expected)
-
-
-@pytest.fixture
-def mj1(con, temp_table):
-    return con.create_table(
-        temp_table,
-        pd.DataFrame(dict(id1=[1, 2], val1=[10.0, 20.0])),
-        schema=ibis.schema(dict(id1="int64", val1="float64")),
-    )
-
-
-@pytest.fixture
-def mj2(con, temp_table_orig):
-    return con.create_table(
-        temp_table_orig,
-        pd.DataFrame(dict(id2=[1, 2], val2=[15.0, 25.0])),
-        schema=ibis.schema(dict(id2="int64", val2="float64")),
-    )
-
-
-def test_simple_join(mj1, mj2):
-    joined = mj1.join(mj2, mj1.id1 == mj2.id2)
-    result = joined.val2.execute()
-    assert len(result) == 2
-
-
-def test_anonymous_aggregate(alltypes, df):
-    expr = alltypes[alltypes.double_col > alltypes.double_col.mean()]
-    result = expr.execute()
-    expected = df[df.double_col > df.double_col.mean()].reset_index(drop=True)
-    tm.assert_frame_equal(result, expected)
-
-
-def test_head(alltypes):
-    t = alltypes
-    result = t.head().execute()
-    expected = t.limit(5).execute()
-    tm.assert_frame_equal(result, expected)
-
-
-def test_identical_to(alltypes):
-    t = alltypes
-    dt = t[['tinyint_col', 'double_col']].execute()
-    expr = t.tinyint_col.identical_to(t.double_col)
-    result = expr.execute()
-    expected = (dt.tinyint_col.isnull() & dt.double_col.isnull()) | (
-        dt.tinyint_col == dt.double_col
-    )
-    expected.name = result.name
-    tm.assert_series_equal(result, expected)
-
-
-@pytest.mark.xfail(
-    raises=AttributeError,
-    reason="truncate method is not yet implemented",
-)
-def test_truncate_method(con, alltypes):
-    expr = alltypes.limit(5)
-    name = str(uuid.uuid4())
-    t = con.create_table(name, expr)
-    assert len(t.execute()) == 5
-    t.truncate()
-    assert len(t.execute()) == 0
-
-
-def test_truncate_from_connection(con, alltypes):
-    expr = alltypes.limit(5)
-    name = str(uuid.uuid4())
-    t = con.create_table(name, expr)
-    assert len(t.execute()) == 5
-    con.truncate_table(name)
-    assert len(t.execute()) == 0
-
-
-def test_not(alltypes):
-    t = alltypes.limit(10)
-    expr = t.select([(~t.double_col.isnull()).name('double_col')])
-    result = expr.execute().double_col
-    expected = ~t.execute().double_col.isnull()
-    tm.assert_series_equal(result, expected)
-
-
-def test_compile_with_named_table():
-    t = ibis.table([('a', 'string')], name='t')
-    result = ibis.sqlite.compile(t.a)
-    st = sa.table('t', sa.column('a', sa.String)).alias('t0')
-    assert str(result) == str(sa.select(st.c.a))
-
-
-def test_compile_with_unnamed_table():
-    t = ibis.table([('a', 'string')])
-    result = ibis.sqlite.compile(t.a)
-    st = sa.table(t.op().name, sa.column('a', sa.String)).alias('t0')
-    assert str(result) == str(sa.select(st.c.a))
-
-
-def test_compile_with_multiple_unnamed_tables():
-    t = ibis.table([('a', 'string')])
-    s = ibis.table([('b', 'string')])
-    join = t.join(s, t.a == s.b)
-    result = ibis.sqlite.compile(join)
-    sqla_t = sa.table(t.op().name, sa.column('a', sa.String)).alias('t0')
-    sqla_s = sa.table(s.op().name, sa.column('b', sa.String)).alias('t1')
-    sqla_join = sqla_t.join(sqla_s, sqla_t.c.a == sqla_s.c.b)
-    expected = sa.select(sqla_t.c.a, sqla_s.c.b).select_from(sqla_join)
-    assert str(result) == str(expected)
-
-
-def test_compile_with_one_unnamed_table():
-    t = ibis.table([('a', 'string')])
-    s = ibis.table([('b', 'string')], name='s')
-    join = t.join(s, t.a == s.b)
-    result = ibis.sqlite.compile(join)
-    sqla_t = sa.table(t.op().name, sa.column('a', sa.String)).alias('t0')
-    sqla_s = sa.table('s', sa.column('b', sa.String)).alias('t1')
-    sqla_join = sqla_t.join(sqla_s, sqla_t.c.a == sqla_s.c.b)
-    expected = sa.select(sqla_t.c.a, sqla_s.c.b).select_from(sqla_join)
-    assert str(result) == str(expected)
-
-
-def test_scalar_parameter(alltypes):
-    start_string, end_string = '2009-03-01', '2010-07-03'
-
-    start = ibis.param(dt.date)
-    end = ibis.param(dt.date)
-    t = alltypes
-    col = t.date_string_col.cast('date')
-    expr = col.between(start, end).name('result')
-    result = expr.execute(params={start: start_string, end: end_string})
-
-    expected_expr = col.between(start_string, end_string).name('result')
-    expected = expected_expr.execute()
-    tm.assert_series_equal(result, expected)
-
-
-def test_count_on_order_by(con, snapshot):
-    t = con.table("batting")
-    sort_key = ibis.desc(t.playerID)
-    sorted_table = t.order_by(sort_key)
-    expr = sorted_table.count()
-    result = str(ibis.to_sql(expr, dialect="sqlite"))
-    snapshot.assert_match(result, "out.sql")
-
-
-def test_memtable_compilation(con):
-    expr = ibis.memtable({"a": [1, 2, 3]})
-    t = con.compile(expr)
-    assert t.exported_columns[0].name == "a"
+    def _to_sqla(self, table):
+        return table.op().sqla_table
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/common/graph.py` & `ibis-framework-v0.6.0/ibis/expr/groupby.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,223 +1,230 @@
-"""Various traversal utilities for the expression graph."""
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# User API for grouped data operations
+
+import ibis.expr.analysis as L
+import ibis.expr.operations as ops
+import ibis.expr.types as ir
+import ibis.expr.window as _window
+import ibis.util as util
+
+
+def _resolve_exprs(table, exprs):
+    exprs = util.promote_list(exprs)
+    return table._resolve(exprs)
 
-from abc import abstractmethod
-from collections import deque
-from collections.abc import Hashable, Iterable, Iterator, Mapping
-from typing import Any, Callable, Dict, Sequence
 
-from ibis.util import recursive_get
+class GroupedTableExpr(object):
 
-
-class Node(Hashable):
-    __slots__ = ()
-
-    @property
-    @abstractmethod
-    def __args__(self) -> Sequence:
-        ...
-
-    @property
-    @abstractmethod
-    def __argnames__(self) -> Sequence:
-        ...
-
-    def __children__(self, filter=None):
-        return tuple(_flatten_collections(self.__args__, filter or Node))
-
-    def __rich_repr__(self):
-        return zip(self.__argnames__, self.__args__)
-
-    def map(self, fn, filter=None):
-        results = {}
-        for node in Graph.from_bfs(self, filter=filter).toposort():
-            kwargs = dict(zip(node.__argnames__, node.__args__))
-            kwargs = recursive_get(kwargs, results)
-            results[node] = fn(node, results, **kwargs)
-
-        return results
-
-    def find(self, type, filter=None):
-        def fn(node, _, **kwargs):
-            if isinstance(node, type):
-                return node
-            return None
-
-        result = self.map(fn, filter=filter)
-
-        return {node for node in result.values() if node is not None}
-
-    def substitute(self, fn, filter=None):
-        return self.map(fn, filter=filter)[self]
-
-    def replace(self, subs, filter=None):
-        def fn(node, _, **kwargs):
-            try:
-                return subs[node]
-            except KeyError:
-                return node.__class__(**kwargs)
-
-        return self.substitute(fn, filter=filter)
-
-
-def _flatten_collections(node, filter=Node):
-    """Flatten collections of nodes into a single iterator.
-
-    We treat common collection types inherently Node (e.g. list, tuple, dict)
-    but as undesired in a graph representation, so we traverse them implicitly.
-
-    Parameters
-    ----------
-    node : Any
-        Flattaneble object unless it's an instance of the types passed as filter.
-    filter : type, default Node
-        Type to filter out for the traversal, e.g. Node.
-
-    Returns
-    -------
-    Iterator : Any
     """
-    if isinstance(node, filter):
-        yield node
-    elif isinstance(node, (str, bytes)):
-        pass
-    elif isinstance(node, Sequence):
-        for item in node:
-            yield from _flatten_collections(item, filter)
-    elif isinstance(node, Mapping):
-        for key, value in node.items():
-            yield from _flatten_collections(key, filter)
-            yield from _flatten_collections(value, filter)
-
-
-class Graph(Dict[Node, Sequence[Node]]):
-    def __init__(self, mapping=(), /, **kwargs):
-        if isinstance(mapping, Node):
-            mapping = self.from_bfs(mapping)
-        super().__init__(mapping, **kwargs)
-
-    @classmethod
-    def from_bfs(cls, root: Node, filter=Node) -> Graph:
-        if not isinstance(root, Node):
-            raise TypeError('node must be an instance of ibis.common.graph.Node')
-
-        queue = deque([root])
-        graph = cls()
-
-        while queue:
-            if (node := queue.popleft()) not in graph:
-                graph[node] = deps = node.__children__(filter)
-                queue.extend(deps)
-
-        return graph
-
-    @classmethod
-    def from_dfs(cls, root: Node, filter=Node) -> Graph:
-        if not isinstance(root, Node):
-            raise TypeError('node must be an instance of ibis.common.graph.Node')
-
-        stack = deque([root])
-        graph = dict()
-
-        while stack:
-            if (node := stack.pop()) not in graph:
-                graph[node] = deps = node.__children__(filter)
-                stack.extend(deps)
-
-        return cls(reversed(graph.items()))
-
-    def __repr__(self):
-        return f"{self.__class__.__name__}({super().__repr__()})"
-
-    def nodes(self):
-        return self.keys()
-
-    def invert(self) -> Graph:
-        result = {node: [] for node in self}
-        for node, dependencies in self.items():
-            for dependency in dependencies:
-                result[dependency].append(node)
-        return self.__class__({k: tuple(v) for k, v in result.items()})
-
-    def toposort(self) -> Graph:
-        dependents = self.invert()
-        in_degree = {k: len(v) for k, v in self.items()}
-
-        queue = deque(node for node, count in in_degree.items() if not count)
-        result = self.__class__()
-
-        while queue:
-            node = queue.popleft()
-            result[node] = self[node]
-
-            for dependent in dependents[node]:
-                in_degree[dependent] -= 1
-                if not in_degree[dependent]:
-                    queue.append(dependent)
-
-        if any(in_degree.values()):
-            raise ValueError("cycle detected in the graph")
-
-        return result
-
-
-def bfs(node: Node) -> Graph:
-    return Graph.from_bfs(node)
-
-
-def dfs(node: Node) -> Graph:
-    return Graph.from_dfs(node)
-
-
-def toposort(node: Node) -> Graph:
-    return Graph(node).toposort()
-
-
-# these could be callables instead
-proceed = True
-halt = False
-
-
-def traverse(
-    fn: Callable[[Node], tuple[bool | Iterable, Any]], node: Iterable[Node], filter=Node
-) -> Iterator[Any]:
-    """Utility for generic expression tree traversal.
-
-    Parameters
-    ----------
-    fn
-        A function applied on each expression. The first element of the tuple
-        controls the traversal, and the second is the result if its not `None`.
-    node
-        The Node expression or a list of expressions.
-    filter
-        Restrict initial traversal to this kind of node
+    Helper intermediate construct
     """
-    args = reversed(node) if isinstance(node, Iterable) else [node]
-    todo = deque(arg for arg in args if isinstance(arg, filter))
-    seen = set()
 
-    while todo:
-        node = todo.pop()
-
-        if node in seen:
-            continue
+    def __init__(self, table, by, having=None, order_by=None, window=None):
+        self.table = table
+        self.by = _resolve_exprs(table, by)
+        self._order_by = order_by or []
+        self._having = having or []
+        self._window = window
+
+    def __getitem__(self, args):
+        # Shortcut for projection with window functions
+        return self.projection(list(args))
+
+    def __getattr__(self, attr):
+        if hasattr(self.table, attr):
+            return self._column_wrapper(attr)
+
+        raise AttributeError("GroupBy has no attribute %r" % attr)
+
+    def _column_wrapper(self, attr):
+        col = self.table[attr]
+        if isinstance(col, ir.NumericValue):
+            return GroupedNumbers(col, self)
         else:
-            seen.add(node)
+            return GroupedArray(col, self)
 
-        control, result = fn(node)
-        if result is not None:
-            yield result
-
-        if control is not halt:
-            if control is proceed:
-                args = node.__children__(filter)
-            elif isinstance(control, Iterable):
-                args = control
-            else:
-                raise TypeError(
-                    'First item of the returned tuple must be '
-                    'an instance of boolean or iterable'
-                )
+    def aggregate(self, metrics=None, **kwds):
+        return self.table.aggregate(metrics, by=self.by,
+                                    having=self._having, **kwds)
+
+    def having(self, expr):
+        """
+        Add a post-aggregation result filter (like the having argument in
+        `aggregate`), for composability with the group_by API
+
+        Parameters
+        ----------
+
+        Returns
+        -------
+        grouped : GroupedTableExpr
+        """
+        exprs = util.promote_list(expr)
+        new_having = self._having + exprs
+        return GroupedTableExpr(self.table, self.by, having=new_having,
+                                order_by=self._order_by,
+                                window=self._window)
+
+    def order_by(self, expr):
+        """
+        Expressions to use for ordering data for a window function
+        computation. Ignored in aggregations.
+
+        Parameters
+        ----------
+        expr : value expression or list of value expressions
+
+        Returns
+        -------
+        grouped : GroupedTableExpr
+        """
+        exprs = util.promote_list(expr)
+        new_order = self._order_by + exprs
+        return GroupedTableExpr(self.table, self.by, having=self._having,
+                                order_by=new_order,
+                                window=self._window)
+
+    def mutate(self, exprs=None, **kwds):
+        """
+        Returns a table projection with analytic / window functions
+        applied. Any arguments can be functions.
+
+        Parameters
+        ----------
+        exprs : list, default None
+        kwds : key=value pairs
+
+        Examples
+        --------
+        >>> expr = (table
+                    .group_by('foo')
+                    .order_by(ibis.desc('bar'))
+                    .mutate(qux=lambda x: x.baz.lag(),
+                            qux2=table.baz.lead()))
+
+        Returns
+        -------
+        mutated : TableExpr
+        """
+        if exprs is None:
+            exprs = []
+        else:
+            exprs = util.promote_list(exprs)
 
-            todo.extend(reversed(args))
+        kwd_names = list(kwds.keys())
+        kwd_values = list(kwds.values())
+        kwd_values = self.table._resolve(kwd_values)
+
+        for k, v in sorted(zip(kwd_names, kwd_values)):
+            exprs.append(v.name(k))
+
+        return self.projection([self.table] + exprs)
+
+    def projection(self, exprs):
+        """
+        Like mutate, but do not include existing table columns
+        """
+        w = self._get_window()
+        windowed_exprs = []
+        exprs = self.table._resolve(exprs)
+        for expr in exprs:
+            expr = L.windowize_function(expr, w=w)
+            windowed_exprs.append(expr)
+        return self.table.projection(windowed_exprs)
+
+    def _get_window(self):
+        if self._window is None:
+            groups = self.by
+            sorts = self._order_by
+            preceding, following = None, None
+        else:
+            w = self._window
+            groups = w.group_by + self.by
+            sorts = w.order_by + self._order_by
+            preceding, following = w.preceding, w.following
+
+        sorts = [ops.to_sort_key(self.table, k) for k in sorts]
+
+        return _window.window(preceding=preceding, following=following,
+                              group_by=groups, order_by=sorts)
+
+    def over(self, window):
+        """
+        Add a window clause to be applied to downstream analytic expressions
+        """
+        return GroupedTableExpr(self.table, self.by, having=self._having,
+                                order_by=self._order_by,
+                                window=window)
+
+    def count(self, metric_name='count'):
+        """
+        Convenience function for computing the group sizes (number of rows per
+        group) given a grouped table.
+
+        Parameters
+        ----------
+        metric_name : string, default 'count'
+          Name to use for the row count metric
+
+        Returns
+        -------
+        aggregated : TableExpr
+          The aggregated table
+        """
+        metric = self.table.count().name(metric_name)
+        return self.table.aggregate([metric], by=self.by)
+
+    size = count
+
+
+def _group_agg_dispatch(name):
+    def wrapper(self, *args, **kwargs):
+        f = getattr(self.arr, name)
+        metric = f(*args, **kwargs)
+        alias = '{0}({1})'.format(name, self.arr.get_name())
+        return self.parent.aggregate(metric.name(alias))
+
+    wrapper.__name__ = name
+    return wrapper
+
+
+class GroupedArray(object):
+
+    def __init__(self, arr, parent):
+        self.arr = arr
+        self.parent = parent
+
+    count = _group_agg_dispatch('count')
+    size = count
+    min = _group_agg_dispatch('min')
+    max = _group_agg_dispatch('max')
+    approx_nunique = _group_agg_dispatch('approx_nunique')
+    approx_median = _group_agg_dispatch('approx_median')
+    group_concat = _group_agg_dispatch('group_concat')
+
+    def summary(self, exact_nunique=False):
+        metric = self.arr.summary(exact_nunique=exact_nunique)
+        return self.parent.aggregate(metric)
+
+
+class GroupedNumbers(GroupedArray):
+
+    mean = _group_agg_dispatch('mean')
+    sum = _group_agg_dispatch('sum')
+
+    def summary(self, exact_nunique=False):
+        metric = self.arr.summary(exact_nunique=exact_nunique)
+        return self.parent.aggregate(metric)
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/expr/analysis.py` & `ibis-framework-v0.6.0/ibis/cloudpickle.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,888 +1,874 @@
-from __future__ import annotations
+"""
+This class is defined to override standard pickle functionality
 
-import functools
-import operator
-from collections import defaultdict
-from typing import Iterable, Iterator, Mapping
+The goals of it follow:
+-Serialize lambdas and nested functions to compiled byte code
+-Deal with main module correctly
+-Deal with other non-serializable objects
+
+It does not include an unpickler, as standard python unpickling suffices.
+
+This module was extracted from the `cloud` package, developed by `PiCloud, Inc.
+<http://www.picloud.com>`_, and as modified for Apache Spark (licensed under
+ASL 2.0).
+
+Copyright (c) 2012, Regents of the University of California.
+Copyright (c) 2009 `PiCloud, Inc. <http://www.picloud.com>`_.
+All rights reserved.
+
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+    * Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+    * Redistributions in binary form must reproduce the above copyright
+      notice, this list of conditions and the following disclaimer in the
+      documentation and/or other materials provided with the distribution.
+    * Neither the name of the University of California, Berkeley nor the
+      names of its contributors may be used to endorse or promote
+      products derived from this software without specific prior written
+      permission.
+
+THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED
+TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
+PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
+LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+"""
 
-import toolz
+# flake8: noqa
 
-import ibis.common.graph as g
-import ibis.expr.operations as ops
-import ibis.expr.operations.relations as rels
-import ibis.expr.types as ir
-from ibis import util
-from ibis.common.exceptions import IbisTypeError, IntegrityError
-
-# ---------------------------------------------------------------------
-# Some expression metaprogramming / graph transformations to support
-# compilation later
-
-
-def sub_for(node: ops.Node, substitutions: Mapping[ops.node, ops.Node]) -> ops.Node:
-    """Substitute operations in `node` with nodes in `substitutions`.
-
-    Parameters
-    ----------
-    node
-        An Ibis operation
-    substitutions
-        A mapping from node to node. If any subnode of `node` is equal to any
-        of the keys in `substitutions`, the value for that key will replace the
-        corresponding node in `node`.
-
-    Returns
-    -------
-    Node
-        An Ibis expression
-    """
-    assert isinstance(node, ops.Node), type(node)
+from copy_reg import _extension_registry
+from functools import partial
 
-    def fn(node):
-        try:
-            return substitutions[node]
-        except KeyError:
-            if isinstance(node, ops.TableNode):
-                return g.halt
-            return g.proceed
-
-    return substitute(fn, node)
-
-
-def sub_immediate_parents(op: ops.Node, table: ops.TableNode) -> ops.Node:
-    """Replace immediate parent tables in `op` with `table`."""
-    return sub_for(op, {base: table for base in find_immediate_parent_tables(op)})
-
-
-class ScalarAggregate:
-    def __init__(self, expr):
-        assert isinstance(expr, ir.Expr)
-        self.expr = expr
-        self.tables = []
-
-    def get_result(self):
-        expr = self.expr
-        subbed_expr = self._visit(expr)
-
-        table = self.tables[0]
-        for other in self.tables[1:]:
-            table = table.cross_join(other)
-
-        return table.select(subbed_expr)
-
-    def _visit(self, expr):
-        assert isinstance(expr, ir.Expr), type(expr)
-
-        if is_scalar_reduction(expr.op()) and not has_multiple_bases(expr.op()):
-            # An aggregation unit
-            if not expr.has_name():
-                expr = expr.name('tmp')
-            agg_expr = reduction_to_aggregation(expr.op())
-            self.tables.append(agg_expr)
-            return agg_expr[expr.get_name()]
-        elif not isinstance(expr, ir.Expr):
-            return expr
-
-        node = expr.op()
-        # TODO(kszucs): use the substitute() utility instead
-        new_args = (
-            self._visit(arg.to_expr()) if isinstance(arg, ops.Node) else arg
-            for arg in node.args
-        )
-        new_node = node.__class__(*new_args)
-        new_expr = new_node.to_expr()
+import dis
+import itertools
+import operator
+import os
+import pickle
+import struct
+import sys
+import traceback
+import types
+
+import logging
+cloudLog = logging.getLogger("Cloud.Transport")
+
+#relevant opcodes
+STORE_GLOBAL = chr(dis.opname.index('STORE_GLOBAL'))
+DELETE_GLOBAL = chr(dis.opname.index('DELETE_GLOBAL'))
+LOAD_GLOBAL = chr(dis.opname.index('LOAD_GLOBAL'))
+GLOBAL_OPS = [STORE_GLOBAL, DELETE_GLOBAL, LOAD_GLOBAL]
+
+HAVE_ARGUMENT = chr(dis.HAVE_ARGUMENT)
+EXTENDED_ARG = chr(dis.EXTENDED_ARG)
+
+
+try:
+    from cStringIO import StringIO
+except ImportError:
+    from StringIO import StringIO
+
+# These helper functions were copied from PiCloud's util module.
+def islambda(func):
+    return getattr(func,'func_name') == '<lambda>'
+
+def xrange_params(xrangeobj):
+    """Returns a 3 element tuple describing the xrange start, step, and len
+    respectively
+
+    Note: Only guarentees that elements of xrange are the same. parameters may
+    be different.
+    e.g. xrange(1,1) is interpretted as xrange(0,0); both behave the same
+    though w/ iteration
+    """
 
-        if expr.has_name():
-            new_expr = new_expr.name(name=expr.get_name())
+    xrange_len = len(xrangeobj)
+    if not xrange_len: #empty
+        return (0,1,0)
+    start = xrangeobj[0]
+    if xrange_len == 1: #one element
+        return start, 1, 1
+    return (start, xrangeobj[1] - xrangeobj[0], xrange_len)
+
+#debug variables intended for developer use:
+printSerialization = False
+printMemoization = False
+
+useForcedImports = True #Should I use forced imports for tracking?
 
-        return new_expr
 
 
-def has_multiple_bases(node):
-    assert isinstance(node, ops.Node), type(node)
-    return len(find_immediate_parent_tables(node)) > 1
+class CloudPickler(pickle.Pickler):
+
+    dispatch = pickle.Pickler.dispatch.copy()
+    savedForceImports = False
+
+    def __init__(self, file, protocol=None, min_size_to_save= 0):
+        pickle.Pickler.__init__(self,file,protocol)
+        self.modules = set() #set of modules needed to depickle
+        self.globals_ref = {}  # map ids to dictionary. used to ensure that functions can share global env
+
+    def dump(self, obj):
+        # note: not thread safe
+        # minimal side-effects, so not fixing
+        recurse_limit = 3000
+        base_recurse = sys.getrecursionlimit()
+        if base_recurse < recurse_limit:
+            sys.setrecursionlimit(recurse_limit)
+        self.inject_addons()
+        try:
+            return pickle.Pickler.dump(self, obj)
+        except RuntimeError as e:
+            if 'recursion' in e.args[0]:
+                msg = """Could not pickle object as excessively deep recursion required.
+                Try _fast_serialization=2 or contact PiCloud support"""
+                raise pickle.PicklingError(msg)
+        finally:
+            new_recurse = sys.getrecursionlimit()
+            if new_recurse == recurse_limit:
+                sys.setrecursionlimit(base_recurse)
+
+    def save_buffer(self, obj):
+        """Fallback to save_string"""
+        pickle.Pickler.save_string(self,str(obj))
+    dispatch[buffer] = save_buffer
+
+    #block broken objects
+    def save_unsupported(self, obj, pack=None):
+        raise pickle.PicklingError("Cannot pickle objects of type %s" % type(obj))
+    dispatch[types.GeneratorType] = save_unsupported
 
+    #python2.6+ supports slice pickling. some py2.5 extensions might as well.  We just test it
+    try:
+        slice(0,1).__reduce__()
+    except TypeError: #can't pickle -
+        dispatch[slice] = save_unsupported
+
+    #itertools objects do not pickle!
+    for v in itertools.__dict__.values():
+        if type(v) is type:
+            dispatch[v] = save_unsupported
+
+
+    def save_dict(self, obj):
+        """hack fix
+        If the dict is a global, deal with it in a special way
+        """
+        #print 'saving', obj
+        if obj is __builtins__:
+            self.save_reduce(_get_module_builtins, (), obj=obj)
+        else:
+            pickle.Pickler.save_dict(self, obj)
+    dispatch[pickle.DictionaryType] = save_dict
+
+
+    def save_module(self, obj, pack=struct.pack):
+        """
+        Save a module as an import
+        """
+        #print 'try save import', obj.__name__
+        self.modules.add(obj)
+        self.save_reduce(subimport,(obj.__name__,), obj=obj)
+    dispatch[types.ModuleType] = save_module    #new type
+
+    def save_codeobject(self, obj, pack=struct.pack):
+        """
+        Save a code object
+        """
+        #print 'try to save codeobj: ', obj
+        args = (
+            obj.co_argcount, obj.co_nlocals, obj.co_stacksize, obj.co_flags, obj.co_code,
+            obj.co_consts, obj.co_names, obj.co_varnames, obj.co_filename, obj.co_name,
+            obj.co_firstlineno, obj.co_lnotab, obj.co_freevars, obj.co_cellvars
+        )
+        self.save_reduce(types.CodeType, args, obj=obj)
+    dispatch[types.CodeType] = save_codeobject    #new type
 
-def reduction_to_aggregation(node):
-    tables = find_immediate_parent_tables(node)
+    def save_function(self, obj, name=None, pack=struct.pack):
+        """ Registered with the dispatch to handle all function types.
 
-    # TODO(kszucs): avoid the expression roundtrip
-    node = ops.Alias(node, node.name)
-    expr = node.to_expr()
-    if len(tables) == 1:
-        (table,) = tables
-        agg = table.to_expr().aggregate([expr])
-    else:
-        agg = ScalarAggregate(expr).get_result()
+        Determines what kind of function obj is (e.g. lambda, defined at
+        interactive prompt, etc) and handles the pickling appropriately.
+        """
+        write = self.write
+
+        name = obj.__name__
+        modname = pickle.whichmodule(obj, name)
+        #print 'which gives %s %s %s' % (modname, obj, name)
+        try:
+            themodule = sys.modules[modname]
+        except KeyError: # eval'd items such as namedtuple give invalid items for their function __module__
+            modname = '__main__'
+
+        if modname == '__main__':
+            themodule = None
+
+        if themodule:
+            self.modules.add(themodule)
+            if getattr(themodule, name, None) is obj:
+                return self.save_global(obj, name)
+
+        # if func is lambda, def'ed at prompt, is in main, or is nested, then
+        # we'll pickle the actual function object rather than simply saving a
+        # reference (as is done in default pickler), via save_function_tuple.
+        if islambda(obj) or obj.func_code.co_filename == '<stdin>' or themodule is None:
+            #Force server to import modules that have been imported in main
+            modList = None
+            if themodule is None and not self.savedForceImports:
+                mainmod = sys.modules['__main__']
+                if useForcedImports and hasattr(mainmod,'___pyc_forcedImports__'):
+                    modList = list(mainmod.___pyc_forcedImports__)
+                self.savedForceImports = True
+            self.save_function_tuple(obj, modList)
+            return
+        else:   # func is nested
+            klass = getattr(themodule, name, None)
+            if klass is None or klass is not obj:
+                self.save_function_tuple(obj, [themodule])
+                return
+
+        if obj.__dict__:
+            # essentially save_reduce, but workaround needed to avoid recursion
+            self.save(_restore_attr)
+            write(pickle.MARK + pickle.GLOBAL + modname + '\n' + name + '\n')
+            self.memoize(obj)
+            self.save(obj.__dict__)
+            write(pickle.TUPLE + pickle.REDUCE)
+        else:
+            write(pickle.GLOBAL + modname + '\n' + name + '\n')
+            self.memoize(obj)
+    dispatch[types.FunctionType] = save_function
+
+    def save_function_tuple(self, func, forced_imports):
+        """  Pickles an actual func object.
+
+        A func comprises: code, globals, defaults, closure, and dict.  We
+        extract and save these, injecting reducing functions at certain points
+        to recreate the func object.  Keep in mind that some of these pieces
+        can contain a ref to the func itself.  Thus, a naive save on these
+        pieces could trigger an infinite loop of save's.  To get around that,
+        we first create a skeleton func object using just the code (this is
+        safe, since this won't contain a ref to the func), and memoize it as
+        soon as it's created.  The other stuff can then be filled in later.
+        """
+        save = self.save
+        write = self.write
+
+        # save the modules (if any)
+        if forced_imports:
+            write(pickle.MARK)
+            save(_modules_to_main)
+            #print 'forced imports are', forced_imports
+
+            forced_names = map(lambda m: m.__name__, forced_imports)
+            save((forced_names,))
+
+            #save((forced_imports,))
+            write(pickle.REDUCE)
+            write(pickle.POP_MARK)
+
+        code, f_globals, defaults, closure, dct, base_globals = self.extract_func_data(func)
+
+        save(_fill_function)  # skeleton function updater
+        write(pickle.MARK)    # beginning of tuple that _fill_function expects
+
+        # create a skeleton function object and memoize it
+        save(_make_skel_func)
+        save((code, closure, base_globals))
+        write(pickle.REDUCE)
+        self.memoize(func)
+
+        # save the rest of the func data needed by _fill_function
+        save(f_globals)
+        save(defaults)
+        save(dct)
+        write(pickle.TUPLE)
+        write(pickle.REDUCE)  # applies _fill_function on the tuple
+
+    @staticmethod
+    def extract_code_globals(co):
+        """
+        Find all globals names read or written to by codeblock co
+        """
+        code = co.co_code
+        names = co.co_names
+        out_names = set()
+
+        n = len(code)
+        i = 0
+        extended_arg = 0
+        while i < n:
+            op = code[i]
+
+            i = i+1
+            if op >= HAVE_ARGUMENT:
+                oparg = ord(code[i]) + ord(code[i+1])*256 + extended_arg
+                extended_arg = 0
+                i = i+2
+                if op == EXTENDED_ARG:
+                    extended_arg = oparg*65536
+                if op in GLOBAL_OPS:
+                    out_names.add(names[oparg])
+        #print 'extracted', out_names, ' from ', names
+
+        if co.co_consts:   # see if nested function have any global refs
+            for const in co.co_consts:
+                if type(const) is types.CodeType:
+                    out_names |= CloudPickler.extract_code_globals(const)
+
+        return out_names
+
+    def extract_func_data(self, func):
+        """
+        Turn the function into a tuple of data necessary to recreate it:
+            code, globals, defaults, closure, dict
+        """
+        code = func.func_code
+
+        # extract all global ref's
+        func_global_refs = CloudPickler.extract_code_globals(code)
+
+        # process all variables referenced by global environment
+        f_globals = {}
+        for var in func_global_refs:
+            #Some names, such as class functions are not global - we don't need them
+            if func.func_globals.has_key(var):
+                f_globals[var] = func.func_globals[var]
 
-    return agg
+        # defaults requires no processing
+        defaults = func.func_defaults
 
+        def get_contents(cell):
+            try:
+                return cell.cell_contents
+            except ValueError as e: #cell is empty error on not yet assigned
+                raise pickle.PicklingError('Function to be pickled has free variables that are referenced before assignment in enclosing scope')
 
-def find_physical_tables(node):
-    """Find every first occurrence of a `ir.PhysicalTable` object in `node`."""
 
-    def finder(node):
-        if isinstance(node, ops.PhysicalTable):
-            return g.halt, node
+        # process closure
+        if func.func_closure:
+            closure = map(get_contents, func.func_closure)
         else:
-            return g.proceed, None
+            closure = []
 
-    return list(toolz.unique(g.traverse(finder, node)))
+        # save the dict
+        dct = func.func_dict
 
+        if printSerialization:
+            outvars = ['code: ' + str(code) ]
+            outvars.append('globals: ' + str(f_globals))
+            outvars.append('defaults: ' + str(defaults))
+            outvars.append('closure: ' + str(closure))
+            print('function {0} is extracted to: {1}'.format(
+                func, ', '.join(outvars)))
 
-def find_immediate_parent_tables(input_node, keep_input=True):
-    """Find every first occurrence of a `ir.Table` object in `input_node`.
+        base_globals = self.globals_ref.get(id(func.func_globals), {})
+        self.globals_ref[id(func.func_globals)] = base_globals
 
-    This function does not traverse into `Table` objects. For example, the
-    underlying `PhysicalTable` of a `Selection` will not be yielded.
+        return (code, f_globals, defaults, closure, dct, base_globals)
 
-    Parameters
-    ----------
-    input_node
-        Input node
-    keep_input
-        Whether to keep the input when traversing
+    def save_builtin_function(self, obj):
+        if obj.__module__ is "__builtin__":
+            return self.save_global(obj)
+        return self.save_function(obj)
+    dispatch[types.BuiltinFunctionType] = save_builtin_function
 
-    Yields
-    ------
-    ir.Expr
-        Parent table expression
+    def save_global(self, obj, name=None, pack=struct.pack):
+        write = self.write
+        memo = self.memo
 
-    Examples
-    --------
-    >>> import ibis, toolz
-    >>> t = ibis.table([('a', 'int64')], name='t')
-    >>> expr = t.mutate(foo=t.a + 1)
-    >>> result, = find_immediate_parent_tables(expr.op())
-    >>> result.equals(expr.op())
-    True
-    >>> result, = find_immediate_parent_tables(expr.op(), keep_input=False)
-    >>> result.equals(t.op())
-    True
-    """
-    assert all(isinstance(arg, ops.Node) for arg in util.promote_list(input_node))
-
-    def finder(node):
-        if isinstance(node, ops.TableNode):
-            if keep_input or node != input_node:
-                return g.halt, node
-            else:
-                return g.proceed, None
+        if name is None:
+            name = obj.__name__
 
-        # HACK: special case ops.Contains to only consider the needle's base
-        # table, since that's the only expression that matters for determining
-        # cardinality
-        elif isinstance(node, ops.Contains):
-            return [node.value], None
-        else:
-            return g.proceed, None
-
-    return list(toolz.unique(g.traverse(finder, input_node)))
-
-
-def substitute(fn, node):
-    """Substitute expressions with other expressions."""
-
-    assert isinstance(node, ops.Node), type(node)
-
-    result = fn(node)
-    if result is g.halt:
-        return node
-    elif result is not g.proceed:
-        assert isinstance(result, ops.Node), type(result)
-        return result
-
-    new_args = []
-    for arg in node.args:
-        if isinstance(arg, tuple):
-            arg = tuple(
-                substitute(fn, x) if isinstance(arg, ops.Node) else x for x in arg
-            )
-        elif isinstance(arg, ops.Node):
-            arg = substitute(fn, arg)
-        new_args.append(arg)
-
-    try:
-        return node.__class__(*new_args)
-    except TypeError:
-        return node
-
-
-def substitute_parents(node):
-    """Rewrite `node` by replacing table nodes that commute."""
-    assert isinstance(node, ops.Node), type(node)
-
-    def fn(node):
-        if isinstance(node, ops.Selection):
-            # stop substituting child nodes
-            return g.halt
-        elif isinstance(node, ops.TableColumn):
-            # For table column references, in the event that we're on top of a
-            # projection, we need to check whether the ref comes from the base
-            # table schema or is a derived field. If we've projected out of
-            # something other than a physical table, then lifting should not
-            # occur
-            table = node.table
-
-            if isinstance(table, ops.Selection):
-                for val in table.selections:
-                    if isinstance(val, ops.PhysicalTable) and node.name in val.schema:
-                        return ops.TableColumn(val, node.name)
-
-        # keep looking for nodes to substitute
-        return g.proceed
-
-    return substitute(fn, node)
-
-
-def substitute_unbound(node):
-    """Rewrite `node` by replacing table expressions with an equivalent unbound table."""
-    assert isinstance(node, ops.Node), type(node)
-
-    def fn(node, _, *args, **kwargs):
-        if isinstance(node, ops.DatabaseTable):
-            return ops.UnboundTable(name=node.name, schema=node.schema)
-        else:
-            return node.__class__(*args, **kwargs)
-
-    return node.substitute(fn)
-
-
-def get_mutation_exprs(exprs: list[ir.Expr], table: ir.Table) -> list[ir.Expr | None]:
-    """Return the exprs to use to instantiate the mutation."""
-    # The below logic computes the mutation node exprs by splitting the
-    # assignment exprs into two disjoint sets:
-    # 1) overwriting_cols_to_expr, which maps a column name to its expr
-    # if the expr contains a column that overwrites an existing table column.
-    # All keys in this dict are columns in the original table that are being
-    # overwritten by an assignment expr.
-    # 2) non_overwriting_exprs, which is a list of all exprs that do not do
-    # any overwriting. That is, if an expr is in this list, then its column
-    # name does not exist in the original table.
-    # Given these two data structures, we can compute the mutation node exprs
-    # based on whether any columns are being overwritten.
-    overwriting_cols_to_expr: dict[str, ir.Expr | None] = {}
-    non_overwriting_exprs: list[ir.Expr] = []
-    table_schema = table.schema()
-    for expr in exprs:
-        expr_contains_overwrite = False
-        if isinstance(expr, ir.Value) and expr.get_name() in table_schema:
-            overwriting_cols_to_expr[expr.get_name()] = expr
-            expr_contains_overwrite = True
-
-        if not expr_contains_overwrite:
-            non_overwriting_exprs.append(expr)
-
-    columns = table.columns
-    if overwriting_cols_to_expr:
-        return [
-            overwriting_cols_to_expr.get(column, table[column])
-            for column in columns
-            if overwriting_cols_to_expr.get(column, table[column]) is not None
-        ] + non_overwriting_exprs
-
-    table_expr: ir.Expr = table
-    return [table_expr] + exprs
-
-
-def apply_filter(op, predicates):
-    # This will attempt predicate pushdown in the cases where we can do it
-    # easily and safely, to make both cleaner SQL and fewer referential errors
-    # for users
-    if not predicates:
-        return op
-
-    if isinstance(op, ops.Selection):
-        return pushdown_selection_filters(op, predicates)
-    elif isinstance(op, ops.Aggregation):
-        return pushdown_aggregation_filters(op, predicates)
-    else:
-        return ops.Selection(op, [], predicates)
-
-
-def pushdown_selection_filters(op, predicates):
-    default = ops.Selection(op, selections=[], predicates=predicates)
-
-    # We can't push down filters on Unnest or Window because they
-    # change the shape and potential values of the data.
-    if any(
-        isinstance(
-            sel.arg if isinstance(sel, ops.Alias) else sel,
-            (ops.Unnest, ops.Window),
-        )
-        for sel in op.selections
-    ):
-        return default
-
-    # if any of the filter predicates have the parent expression among
-    # their roots, then pushdown (at least of that predicate) is not
-    # possible
-
-    # It's not unusual for the filter to reference the projection
-    # itself. If a predicate can be pushed down, in this case we must
-    # rewrite replacing the table refs with the roots internal to the
-    # projection we are referencing
-    #
-    # Assuming that the fields referenced by the filter predicate originate
-    # below the projection, we need to rewrite the predicate referencing
-    # the parent tables in the join being projected
-
-    # Potential fusion opportunity. The predicates may need to be
-    # rewritten in terms of the child table. This prevents the broken
-    # ref issue (described in more detail in #59)
-    try:
-        simplified_predicates = tuple(
-            sub_for(predicate, {op: op.table})
-            if not is_reduction(predicate)
-            else predicate
-            for predicate in predicates
-        )
-    except IntegrityError:
-        return default
+        modname = getattr(obj, "__module__", None)
+        if modname is None:
+            modname = pickle.whichmodule(obj, name)
 
-    if not shares_all_roots(simplified_predicates, op.table):
-        return default
+        try:
+            __import__(modname)
+            themodule = sys.modules[modname]
+        except (ImportError, KeyError, AttributeError):  #should never occur
+            raise pickle.PicklingError(
+                "Can't pickle %r: Module %s cannot be found" %
+                (obj, modname))
+
+        if modname == '__main__':
+            themodule = None
+
+        if themodule:
+            self.modules.add(themodule)
+
+        sendRef = True
+        typ = type(obj)
+        #print 'saving', obj, typ
+        try:
+            try: #Deal with case when getattribute fails with exceptions
+                klass = getattr(themodule, name)
+            except (AttributeError):
+                if modname == '__builtin__':  #new.* are misrepeported
+                    modname = 'new'
+                    __import__(modname)
+                    themodule = sys.modules[modname]
+                    try:
+                        klass = getattr(themodule, name)
+                    except AttributeError as a:
+                        # print themodule, name, obj, type(obj)
+                        raise pickle.PicklingError("Can't pickle builtin %s" % obj)
+                else:
+                    raise
+
+        except (ImportError, KeyError, AttributeError):
+            if typ == types.TypeType or typ == types.ClassType:
+                sendRef = False
+            else: #we can't deal with this
+                raise
+        else:
+            if klass is not obj and (typ == types.TypeType or typ == types.ClassType):
+                sendRef = False
+        if not sendRef:
+            #note: Third party types might crash this - add better checks!
+            d = dict(obj.__dict__) #copy dict proxy to a dict
+            if not isinstance(d.get('__dict__', None), property): # don't extract dict that are properties
+                d.pop('__dict__',None)
+            d.pop('__weakref__',None)
+
+            # hack as __new__ is stored differently in the __dict__
+            new_override = d.get('__new__', None)
+            if new_override:
+                d['__new__'] = obj.__new__
+
+            self.save_reduce(type(obj),(obj.__name__,obj.__bases__,
+                                   d),obj=obj)
+            #print 'internal reduce dask %s %s'  % (obj, d)
+            return
+
+        if self.proto >= 2:
+            code = _extension_registry.get((modname, name))
+            if code:
+                assert code > 0
+                if code <= 0xff:
+                    write(pickle.EXT1 + chr(code))
+                elif code <= 0xffff:
+                    write("%c%c%c" % (pickle.EXT2, code&0xff, code>>8))
+                else:
+                    write(pickle.EXT4 + pack("<i", code))
+                return
+
+        write(pickle.GLOBAL + modname + '\n' + name + '\n')
+        self.memoize(obj)
+    dispatch[types.ClassType] = save_global
+    dispatch[types.TypeType] = save_global
+
+    def save_instancemethod(self, obj):
+        #Memoization rarely is ever useful due to python bounding
+        self.save_reduce(types.MethodType, (obj.im_func, obj.im_self,obj.im_class), obj=obj)
+    dispatch[types.MethodType] = save_instancemethod
+
+    def save_inst_logic(self, obj):
+        """Inner logic to save instance. Based off pickle.save_inst
+        Supports __transient__"""
+        cls = obj.__class__
+
+        memo  = self.memo
+        write = self.write
+        save  = self.save
+
+        if hasattr(obj, '__getinitargs__'):
+            args = obj.__getinitargs__()
+            len(args) # XXX Assert it's a sequence
+            pickle._keep_alive(args, memo)
+        else:
+            args = ()
+
+        write(pickle.MARK)
+
+        if self.bin:
+            save(cls)
+            for arg in args:
+                save(arg)
+            write(pickle.OBJ)
+        else:
+            for arg in args:
+                save(arg)
+            write(pickle.INST + cls.__module__ + '\n' + cls.__name__ + '\n')
 
-    # find spuriously simplified predicates
-    for predicate in simplified_predicates:
-        # find columns in the predicate
-        depends_on = predicate.find((ops.TableColumn, ops.Literal))
-        for projection in op.selections:
-            if not isinstance(projection, (ops.TableColumn, ops.Literal)):
-                # if the projection's table columns overlap with columns
-                # used in the predicate then we return immediately
-                #
-                # this means that we were too aggressive during simplification
-                # example: t.mutate(a=_.a + 1).filter(_.a > 1)
-                if projection.find((ops.TableColumn, ops.Literal)) & depends_on:
-                    return default
-
-    return ops.Selection(
-        op.table,
-        selections=op.selections,
-        predicates=op.predicates + simplified_predicates,
-        sort_keys=op.sort_keys,
-    )
-
-
-def pushdown_aggregation_filters(op, predicates):
-    # Potential fusion opportunity
-    # GH1344: We can't sub in things with correlated subqueries
-    simplified_predicates = tuple(
-        # Originally this line tried substituting op.table in for expr, but
-        # that is too aggressive in the presence of filters that occur
-        # after aggregations.
-        #
-        # See https://github.com/ibis-project/ibis/pull/3341 for details
-        sub_for(predicate, {op.table: op}) if not is_reduction(predicate) else predicate
-        for predicate in predicates
-    )
-
-    if shares_all_roots(simplified_predicates, op.table):
-        return ops.Aggregation(
-            op.table,
-            op.metrics,
-            by=op.by,
-            having=op.having,
-            predicates=op.predicates + simplified_predicates,
-            sort_keys=op.sort_keys,
-        )
-    else:
-        return ops.Selection(op, [], predicates)
+        self.memoize(obj)
 
+        try:
+            getstate = obj.__getstate__
+        except AttributeError:
+            stuff = obj.__dict__
+            #remove items if transient
+            if hasattr(obj, '__transient__'):
+                transient = obj.__transient__
+                stuff = stuff.copy()
+                for k in list(stuff.keys()):
+                    if k in transient:
+                        del stuff[k]
+        else:
+            stuff = getstate()
+            pickle._keep_alive(stuff, memo)
+        save(stuff)
+        write(pickle.BUILD)
+
+
+    def save_inst(self, obj):
+        # Hack to detect PIL Image instances without importing Imaging
+        # PIL can be loaded with multiple names, so we don't check sys.modules for it
+        if hasattr(obj,'im') and hasattr(obj,'palette') and 'Image' in obj.__module__:
+            self.save_image(obj)
+        else:
+            self.save_inst_logic(obj)
+    dispatch[types.InstanceType] = save_inst
+
+    def save_property(self, obj):
+        # properties not correctly saved in python
+        self.save_reduce(property, (obj.fget, obj.fset, obj.fdel, obj.__doc__), obj=obj)
+    dispatch[property] = save_property
+
+    def save_itemgetter(self, obj):
+        """itemgetter serializer (needed for namedtuple support)"""
+        class Dummy:
+            def __getitem__(self, item):
+                return item
+        items = obj(Dummy())
+        if not isinstance(items, tuple):
+            items = (items, )
+        return self.save_reduce(operator.itemgetter, items)
+
+    if type(operator.itemgetter) is type:
+        dispatch[operator.itemgetter] = save_itemgetter
+
+    def save_attrgetter(self, obj):
+        """attrgetter serializer"""
+        class Dummy(object):
+            def __init__(self, attrs, index=None):
+                self.attrs = attrs
+                self.index = index
+            def __getattribute__(self, item):
+                attrs = object.__getattribute__(self, "attrs")
+                index = object.__getattribute__(self, "index")
+                if index is None:
+                    index = len(attrs)
+                    attrs.append(item)
+                else:
+                    attrs[index] = ".".join([attrs[index], item])
+                return type(self)(attrs, index)
+        attrs = []
+        obj(Dummy(attrs))
+        return self.save_reduce(operator.attrgetter, tuple(attrs))
+
+    if type(operator.attrgetter) is type:
+        dispatch[operator.attrgetter] = save_attrgetter
+
+    def save_reduce(self, func, args, state=None,
+                    listitems=None, dictitems=None, obj=None):
+        """Modified to support __transient__ on new objects
+        Change only affects protocol level 2 (which is always used by PiCloud"""
+        # Assert that args is a tuple or None
+        if not isinstance(args, types.TupleType):
+            raise pickle.PicklingError("args from reduce() should be a tuple")
+
+        # Assert that func is callable
+        if not hasattr(func, '__call__'):
+            raise pickle.PicklingError("func from reduce should be callable")
+
+        save = self.save
+        write = self.write
+
+        # Protocol 2 special case: if func's name is __newobj__, use NEWOBJ
+        if self.proto >= 2 and getattr(func, "__name__", "") == "__newobj__":
+            #Added fix to allow transient
+            cls = args[0]
+            if not hasattr(cls, "__new__"):
+                raise pickle.PicklingError(
+                    "args[0] from __newobj__ args has no __new__")
+            if obj is not None and cls is not obj.__class__:
+                raise pickle.PicklingError(
+                    "args[0] from __newobj__ args has the wrong class")
+            args = args[1:]
+            save(cls)
+
+            #Don't pickle transient entries
+            if hasattr(obj, '__transient__'):
+                transient = obj.__transient__
+                state = state.copy()
+
+                for k in list(state.keys()):
+                    if k in transient:
+                        del state[k]
+
+            save(args)
+            write(pickle.NEWOBJ)
+        else:
+            save(func)
+            save(args)
+            write(pickle.REDUCE)
+
+        if obj is not None:
+            self.memoize(obj)
+
+        # More new special cases (that work with older protocols as
+        # well): when __reduce__ returns a tuple with 4 or 5 items,
+        # the 4th and 5th item should be iterators that provide list
+        # items and dict items (as (key, value) tuples), or None.
+
+        if listitems is not None:
+            self._batch_appends(listitems)
+
+        if dictitems is not None:
+            self._batch_setitems(dictitems)
+
+        if state is not None:
+            #print 'obj %s has state %s' % (obj, state)
+            save(state)
+            write(pickle.BUILD)
+
+
+    def save_xrange(self, obj):
+        """Save an xrange object in python 2.5
+        Python 2.6 supports this natively
+        """
+        range_params = xrange_params(obj)
+        self.save_reduce(_build_xrange,range_params)
 
-# TODO(kszucs): use ibis.expr.analysis.substitute instead
-def propagate_down_window(func: ops.Value, frame: ops.WindowFrame):
-    import ibis.expr.operations as ops
-
-    clean_args = []
-    for arg in func.args:
-        if isinstance(arg, ops.Value) and not isinstance(func, ops.WindowFunction):
-            arg = propagate_down_window(arg, frame)
-            if isinstance(arg, ops.Analytic):
-                arg = ops.WindowFunction(arg, frame)
-        clean_args.append(arg)
-
-    return type(func)(*clean_args)
-
-
-# TODO(kszucs): rewrite to receive and return an ops.Node
-def windowize_function(expr, frame):
-    assert isinstance(expr, ir.Expr), type(expr)
-    assert isinstance(frame, ops.WindowFrame)
-
-    def _windowize(op, frame):
-        if isinstance(op, ops.WindowFunction):
-            walked_child = _walk(op.func, frame)
-            walked = walked_child.to_expr().over(op.frame).op()
-        elif isinstance(op, ops.Value):
-            walked = _walk(op, frame)
-        else:
-            walked = op
-
-        if isinstance(walked, (ops.Analytic, ops.Reduction)):
-            return op.to_expr().over(frame).op()
-        elif isinstance(walked, ops.WindowFunction):
-            if frame is not None:
-                frame = walked.frame.copy(
-                    group_by=frame.group_by + walked.frame.group_by,
-                    order_by=frame.order_by + walked.frame.order_by,
-                )
-                return walked.to_expr().over(frame).op()
-            else:
-                return walked
+    #python2.6+ supports xrange pickling. some py2.5 extensions might as well.  We just test it
+    try:
+        xrange(0).__reduce__()
+    except TypeError: #can't pickle -- use PiCloud pickler
+        dispatch[xrange] = save_xrange
+
+    def save_partial(self, obj):
+        """Partial objects do not serialize correctly in python2.x -- this fixes the bugs"""
+        self.save_reduce(_genpartial, (obj.func, obj.args, obj.keywords))
+
+    if sys.version_info < (2,7): #2.7 supports partial pickling
+        dispatch[partial] = save_partial
+
+
+    def save_file(self, obj):
+        """Save a file"""
+        import StringIO as pystringIO #we can't use cStringIO as it lacks the name attribute
+
+        if not hasattr(obj, 'name') or  not hasattr(obj, 'mode'):
+            raise pickle.PicklingError("Cannot pickle files that do not map to an actual file")
+        if obj is sys.stdout:
+            return self.save_reduce(getattr, (sys,'stdout'), obj=obj)
+        if obj is sys.stderr:
+            return self.save_reduce(getattr, (sys,'stderr'), obj=obj)
+        if obj is sys.stdin:
+            raise pickle.PicklingError("Cannot pickle standard input")
+        if  hasattr(obj, 'isatty') and obj.isatty():
+            raise pickle.PicklingError("Cannot pickle files that map to tty objects")
+        if 'r' not in obj.mode:
+            raise pickle.PicklingError("Cannot pickle files that are not opened for reading")
+        name = obj.name
+        try:
+            fsize = os.stat(name).st_size
+        except OSError:
+            raise pickle.PicklingError("Cannot pickle file %s as it cannot be stat" % name)
+
+        if obj.closed:
+            #create an empty closed string io
+            retval = pystringIO.StringIO("")
+            retval.close()
+        elif not fsize: #empty file
+            retval = pystringIO.StringIO("")
+            try:
+                tmpfile = file(name)
+                tst = tmpfile.read(1)
+            except IOError:
+                raise pickle.PicklingError("Cannot pickle file %s as it cannot be read" % name)
+            tmpfile.close()
+            if tst != '':
+                raise pickle.PicklingError("Cannot pickle file %s as it does not appear to map to a physical, real file" % name)
         else:
-            return walked
-
-    def _walk(op, frame):
-        # TODO(kszucs): rewrite to use the substitute utility
-        windowed_args = []
-        for arg in op.args:
-            if isinstance(arg, ops.Value):
-                arg = _windowize(arg, frame)
-            elif isinstance(arg, tuple):
-                arg = tuple(_windowize(x, frame) for x in arg)
-
-            windowed_args.append(arg)
-
-        return type(op)(*windowed_args)
-
-    return _windowize(expr.op(), frame).to_expr()
-
-
-def contains_first_or_last_agg(exprs):
-    def fn(node: ops.Node) -> tuple[bool, bool | None]:
-        if not isinstance(node, ops.Value):
-            return g.halt, None
-        return g.proceed, isinstance(node, (ops.First, ops.Last))
-
-    return any(g.traverse(fn, exprs))
-
-
-def simplify_aggregation(agg):
-    def _pushdown(nodes):
-        subbed = []
-        for node in nodes:
-            subbed.append(sub_for(node, {agg.table: agg.table.table}))
-
-        # TODO(kszucs): perhaps this validation could be omitted
-        if subbed:
-            valid = shares_all_roots(subbed, agg.table.table)
-        else:
-            valid = True
-
-        return valid, subbed
-
-    table = agg.table
-    if (
-        isinstance(table, ops.Selection)
-        and not table.selections
-        # more aggressive than necessary, a better solution would be to check
-        # whether the selections have any order sensitive aggregates that
-        # *depend on* the sort_keys
-        and not (table.sort_keys or contains_first_or_last_agg(table.selections))
-    ):
-        metrics_valid, lowered_metrics = _pushdown(agg.metrics)
-        by_valid, lowered_by = _pushdown(agg.by)
-        having_valid, lowered_having = _pushdown(agg.having)
-
-        if metrics_valid and by_valid and having_valid:
-            valid_lowered_sort_keys = frozenset(lowered_metrics).union(lowered_by)
-            return ops.Aggregation(
-                table.table,
-                lowered_metrics,
-                by=lowered_by,
-                having=lowered_having,
-                predicates=agg.table.predicates,
-                # only the sort keys that exist as grouping keys or metrics can
-                # be included
-                sort_keys=[
-                    key
-                    for key in agg.table.sort_keys
-                    if key.expr in valid_lowered_sort_keys
-                ],
-            )
-
-    return agg
-
-
-class Projector:
-    """Analysis and validation of projection operation.
-
-    This pass tries to take advantage of projection fusion opportunities where
-    they exist, i.e. combining compatible projections together rather than
-    nesting them.
-
-    Translation / evaluation later will not attempt to do any further fusion /
-    simplification.
-    """
-
-    def __init__(self, parent, proj_exprs):
-        # TODO(kszucs): rewrite projector to work with operations exclusively
-        proj_exprs = util.promote_list(proj_exprs)
-        self.parent = parent
-        self.input_exprs = proj_exprs
-        self.resolved_exprs = [parent._ensure_expr(e) for e in proj_exprs]
-
-        default_frame = ops.RowsWindowFrame(table=parent)
-        self.clean_exprs = [
-            windowize_function(expr, frame=default_frame)
-            for expr in self.resolved_exprs
-        ]
-
-    def get_result(self):
-        roots = find_immediate_parent_tables(self.parent.op())
-        first_root = roots[0]
-
-        if len(roots) == 1 and isinstance(first_root, ops.Selection):
-            fused_op = self.try_fusion(first_root)
-            if fused_op is not None:
-                return fused_op
-
-        return ops.Selection(self.parent, self.clean_exprs)
-
-    def try_fusion(self, root):
-        assert self.parent.op() == root
-
-        root_table = root.table
-        root_table_expr = root_table.to_expr()
-        roots = find_immediate_parent_tables(root_table)
-        fused_exprs = []
-        clean_exprs = self.clean_exprs
-
-        if not isinstance(root_table, ops.Join):
             try:
-                resolved = [
-                    root_table_expr._ensure_expr(expr) for expr in self.input_exprs
-                ]
-            except (AttributeError, IbisTypeError):
-                resolved = clean_exprs
-            else:
-                # if any expressions aren't exactly equivalent then don't try
-                # to fuse them
-                if any(
-                    not res_root_root.equals(res_root)
-                    for res_root_root, res_root in zip(resolved, clean_exprs)
-                ):
-                    return None
-        else:
-            # joins cannot be used to resolve expressions, but we still may be
-            # able to fuse columns from a projection off of a join. In that
-            # case, use the projection's input expressions as the columns with
-            # which to attempt fusion
-            resolved = clean_exprs
-
-        root_selections = root.selections
-        parent_op = self.parent.op()
-        for val in resolved:
-            # a * projection
-            if isinstance(val, ir.Table) and (
-                parent_op.equals(val.op())
-                # gross we share the same table root. Better way to
-                # detect?
-                or len(roots) == 1
-                and find_immediate_parent_tables(val.op())[0] == roots[0]
-            ):
-                have_root = False
-                for root_sel in root_selections:
-                    # Don't add the * projection twice
-                    if root_sel.equals(root_table):
-                        fused_exprs.append(root_table)
-                        have_root = True
-                        continue
-                    fused_exprs.append(root_sel)
-
-                # This was a filter, so implicitly a select *
-                if not have_root and not root_selections:
-                    fused_exprs = [root_table, *fused_exprs]
-            elif shares_all_roots(val.op(), root_table):
-                fused_exprs.append(val)
-            else:
-                return None
-
-        return ops.Selection(
-            root_table,
-            fused_exprs,
-            predicates=root.predicates,
-            sort_keys=root.sort_keys,
-        )
-
-
-def find_first_base_table(node):
-    def predicate(node):
-        if isinstance(node, ops.TableNode):
-            return g.halt, node
+                tmpfile = file(name)
+                contents = tmpfile.read()
+                tmpfile.close()
+            except IOError:
+                raise pickle.PicklingError("Cannot pickle file %s as it cannot be read" % name)
+            retval = pystringIO.StringIO(contents)
+            curloc = obj.tell()
+            retval.seek(curloc)
+
+        retval.name = name
+        self.save(retval)  #save stringIO
+        self.memoize(obj)
+
+    dispatch[file] = save_file
+    """Special functions for Add-on libraries"""
+
+    def inject_numpy(self):
+        numpy = sys.modules.get('numpy')
+        if not numpy or not hasattr(numpy, 'ufunc'):
+            return
+        self.dispatch[numpy.ufunc] = self.__class__.save_ufunc
+
+    numpy_tst_mods = ['numpy', 'scipy.special']
+    def save_ufunc(self, obj):
+        """Hack function for saving numpy ufunc objects"""
+        name = obj.__name__
+        for tst_mod_name in self.numpy_tst_mods:
+            tst_mod = sys.modules.get(tst_mod_name, None)
+            if tst_mod:
+                if name in tst_mod.__dict__:
+                    self.save_reduce(_getobject, (tst_mod_name, name))
+                    return
+        raise pickle.PicklingError('cannot save %s. Cannot resolve what module it is defined in' % str(obj))
+
+    def inject_email(self):
+        """Block email LazyImporters from being saved"""
+        email = sys.modules.get('email')
+        if not email:
+            return
+        self.dispatch[email.LazyImporter] = self.__class__.save_unsupported
+
+    def inject_addons(self):
+        """Plug in system. Register additional pickling functions if modules already loaded"""
+        self.inject_numpy()
+        self.inject_email()
+
+    """Python Imaging Library"""
+    def save_image(self, obj):
+        if not obj.im and obj.fp and 'r' in obj.fp.mode and obj.fp.name \
+            and not obj.fp.closed and (not hasattr(obj, 'isatty') or not obj.isatty()):
+            #if image not loaded yet -- lazy load
+            self.save_reduce(_lazyloadImage,(obj.fp,), obj=obj)
         else:
-            return g.proceed, None
+            #image is loaded - just transmit it over
+            self.save_reduce(_generateImage, (obj.size, obj.mode, obj.tostring()), obj=obj)
 
-    try:
-        return next(g.traverse(predicate, node))
-    except StopIteration:
-        return None
-
-
-def _find_projections(node):
-    if isinstance(node, ops.Selection):
-        # remove predicates and sort_keys, so that child tables are considered
-        # equivalent even if their predicates and sort_keys are not
-        return g.proceed, node._projection
-    elif isinstance(node, ops.SelfReference):
-        return g.proceed, node
-    elif isinstance(node, ops.Join):
-        return g.proceed, None
-    elif isinstance(node, ops.TableNode):
-        return g.halt, node
-    elif isinstance(node, ops.Contains):
-        return [node.value], None
-    else:
-        return g.proceed, None
-
-
-def shares_all_roots(exprs, parents):
-    # unique table dependencies of exprs and parents
-    exprs_deps = set(g.traverse(_find_projections, exprs))
-    parents_deps = set(g.traverse(_find_projections, parents))
-    return exprs_deps <= parents_deps
-
-
-def shares_some_roots(exprs, parents):
-    # unique table dependencies of exprs and parents
-    exprs_deps = set(g.traverse(_find_projections, exprs))
-    parents_deps = set(g.traverse(_find_projections, parents))
-    # Also return True if exprs has no roots (e.g. literal-only expressions)
-    return bool(exprs_deps & parents_deps) or not exprs_deps
-
-
-def flatten_predicate(node):
-    """Yield the expressions corresponding to the `And` nodes of a predicate.
-
-    Examples
-    --------
-    >>> import ibis
-    >>> t = ibis.table([('a', 'int64'), ('b', 'string')], name='t')
-    >>> filt = (t.a == 1) & (t.b == 'foo')
-    >>> predicates = flatten_predicate(filt.op())
-    >>> len(predicates)
-    2
-    >>> predicates[0].to_expr().name("left")
-    r0 := UnboundTable: t
-      a int64
-      b string
-    left: r0.a == 1
-    >>> predicates[1].to_expr().name("right")
-    r0 := UnboundTable: t
-      a int64
-      b string
-    right: r0.b == 'foo'
     """
-
-    def predicate(node):
-        if isinstance(node, ops.And):
-            return g.proceed, None
-        else:
-            return g.halt, node
-
-    return list(g.traverse(predicate, node))
-
-
-def is_analytic(node):
-    def predicate(node):
-        if isinstance(node, (ops.Reduction, ops.Analytic)):
-            return g.halt, True
-        else:
-            return g.proceed, None
-
-    return any(g.traverse(predicate, node))
-
-
-def is_reduction(node):
-    """Check whether an expression contains a reduction or not.
-
-    Aggregations yield typed scalar expressions, since the result of an
-    aggregation is a single value. When creating an table expression
-    containing a GROUP BY equivalent, we need to be able to easily check
-    that we are looking at the result of an aggregation.
-
-    As an example, the expression we are looking at might be something
-    like: foo.sum().log10() + bar.sum().log10()
-
-    We examine the operator DAG in the expression to determine if there
-    are aggregations present.
-
-    A bound aggregation referencing a separate table is a "false
-    aggregation" in a GROUP BY-type expression and should be treated a
-    literal, and must be computed as a separate query and stored in a
-    temporary variable (or joined, for bound aggregations with keys)
+    def memoize(self, obj):
+        pickle.Pickler.memoize(self, obj)
+        if printMemoization:
+            print 'memoizing ' + str(obj)
     """
 
-    def predicate(node):
-        if isinstance(node, ops.Reduction):
-            return g.halt, True
-        elif isinstance(node, ops.TableNode):
-            # don't go below any table nodes
-            return g.halt, None
-        else:
-            return g.proceed, None
 
-    return any(g.traverse(predicate, node))
 
+# Shorthands for legacy support
 
-def is_scalar_reduction(node):
-    assert isinstance(node, ops.Node), type(node)
-    return node.output_shape.is_scalar() and is_reduction(node)
-
-
-_ANY_OP_MAPPING = {
-    ops.Any: ops.UnresolvedExistsSubquery,
-    ops.NotAny: ops.UnresolvedNotExistsSubquery,
-}
-
-
-def find_predicates(node, flatten=True):
-    # TODO(kszucs): consider to remove flatten argument and compose with
-    # flatten_predicates instead
-    def predicate(node):
-        assert isinstance(node, ops.Node), type(node)
-        if isinstance(node, ops.Value) and node.output_dtype.is_boolean():
-            if flatten and isinstance(node, ops.And):
-                return g.proceed, None
-            else:
-                return g.halt, node
-        return g.proceed, None
-
-    return list(g.traverse(predicate, node))
-
-
-def find_subqueries(node: ops.Node, min_dependents=1) -> tuple[ops.Node, ...]:
-    subquery_dependents = defaultdict(set)
-    for n in filter(None, util.promote_list(node)):
-        dependents = g.Graph.from_dfs(n).invert()
-        for u, vs in dependents.toposort().items():
-            # count the number of table-node dependents on the current node
-            # but only if the current node is a selection or aggregation
-            if isinstance(u, (rels.Projection, rels.Aggregation, rels.Limit)):
-                subquery_dependents[u].update(vs)
-
-    return tuple(
-        node
-        for node, dependents in reversed(subquery_dependents.items())
-        if len(dependents) >= min_dependents
-    )
-
-
-# TODO(kszucs): move to types/logical.py
-def _make_any(
-    expr,
-    any_op_class: type[ops.Any] | type[ops.NotAny],
-    *,
-    where: ir.BooleanValue | None = None,
-):
-    assert isinstance(expr, ir.Expr), type(expr)
-
-    tables = find_immediate_parent_tables(expr.op())
-    predicates = find_predicates(expr.op(), flatten=True)
-
-    if len(tables) > 1:
-        op = _ANY_OP_MAPPING[any_op_class](
-            tables=[t.to_expr() for t in tables],
-            predicates=predicates,
-        )
-    else:
-        op = any_op_class(expr, where=where)
-    return op.to_expr()
-
+def dump(obj, file, protocol=2):
+    CloudPickler(file, protocol).dump(obj)
 
-# TODO(kszucs): use substitute instead
-@functools.singledispatch
-def _rewrite_filter(op, **kwargs):
-    raise NotImplementedError(type(op))
+def dumps(obj, protocol=2):
+    file = StringIO()
 
+    cp = CloudPickler(file,protocol)
+    cp.dump(obj)
 
-@_rewrite_filter.register(ops.Reduction)
-def _rewrite_filter_reduction(op, name: str | None = None, **kwargs):
-    """Turn a reduction inside of a filter into an aggregate."""
-    # TODO: what about reductions that reference a join that isn't visible at
-    # this level? Means we probably have the wrong design, but will have to
-    # revisit when it becomes a problem.
+    #print 'cloud dumped', str(obj), str(cp.modules)
 
-    if name is not None:
-        op = ops.Alias(op, name=name)
-    aggregation = reduction_to_aggregation(op)
-    return ops.TableArrayView(aggregation)
+    return file.getvalue()
 
 
-@_rewrite_filter.register(ops.Any)
-@_rewrite_filter.register(ops.TableColumn)
-@_rewrite_filter.register(ops.Literal)
-@_rewrite_filter.register(ops.ExistsSubquery)
-@_rewrite_filter.register(ops.NotExistsSubquery)
-@_rewrite_filter.register(ops.WindowFunction)
-def _rewrite_filter_subqueries(op, **kwargs):
-    """Don't rewrite any of these operations in filters."""
-    return op
+#hack for __import__ not working as desired
+def subimport(name):
+    __import__(name)
+    return sys.modules[name]
 
+# restores function attributes
+def _restore_attr(obj, attr):
+    for key, val in attr.items():
+        setattr(obj, key, val)
+    return obj
 
-@_rewrite_filter.register(ops.Alias)
-def _rewrite_filter_alias(op, name: str | None = None, **kwargs):
-    """Rewrite filters on aliases."""
-    return _rewrite_filter(
-        op.arg,
-        name=name if name is not None else op.name,
-        **kwargs,
-    )
+def _get_module_builtins():
+    return pickle.__builtins__
 
+def print_exec(stream):
+    ei = sys.exc_info()
+    traceback.print_exception(ei[0], ei[1], ei[2], None, stream)
 
-@_rewrite_filter.register(ops.Value)
-def _rewrite_filter_value(op, **kwargs):
-    """Recursively apply filter rewriting on operations."""
+def _modules_to_main(modList):
+    """Force every module in modList to be placed into main"""
+    if not modList:
+        return
 
-    visited = [
-        _rewrite_filter(arg, **kwargs) if isinstance(arg, ops.Node) else arg
-        for arg in op.args
-    ]
-    if all(map(operator.is_, visited, op.args)):
-        return op
-
-    return op.__class__(*visited)
-
-
-@_rewrite_filter.register(tuple)
-def _rewrite_filter_value_list(op, **kwargs):
-    visited = [
-        _rewrite_filter(arg, **kwargs) if isinstance(arg, ops.Node) else arg
-        for arg in op.args
-    ]
-
-    if all(map(operator.is_, visited, op.args)):
-        return op
-
-    return op.__class__(*visited)
-
-
-def find_memtables(node: ops.Node) -> Iterator[ops.InMemoryTable]:
-    """Find all in-memory tables in `node`."""
-
-    def finder(node):
-        return g.proceed, node if isinstance(node, ops.InMemoryTable) else None
-
-    return g.traverse(finder, node, filter=ops.Node)
-
-
-def find_toplevel_unnest_children(nodes: Iterable[ops.Node]) -> Iterator[ops.Table]:
-    def finder(node):
-        return (
-            isinstance(node, ops.Value),
-            find_first_base_table(node) if isinstance(node, ops.Unnest) else None,
-        )
-
-    return g.traverse(finder, nodes, filter=ops.Node)
-
-
-def find_toplevel_aggs(nodes: Iterable[ops.Node]) -> Iterator[ops.Table]:
-    def finder(node):
-        return (
-            isinstance(node, ops.Value),
-            node if isinstance(node, ops.Reduction) else None,
-        )
+    main = sys.modules['__main__']
+    for modname in modList:
+        if type(modname) is str:
+            try:
+                mod = __import__(modname)
+            except Exception as i: #catch all...
+                sys.stderr.write('warning: could not import %s\n.  Your function may unexpectedly error due to this import failing; \
+A version mismatch is likely.  Specific error was:\n' % modname)
+                print_exec(sys.stderr)
+            else:
+                setattr(main,mod.__name__, mod)
+        else:
+            #REVERSE COMPATIBILITY FOR CLOUD CLIENT 1.5 (WITH EPD)
+            #In old version actual module was sent
+            setattr(main,modname.__name__, modname)
+
+#object generators:
+def _build_xrange(start, step, len):
+    """Built xrange explicitly"""
+    return xrange(start, start + step*len, step)
+
+def _genpartial(func, args, kwds):
+    if not args:
+        args = ()
+    if not kwds:
+        kwds = {}
+    return partial(func, *args, **kwds)
+
+def _fill_function(func, globals, defaults, dict):
+    """ Fills in the rest of function data into the skeleton function object
+        that were created via _make_skel_func().
+         """
+    func.func_globals.update(globals)
+    func.func_defaults = defaults
+    func.func_dict = dict
+
+    return func
+
+def _make_cell(value):
+    return (lambda: value).func_closure[0]
+
+def _reconstruct_closure(values):
+    return tuple([_make_cell(v) for v in values])
+
+def _make_skel_func(code, closures, base_globals = None):
+    """ Creates a skeleton function object that contains just the provided
+        code and the correct number of cells in func_closure.  All other
+        func attributes (e.g. func_globals) are empty.
+    """
+    closure = _reconstruct_closure(closures) if closures else None
 
-    return g.traverse(finder, nodes, filter=ops.Node)
+    if base_globals is None:
+        base_globals = {}
+    base_globals['__builtins__'] = __builtins__
+
+    return types.FunctionType(code, base_globals,
+                              None, None, closure)
+
+
+"""Constructors for 3rd party libraries
+Note: These can never be renamed due to client compatibility issues"""
+
+def _getobject(modname, attribute):
+    mod = __import__(modname, fromlist=[attribute])
+    return mod.__dict__[attribute]
+
+def _generateImage(size, mode, str_rep):
+    """Generate image from string representation"""
+    import Image
+    i = Image.new(mode, size)
+    i.fromstring(str_rep)
+    return i
+
+def _lazyloadImage(fp):
+    import Image
+    fp.seek(0)  #works in almost any case
+    return Image.open(fp)
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/expr/datatypes/core.py` & `ibis-framework-v0.6.0/ibis/expr/types.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,1088 +1,1146 @@
-from __future__ import annotations
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+import datetime
+import six
+
+from ibis.common import IbisError, RelationError
+import ibis.common as com
+import ibis.compat as compat
+import ibis.config as config
+import ibis.util as util
 
-import datetime as pydatetime
-import decimal as pydecimal
-import numbers
-import uuid as pyuuid
-from abc import abstractmethod
-from collections.abc import Iterator, Mapping, Sequence
-from numbers import Integral, Real
-from typing import Any, Iterable, Literal, NamedTuple, Optional
-
-import toolz
-from public import public
-from typing_extensions import Self, get_args, get_origin, get_type_hints
-
-from ibis.common.annotations import attribute
-from ibis.common.collections import FrozenDict, MapSet
-from ibis.common.dispatch import lazy_singledispatch
-from ibis.common.grounds import Concrete, Singleton
-from ibis.common.temporal import IntervalUnit, TimestampUnit
-from ibis.common.validators import Coercible
-
-
-@lazy_singledispatch
-def dtype(value: Any, nullable: bool = True) -> DataType:
-    """Create a DataType object.
 
-    Parameters
-    ----------
-    value
-        The object to coerce to an Ibis DataType. Supported inputs include
-        strings, python type annotations, numpy dtypes, pandas dtypes, and
-        pyarrow types.
-    nullable
-        Whether the type should be nullable. Defaults to True.
-
-    Examples
-    --------
-    >>> import ibis
-    >>> ibis.dtype("int32")
-    Int32(nullable=True)
-    >>> ibis.dtype("array<float>")
-    Array(value_type=Float64(nullable=True), nullable=True)
-
-    DataType objects may also be created from Python types:
-
-    >>> ibis.dtype(int)
-    Int64(nullable=True)
-    >>> ibis.dtype(list[float])
-    Array(value_type=Float64(nullable=True), nullable=True)
-
-    Or other type systems, like numpy/pandas/pyarrow types:
-
-    >>> import pyarrow as pa
-    >>> ibis.dtype(pa.int32())
-    Int32(nullable=True)
+class Parameter(object):
+
+    """
+    Placeholder, to be implemented
     """
-    if isinstance(value, DataType):
-        return value
-    else:
-        return DataType.from_typehint(value)
 
+    pass
 
-@dtype.register(str)
-def from_string(value):
-    return DataType.from_string(value)
 
+# ---------------------------------------------------------------------
 
-@dtype.register("numpy.dtype")
-def from_numpy_dtype(value, nullable=True):
-    return DataType.from_numpy(value, nullable)
 
+class Expr(object):
 
-@dtype.register("pandas.core.dtypes.base.ExtensionDtype")
-def from_pandas_extension_dtype(value, nullable=True):
-    return DataType.from_pandas(value, nullable)
+    """
 
+    """
 
-@dtype.register("pyarrow.lib.DataType")
-def from_pyarrow(value, nullable=True):
-    return DataType.from_pyarrow(value, nullable)
+    def __init__(self, arg):
+        # TODO: all inputs must inherit from a common table API
+        self._arg = arg
+
+    def __repr__(self):
+        if config.options.interactive:
+            try:
+                result = self.execute()
+                return repr(result)
+            except com.TranslationError as e:
+                output = ('Translation to backend failed\n'
+                          'Error message: {0}\n'
+                          'Expression repr follows:\n{1}'
+                          .format(e.args[0], self._repr()))
+                return output
+        else:
+            return self._repr()
 
+    def _repr(self, memo=None):
+        from ibis.expr.format import ExprFormatter
+        return ExprFormatter(self).get_result()
 
-# lock the dispatcher to prevent new types from being registered
-del dtype.register
+    def pipe(self, f, *args, **kwargs):
+        """
+        Generic composition function to enable expression pipelining
 
+        >>> (expr
+             .pipe(f, *args, **kwargs)
+             .pipe(g, *args2, **kwargs2))
 
-@public
-class DataType(Concrete, Coercible):
-    """Base class for all data types.
+        is equivalent to
 
-    [`DataType`][ibis.expr.datatypes.DataType] instances are immutable.
-    """
+        >>> g(f(expr, *args, **kwargs), *args2, **kwargs2)
 
-    nullable: bool = True
+        Parameters
+        ----------
+        f : function or (function, arg_name) tuple
+          If the expression needs to be passed as anything other than the first
+          argument to the function, pass a tuple with the argument name. For
+          example, (f, 'data') if the function f expects a 'data' keyword
+        args : positional arguments
+        kwargs : keyword arguments
+
+        Examples
+        --------
+        >>> def foo(data, a=None, b=None):
+                pass
+        >>> def bar(a, b, data=None):
+                pass
+        >>> expr.pipe(foo, a=5, b=10)
+        >>> expr.pipe((bar, 'data'), 1, 2)
 
-    # TODO(kszucs): remove it, prefer to use Annotable.__repr__ instead
-    @property
-    def _pretty_piece(self) -> str:
-        return ""
+        Returns
+        -------
+        result : result type of passed function
+        """
+        if isinstance(f, tuple):
+            f, data_keyword = f
+            kwargs = kwargs.copy()
+            kwargs[data_keyword] = self
+            return f(*args, **kwargs)
+        else:
+            return f(self, *args, **kwargs)
+
+    __call__ = pipe
+
+    def op(self):
+        return self._arg
 
-    # TODO(kszucs): should remove it, only used internally
     @property
-    def name(self) -> str:
-        """Return the name of the data type."""
-        return self.__class__.__name__
+    def _factory(self):
+        def factory(arg, name=None):
+            return type(self)(arg, name=name)
+        return factory
 
-    @classmethod
-    def __coerce__(cls, value):
-        return dtype(value)
+    def _can_implicit_cast(self, arg):
+        return False
 
-    def __call__(self, **kwargs):
-        return self.copy(**kwargs)
+    def execute(self, limit='default', async=False):
+        """
+        If this expression is based on physical tables in a database backend,
+        execute it against that backend.
 
-    def __str__(self) -> str:
-        prefix = "!" * (not self.nullable)
-        return f"{prefix}{self.name.lower()}{self._pretty_piece}"
+        Parameters
+        ----------
+        limit : integer or None, default 'default'
+          Pass an integer to effect a specific row limit. limit=None means "no
+          limit". The default is whatever is in ibis.options.
 
-    def equals(self, other):
-        if not isinstance(other, DataType):
-            raise TypeError(
-                f"invalid equality comparison between DataType and {type(other)}"
-            )
-        return super().__cached_equals__(other)
-
-    def cast(self, other, **kwargs):
-        # TODO(kszucs): remove it or deprecate it?
-        from ibis.expr.datatypes.cast import cast
-
-        return cast(self, other, **kwargs)
-
-    def castable(self, other, **kwargs):
-        # TODO(kszucs): remove it or deprecate it?
-        from ibis.expr.datatypes.cast import castable
+        Returns
+        -------
+        result : expression-dependent
+          Result of compiling expression and executing in backend
+        """
+        from ibis.client import execute
+        return execute(self, limit=limit, async=async)
 
-        return castable(self, other, **kwargs)
+    def compile(self, limit=None):
+        """
+        Compile expression to whatever execution target, to verify
 
-    @classmethod
-    def from_string(cls, value) -> Self:
-        from ibis.expr.datatypes.parse import parse
+        Returns
+        -------
+        compiled : value or list
+           query representation or list thereof
+        """
+        from ibis.client import compile
+        return compile(self, limit=limit)
 
+    def verify(self):
+        """
+        Returns True if expression can be compiled to its attached client
+        """
         try:
-            return parse(value)
-        except SyntaxError:
-            raise TypeError(f'{value!r} cannot be parsed as a datatype')
+            self.compile()
+            return True
+        except:
+            return False
 
-    @classmethod
-    def from_typehint(cls, typ, nullable=True) -> Self:
-        origin_type = get_origin(typ)
-        if origin_type is None:
-            if isinstance(typ, type):
-                if issubclass(typ, DataType):
-                    return typ(nullable=nullable)
-                elif typ is type(None):
-                    return null
-                elif issubclass(typ, bool):
-                    return Boolean(nullable=nullable)
-                elif issubclass(typ, bytes):
-                    return Binary(nullable=nullable)
-                elif issubclass(typ, str):
-                    return String(nullable=nullable)
-                elif issubclass(typ, Integral):
-                    return Int64(nullable=nullable)
-                elif issubclass(typ, Real):
-                    return Float64(nullable=nullable)
-                elif issubclass(typ, pydecimal.Decimal):
-                    return Decimal(nullable=nullable)
-                elif issubclass(typ, pydatetime.datetime):
-                    return Timestamp(nullable=nullable)
-                elif issubclass(typ, pydatetime.date):
-                    return Date(nullable=nullable)
-                elif issubclass(typ, pydatetime.time):
-                    return Time(nullable=nullable)
-                elif issubclass(typ, pydatetime.timedelta):
-                    return Interval(unit='us', nullable=nullable)
-                elif issubclass(typ, pyuuid.UUID):
-                    return UUID(nullable=nullable)
-                elif annots := get_type_hints(typ):
-                    return Struct(toolz.valmap(dtype, annots), nullable=nullable)
-                else:
-                    raise TypeError(
-                        f"Cannot construct an ibis datatype from python type `{typ!r}`"
-                    )
-            else:
-                raise TypeError(
-                    f"Cannot construct an ibis datatype from python value `{typ!r}`"
-                )
-        elif issubclass(origin_type, (Sequence, Array)):
-            (value_type,) = map(dtype, get_args(typ))
-            return Array(value_type)
-        elif issubclass(origin_type, (Mapping, Map)):
-            key_type, value_type = map(dtype, get_args(typ))
-            return Map(key_type, value_type)
-        else:
-            raise TypeError(f'Value {typ!r} is not a valid datatype')
+    def equals(self, other):
+        if type(self) != type(other):
+            return False
+        return self._arg.equals(other._arg)
 
-    @classmethod
-    def from_numpy(cls, numpy_type, nullable=True) -> Self:
-        """Return the equivalent ibis datatype."""
-        from ibis.formats.numpy import NumpyType
+    def _can_compare(self, other):
+        return False
 
-        return NumpyType.to_ibis(numpy_type, nullable=nullable)
+    def _root_tables(self):
+        return self.op().root_tables()
 
-    @classmethod
-    def from_pandas(cls, pandas_type, nullable=True) -> Self:
-        """Return the equivalent ibis datatype."""
-        from ibis.formats.pandas import PandasType
+    def _get_unbound_tables(self):
+        # The expression graph may contain one or more tables of a particular
+        # known schema
+        pass
 
-        return PandasType.to_ibis(pandas_type, nullable=nullable)
 
-    @classmethod
-    def from_pyarrow(cls, arrow_type, nullable=True) -> Self:
-        """Return the equivalent ibis datatype."""
-        from ibis.formats.pyarrow import PyArrowType
+def _safe_repr(x, memo=None):
+    return x._repr(memo=memo) if isinstance(x, (Expr, Node)) else repr(x)
 
-        return PyArrowType.to_ibis(arrow_type, nullable=nullable)
 
-    @classmethod
-    def from_dask(cls, dask_type, nullable=True) -> Self:
-        """Return the equivalent ibis datatype."""
-        return cls.from_pandas(dask_type, nullable=nullable)
+class Node(object):
 
-    def to_numpy(self):
-        """Return the equivalent numpy datatype."""
-        from ibis.formats.numpy import NumpyFormat
+    """
+    Node is the base class for all relational algebra and analytical
+    functionality. It transforms the input expressions into an output
+    expression.
+
+    Each node implementation is responsible for validating the inputs,
+    including any type promotion and / or casting issues, and producing a
+    well-typed expression
 
-        return NumpyFormat.from_dtype(self)
+    Note that Node is deliberately not made an expression subclass: think
+    of Node as merely a typed expression builder.
+    """
 
-    def to_pandas(self):
-        """Return the equivalent pandas datatype."""
-        from ibis.formats.pandas import PandasType
+    def __init__(self, args):
+        self.args = args
 
-        return PandasType.from_ibis(self)
+    def __repr__(self):
+        return self._repr()
 
-    def to_pyarrow(self):
-        """Return the equivalent pyarrow datatype."""
-        from ibis.formats.pyarrow import PyArrowType
+    def _repr(self, memo=None):
+        # Quick and dirty to get us started
+        opname = type(self).__name__
+        pprint_args = []
 
-        return PyArrowType.from_ibis(self)
+        memo = memo or {}
 
-    def to_dask(self):
-        """Return the equivalent dask datatype."""
-        return self.to_pandas()
+        if id(self) in memo:
+            return memo[id(self)]
 
-    def is_array(self) -> bool:
-        """Return True if an instance of an Array type."""
-        return isinstance(self, Array)
+        def _pp(x):
+            if isinstance(x, Expr):
+                key = id(x.op())
+            else:
+                key = id(x)
 
-    def is_binary(self) -> bool:
-        """Return True if an instance of a Binary type."""
-        return isinstance(self, Binary)
+            if key in memo:
+                return memo[key]
+            result = _safe_repr(x, memo=memo)
+            memo[key] = result
+            return result
+
+        for x in self.args:
+            if isinstance(x, (tuple, list)):
+                pp = repr([_pp(y) for y in x])
+            else:
+                pp = _pp(x)
+            pprint_args.append(pp)
 
-    def is_boolean(self) -> bool:
-        """Return True if an instance of a Boolean type."""
-        return isinstance(self, Boolean)
+        return '%s(%s)' % (opname, ', '.join(pprint_args))
 
-    def is_date(self) -> bool:
-        """Return True if an instance of a Date type."""
-        return isinstance(self, Date)
+    def flat_args(self):
+        for arg in self.args:
+            if isinstance(arg, (tuple, list)):
+                for x in arg:
+                    yield x
+            else:
+                yield arg
 
-    def is_decimal(self) -> bool:
-        """Return True if an instance of a Decimal type."""
-        return isinstance(self, Decimal)
+    def equals(self, other):
+        if type(self) != type(other):
+            return False
 
-    def is_enum(self) -> bool:
-        """Return True if an instance of an Enum type."""
-        return isinstance(self, Enum)
+        if len(self.args) != len(other.args):
+            return False
 
-    def is_float16(self) -> bool:
-        """Return True if an instance of a Float16 type."""
-        return isinstance(self, Float16)
+        for left, right in zip(self.args, other.args):
+            if not all_equal(left, right):
+                return False
+        return True
 
-    def is_float32(self) -> bool:
-        """Return True if an instance of a Float32 type."""
-        return isinstance(self, Float32)
+    def is_ancestor(self, other):
+        if isinstance(other, Expr):
+            other = other.op()
 
-    def is_float64(self) -> bool:
-        """Return True if an instance of a Float64 type."""
-        return isinstance(self, Float64)
+        return self.equals(other)
 
-    def is_floating(self) -> bool:
-        """Return True if an instance of any Floating type."""
-        return isinstance(self, Floating)
+    def to_expr(self):
+        klass = self.output_type()
+        return klass(self)
 
-    def is_geospatial(self) -> bool:
-        """Return True if an instance of a Geospatial type."""
-        return isinstance(self, GeoSpatial)
+    def output_type(self):
+        """
+        This function must resolve the output type of the expression and return
+        the node wrapped in the appropriate ValueExpr type.
+        """
+        raise NotImplementedError
 
-    def is_inet(self) -> bool:
-        """Return True if an instance of an Inet type."""
-        return isinstance(self, INET)
 
-    def is_int16(self) -> bool:
-        """Return True if an instance of an Int16 type."""
-        return isinstance(self, Int16)
+def all_equal(left, right):
+    if isinstance(left, list):
+        if not isinstance(right, list):
+            return False
+        for a, b in zip(left, right):
+            if not all_equal(a, b):
+                return False
+        return True
 
-    def is_int32(self) -> bool:
-        """Return True if an instance of an Int32 type."""
-        return isinstance(self, Int32)
+    if hasattr(left, 'equals'):
+        return left.equals(right)
+    else:
+        return left == right
+    return True
 
-    def is_int64(self) -> bool:
-        """Return True if an instance of an Int64 type."""
-        return isinstance(self, Int64)
 
-    def is_int8(self) -> bool:
-        """Return True if an instance of an Int8 type."""
-        return isinstance(self, Int8)
+class ValueNode(Node):
 
-    def is_integer(self) -> bool:
-        """Return True if an instance of any Integer type."""
-        return isinstance(self, Integer)
+    def __init__(self, *args):
+        args = self._validate_args(args)
+        Node.__init__(self, args)
 
-    def is_interval(self) -> bool:
-        """Return True if an instance of an Interval type."""
-        return isinstance(self, Interval)
+    def _validate_args(self, args):
+        if not hasattr(self, 'input_type'):
+            return args
 
-    def is_json(self) -> bool:
-        """Return True if an instance of a JSON type."""
-        return isinstance(self, JSON)
+        return self.input_type.validate(args)
 
-    def is_linestring(self) -> bool:
-        """Return True if an instance of a LineString type."""
-        return isinstance(self, LineString)
+    def root_tables(self):
+        exprs = [arg for arg in self.args if isinstance(arg, Expr)]
+        return distinct_roots(*exprs)
 
-    def is_macaddr(self) -> bool:
-        """Return True if an instance of a MACADDR type."""
-        return isinstance(self, MACADDR)
+    def resolve_name(self):
+        raise com.ExpressionError('Expression is not named: %s' % repr(self))
 
-    def is_map(self) -> bool:
-        """Return True if an instance of a Map type."""
-        return isinstance(self, Map)
 
-    def is_multilinestring(self) -> bool:
-        """Return True if an instance of a MultiLineString type."""
-        return isinstance(self, MultiLineString)
+class TableColumn(ValueNode):
 
-    def is_multipoint(self) -> bool:
-        """Return True if an instance of a MultiPoint type."""
-        return isinstance(self, MultiPoint)
+    """
+    Selects a column from a TableExpr
+    """
 
-    def is_multipolygon(self) -> bool:
-        """Return True if an instance of a MultiPolygon type."""
-        return isinstance(self, MultiPolygon)
+    def __init__(self, name, table_expr):
+        Node.__init__(self, [name, table_expr])
 
-    def is_nested(self) -> bool:
-        """Return true if an instance of any nested (Array/Map/Struct) type."""
-        return isinstance(self, (Array, Map, Struct))
+        if name not in table_expr.schema():
+            raise KeyError("'{0}' is not a field".format(name))
 
-    def is_null(self) -> bool:
-        """Return true if an instance of a Null type."""
-        return isinstance(self, Null)
+        self.name = name
+        self.table = table_expr
 
-    def is_numeric(self) -> bool:
-        """Return true if an instance of a Numeric type."""
-        return isinstance(self, Numeric)
+    def parent(self):
+        return self.table
 
-    def is_point(self) -> bool:
-        """Return true if an instance of a Point type."""
-        return isinstance(self, Point)
+    def resolve_name(self):
+        return self.name
 
-    def is_polygon(self) -> bool:
-        """Return true if an instance of a Polygon type."""
-        return isinstance(self, Polygon)
+    def root_tables(self):
+        return self.table._root_tables()
 
-    def is_primitive(self) -> bool:
-        """Return true if an instance of a Primitive type."""
-        return isinstance(self, Primitive)
+    def to_expr(self):
+        ctype = self.table._get_type(self.name)
+        klass = ctype.array_type()
+        return klass(self, name=self.name)
 
-    def is_signed_integer(self) -> bool:
-        """Return true if an instance of a SignedInteger type."""
-        return isinstance(self, SignedInteger)
 
-    def is_string(self) -> bool:
-        """Return true if an instance of a String type."""
-        return isinstance(self, String)
+class ExpressionList(Node):
 
-    def is_struct(self) -> bool:
-        """Return true if an instance of a Struct type."""
-        return isinstance(self, Struct)
+    def __init__(self, exprs):
+        exprs = [as_value_expr(x) for x in exprs]
+        Node.__init__(self, exprs)
 
-    def is_temporal(self) -> bool:
-        """Return true if an instance of a Temporal type."""
-        return isinstance(self, Temporal)
+    def root_tables(self):
+        return distinct_roots(*self.args)
 
-    def is_time(self) -> bool:
-        """Return true if an instance of a Time type."""
-        return isinstance(self, Time)
+    def output_type(self):
+        return ExprList
 
-    def is_timestamp(self) -> bool:
-        """Return true if an instance of a Timestamp type."""
-        return isinstance(self, Timestamp)
 
-    def is_uint16(self) -> bool:
-        """Return true if an instance of a UInt16 type."""
-        return isinstance(self, UInt16)
+class ExprList(Expr):
 
-    def is_uint32(self) -> bool:
-        """Return true if an instance of a UInt32 type."""
-        return isinstance(self, UInt32)
+    def exprs(self):
+        return self.op().args
 
-    def is_uint64(self) -> bool:
-        """Return true if an instance of a UInt64 type."""
-        return isinstance(self, UInt64)
+    def names(self):
+        return [x.get_name() for x in self.exprs()]
 
-    def is_uint8(self) -> bool:
-        """Return true if an instance of a UInt8 type."""
-        return isinstance(self, UInt8)
+    def rename(self, f):
+        new_exprs = [x.name(f(x.get_name())) for x in self.exprs()]
+        return ExpressionList(new_exprs).to_expr()
 
-    def is_unknown(self) -> bool:
-        """Return true if an instance of an Unknown type."""
-        return isinstance(self, Unknown)
+    def prefix(self, value):
+        return self.rename(lambda x: value + x)
 
-    def is_unsigned_integer(self) -> bool:
-        """Return true if an instance of an UnsignedInteger type."""
-        return isinstance(self, UnsignedInteger)
+    def suffix(self, value):
+        return self.rename(lambda x: x + value)
 
-    def is_uuid(self) -> bool:
-        """Return true if an instance of a UUID type."""
-        return isinstance(self, UUID)
+    def concat(self, *others):
+        """
+        Concatenate expression lists
 
-    def is_variadic(self) -> bool:
-        """Return true if an instance of a Variadic type."""
-        return isinstance(self, Variadic)
+        Returns
+        -------
+        combined : ExprList
+        """
+        exprs = list(self.exprs())
+        for o in others:
+            if not isinstance(o, ExprList):
+                raise TypeError(o)
+            exprs.extend(o.exprs())
+        return ExpressionList(exprs).to_expr()
 
 
-@public
-class Unknown(DataType, Singleton):
-    """An unknown type."""
+class Literal(ValueNode):
 
-    scalar = "UnknownScalar"
-    column = "UnknownColumn"
+    def __init__(self, value):
+        self.value = value
 
+    def __repr__(self):
+        return 'Literal(%s)' % repr(self.value)
 
-@public
-class Primitive(DataType, Singleton):
-    """Values with known size."""
+    @property
+    def args(self):
+        return [self.value]
 
+    def equals(self, other):
+        if not isinstance(other, Literal):
+            return False
+        return (isinstance(other.value, type(self.value)) and
+                self.value == other.value)
+
+    def output_type(self):
+        import ibis.expr.rules as rules
+        if isinstance(self.value, bool):
+            klass = BooleanScalar
+        elif isinstance(self.value, compat.integer_types):
+            int_type = rules.int_literal_class(self.value)
+            klass = int_type.scalar_type()
+        elif isinstance(self.value, float):
+            klass = DoubleScalar
+        elif isinstance(self.value, six.string_types):
+            klass = StringScalar
+        elif isinstance(self.value, datetime.datetime):
+            klass = TimestampScalar
+        else:
+            raise com.InputTypeError(self.value)
 
-# TODO(kszucs): consider to remove since we don't actually use this information
-@public
-class Variadic(DataType):
-    """Values with unknown size."""
+        return klass
 
+    def root_tables(self):
+        return []
 
-@public
-class Parametric(DataType):
-    """Types that can be parameterized."""
 
-    def __class_getitem__(cls, params):
-        return cls(*params) if isinstance(params, tuple) else cls(params)
+class TableNode(Node):
 
+    def get_type(self, name):
+        return self.get_schema().get_type(name)
 
-@public
-class Null(Primitive):
-    """Null values."""
+    def to_expr(self):
+        return TableExpr(self)
 
-    scalar = "NullScalar"
-    column = "NullColumn"
 
+class BlockingTableNode(TableNode):
+    # Try to represent the fact that whatever lies here is a semantically
+    # distinct table. Like projections, aggregations, and so forth
+    pass
 
-@public
-class Boolean(Primitive):
-    """[`True`][True] or [`False`][False] values."""
 
-    scalar = "BooleanScalar"
-    column = "BooleanColumn"
+def distinct_roots(*args):
+    all_roots = []
+    for arg in args:
+        all_roots.extend(arg._root_tables())
+    return util.unique_by_key(all_roots, id)
 
 
-@public
-class Bounds(NamedTuple):
-    """The lower and upper bound of a fixed-size value."""
+# ---------------------------------------------------------------------
+# Helper / factory functions
 
-    lower: int
-    upper: int
 
-    def __contains__(self, value: int) -> bool:
-        return self.lower <= value <= self.upper
+class ValueExpr(Expr):
 
+    """
+    Base class for a data generating expression having a fixed and known type,
+    either a single value (scalar)
+    """
 
-@public
-class Numeric(DataType):
-    """Numeric types."""
+    _implicit_casts = set()
 
+    def __init__(self, arg, name=None):
+        Expr.__init__(self, arg)
+        self._name = name
 
-@public
-class Integer(Primitive, Numeric):
-    """Integer values."""
+    def equals(self, other):
+        if not isinstance(other, ValueExpr):
+            return False
 
-    scalar = "IntegerScalar"
-    column = "IntegerColumn"
+        if self._name != other._name:
+            return False
 
-    @property
-    @abstractmethod
-    def nbytes(self) -> int:
-        """Return the number of bytes used to store values of this type."""
+        return Expr.equals(self, other)
 
+    def type(self):
+        import ibis.expr.datatypes as dt
+        return dt._primitive_types[self._typename]
 
-@public
-class String(Variadic, Singleton):
-    """A type representing a string.
+    def _base_type(self):
+        # Parametric types like "decimal"
+        return self.type()
 
-    Notes
-    -----
-    Because of differences in the way different backends handle strings, we
-    cannot assume that strings are UTF-8 encoded.
-    """
+    def _can_cast_implicit(self, typename):
+        from ibis.expr.rules import ImplicitCast
+        rule = ImplicitCast(self.type(), self._implicit_casts)
+        return rule.can_cast(typename)
 
-    scalar = "StringScalar"
-    column = "StringColumn"
+    def get_name(self):
+        if self._name is not None:
+            # This value has been explicitly named
+            return self._name
 
+        # In some but not all cases we can get a name from the node that
+        # produces the value
+        return self.op().resolve_name()
 
-@public
-class Binary(Variadic, Singleton):
-    """A type representing a sequence of bytes.
+    def name(self, name):
+        return self._factory(self._arg, name=name)
 
-    Notes
-    -----
-    Some databases treat strings and blobs of equally, and some do not.
 
-    For example, Impala doesn't make a distinction between string and binary
-    types but PostgreSQL has a `TEXT` type and a `BYTEA` type which are
-    distinct types that have different behavior.
-    """
+class ScalarExpr(ValueExpr):
 
-    scalar = "BinaryScalar"
-    column = "BinaryColumn"
+    pass
 
 
-@public
-class Temporal(DataType):
-    """Data types related to time."""
+class ArrayExpr(ValueExpr):
 
+    def parent(self):
+        return self._arg
 
-@public
-class Date(Temporal, Primitive):
-    """Date values."""
+    def to_projection(self):
+        """
+        Promote this column expression to a table projection
+        """
+        roots = self._root_tables()
+        if len(roots) > 1:
+            raise RelationError('Cannot convert array expression involving '
+                                'multiple base table references to a '
+                                'projection')
 
-    scalar = "DateScalar"
-    column = "DateColumn"
+        table = TableExpr(roots[0])
+        return table.projection([self])
 
 
-@public
-class Time(Temporal, Primitive):
-    """Time values."""
+class AnalyticExpr(Expr):
 
-    scalar = "TimeScalar"
-    column = "TimeColumn"
+    @property
+    def _factory(self):
+        def factory(arg):
+            return type(self)(arg)
+        return factory
 
+    def type(self):
+        return 'analytic'
 
-@public
-class Timestamp(Temporal, Parametric):
-    """Timestamp values."""
 
-    timezone: Optional[str] = None
-    """The timezone of values of this type."""
+class TableExpr(Expr):
 
-    # Literal[*range(10)] is only supported from 3.11
-    scale: Optional[Literal[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]] = None
-    """The scale of the timestamp if known."""
+    @property
+    def _factory(self):
+        def factory(arg):
+            return TableExpr(arg)
+        return factory
+
+    def _assert_valid(self, exprs):
+        from ibis.expr.analysis import ExprValidator
+        ExprValidator([self]).validate_all(exprs)
+
+    def __contains__(self, name):
+        return name in self.schema()
+
+    def __getitem__(self, what):
+        if isinstance(what, six.string_types):
+            return self.get_column(what)
+
+        if isinstance(what, slice):
+            step = what.step
+            if step is not None and step != 1:
+                raise ValueError('Slice step can only be 1')
+            start = what.start or 0
+            stop = what.stop
+
+            if stop is None or stop < 0:
+                raise ValueError('End index must be a positive number')
+
+            if start < 0:
+                raise ValueError('Start index must be a positive number')
+
+            return self.limit(stop - start, offset=start)
+
+        what = bind_expr(self, what)
+
+        if isinstance(what, AnalyticExpr):
+            what = what._table_getitem()
+
+        if isinstance(what, (list, tuple, TableExpr)):
+            # Projection case
+            return self.projection(what)
+        elif isinstance(what, BooleanArray):
+            # Boolean predicate
+            return self.filter([what])
+        elif isinstance(what, ArrayExpr):
+            # Projection convenience
+            return self.projection(what)
+        else:
+            raise NotImplementedError
 
-    scalar = "TimestampScalar"
-    column = "TimestampColumn"
+    def __len__(self):
+        raise com.ExpressionError('Use .count() instead')
 
-    @classmethod
-    def from_unit(cls, unit, timezone=None, nullable=True):
-        """Return a timestamp type with the given unit and timezone."""
-        unit = TimestampUnit(unit)
-        if unit == TimestampUnit.SECOND:
-            scale = 0
-        elif unit == TimestampUnit.MILLISECOND:
-            scale = 3
-        elif unit == TimestampUnit.MICROSECOND:
-            scale = 6
-        elif unit == TimestampUnit.NANOSECOND:
-            scale = 9
+    def __getattr__(self, key):
+        try:
+            return object.__getattribute__(self, key)
+        except AttributeError:
+            if not self._is_materialized() or key not in self.schema():
+                raise
+
+            return self.get_column(key)
+
+    def __dir__(self):
+        attrs = dir(type(self))
+        if self._is_materialized():
+            attrs = list(sorted(set(attrs + self.schema().names)))
+        return attrs
+
+    def _resolve(self, exprs):
+        exprs = util.promote_list(exprs)
+
+        # Stash this helper method here for now
+        out_exprs = []
+        for expr in exprs:
+            expr = self._ensure_expr(expr)
+            out_exprs.append(expr)
+        return out_exprs
+
+    def _ensure_expr(self, expr):
+        if isinstance(expr, six.string_types):
+            return self[expr]
+        elif not isinstance(expr, Expr):
+            return expr(self)
         else:
-            raise ValueError(f"Invalid unit {unit}")
-        return cls(scale=scale, timezone=timezone, nullable=nullable)
+            return expr
 
-    @property
-    def unit(self) -> str:
-        """Return the unit of the timestamp."""
-        if self.scale is None or self.scale == 0:
-            return TimestampUnit.SECOND
-        elif 1 <= self.scale <= 3:
-            return TimestampUnit.MILLISECOND
-        elif 4 <= self.scale <= 6:
-            return TimestampUnit.MICROSECOND
-        elif 7 <= self.scale <= 9:
-            return TimestampUnit.NANOSECOND
-        else:
-            raise ValueError(f"Invalid scale {self.scale}")
+    def _get_type(self, name):
+        return self._arg.get_type(name)
 
-    @property
-    def _pretty_piece(self) -> str:
-        pieces = [
-            repr(piece) for piece in (self.scale, self.timezone) if piece is not None
-        ]
-        return f"({', '.join(pieces)})" * bool(pieces)
+    def get_columns(self, iterable):
+        """
+        Get multiple columns from the table
+
+        Examples
+        --------
+        a, b, c = table.get_columns(['a', 'b', 'c'])
 
+        Returns
+        -------
+        columns : list of column/array expressions
+        """
+        return [self.get_column(x) for x in iterable]
 
-@public
-class SignedInteger(Integer):
-    """Signed integer values."""
+    def get_column(self, name):
+        """
+        Get a reference to a single column from the table
 
-    @property
-    def largest(self):
-        """Return the largest type of signed integer."""
-        return int64
+        Returns
+        -------
+        column : array expression
+        """
+        ref = TableColumn(name, self)
+        return ref.to_expr()
 
     @property
-    def bounds(self):
-        exp = self.nbytes * 8 - 1
-        upper = (1 << exp) - 1
-        return Bounds(lower=~upper, upper=upper)
+    def columns(self):
+        return self.schema().names
 
+    def schema(self):
+        """
+        Get the schema for this table (if one is known)
 
-@public
-class UnsignedInteger(Integer):
-    """Unsigned integer values."""
+        Returns
+        -------
+        schema : Schema
+        """
+        if not self._is_materialized():
+            raise IbisError('Table operation is not yet materialized')
+        return self.op().get_schema()
+
+    def _is_materialized(self):
+        # The operation produces a known schema
+        return self.op().has_schema()
 
-    @property
-    def largest(self):
-        """Return the largest type of unsigned integer."""
-        return uint64
+    def add_column(self, expr, name=None):
+        """
+        Add indicated column expression to table, producing a new table. Note:
+        this is a shortcut for performing a projection having the same effect.
 
-    @property
-    def bounds(self):
-        exp = self.nbytes * 8 - 1
-        upper = 1 << exp
-        return Bounds(lower=0, upper=upper)
+        Returns
+        -------
+        modified_table : TableExpr
+        """
+        expr = self._ensure_expr(expr)
 
+        if not isinstance(expr, ArrayExpr):
+            raise com.InputTypeError('Must pass array expression')
 
-@public
-class Floating(Primitive, Numeric):
-    """Floating point values."""
+        if name is not None:
+            expr = expr.name(name)
 
-    scalar = "FloatingScalar"
-    column = "FloatingColumn"
+        return self.projection([self, expr])
 
-    @property
-    def largest(self):
-        """Return the largest type of floating point values."""
-        return float64
+    def group_by(self, by):
+        """
+        Create an intermediate grouped table expression, pending some group
+        operation to be applied with it.
 
-    @property
-    @abstractmethod
-    def nbytes(self) -> int:  # pragma: no cover
-        ...
+        Examples
+        --------
+        x.group_by([b1, b2]).aggregate(metrics)
 
+        Returns
+        -------
+        grouped_expr : GroupedTableExpr
+        """
+        from ibis.expr.groupby import GroupedTableExpr
+        return GroupedTableExpr(self, by)
 
-@public
-class Int8(SignedInteger):
-    """Signed 8-bit integers."""
 
-    nbytes = 1
+# -----------------------------------------------------------------------------
+# Declare all typed ValueExprs. This is what the user will actually interact
+# with: an instance of each is well-typed and includes all valid methods
+# defined for each type.
 
 
-@public
-class Int16(SignedInteger):
-    """Signed 16-bit integers."""
+class AnyValue(ValueExpr):
 
-    nbytes = 2
+    _typename = 'any'
 
 
-@public
-class Int32(SignedInteger):
-    """Signed 32-bit integers."""
+class NullValue(AnyValue):
 
-    nbytes = 4
+    _typename = 'null'
 
+    def _can_cast_implicit(self, typename):
+        return True
 
-@public
-class Int64(SignedInteger):
-    """Signed 64-bit integers."""
 
-    nbytes = 8
+class NumericValue(AnyValue):
 
+    def _can_compare(self, other):
+        return isinstance(other, NumericValue)
 
-@public
-class UInt8(UnsignedInteger):
-    """Unsigned 8-bit integers."""
 
-    nbytes = 1
+class IntegerValue(NumericValue):
+    pass
 
 
-@public
-class UInt16(UnsignedInteger):
-    """Unsigned 16-bit integers."""
+class BooleanValue(NumericValue):
 
-    nbytes = 2
+    _typename = 'boolean'
 
 
-@public
-class UInt32(UnsignedInteger):
-    """Unsigned 32-bit integers."""
+class Int8Value(IntegerValue):
 
-    nbytes = 4
+    _typename = 'int8'
+    _implicit_casts = set(['int16', 'int32', 'int64', 'float', 'double',
+                           'decimal'])
 
 
-@public
-class UInt64(UnsignedInteger):
-    """Unsigned 64-bit integers."""
+class Int16Value(IntegerValue):
 
-    nbytes = 8
+    _typename = 'int16'
+    _implicit_casts = set(['int32', 'int64', 'float', 'double', 'decimal'])
 
 
-@public
-class Float16(Floating):
-    """16-bit floating point numbers."""
+class Int32Value(IntegerValue):
 
-    nbytes = 2
+    _typename = 'int32'
+    _implicit_casts = set(['int64', 'float', 'double', 'decimal'])
 
 
-@public
-class Float32(Floating):
-    """32-bit floating point numbers."""
+class Int64Value(IntegerValue):
 
-    nbytes = 4
+    _typename = 'int64'
+    _implicit_casts = set(['float', 'double', 'decimal'])
 
 
-@public
-class Float64(Floating):
-    """64-bit floating point numbers."""
+class FloatingValue(NumericValue):
+    pass
 
-    nbytes = 8
 
+class FloatValue(FloatingValue):
 
-@public
-class Decimal(Numeric, Parametric):
-    """Fixed-precision decimal values."""
+    _typename = 'float'
+    _implicit_casts = set(['double', 'decimal'])
 
-    precision: Optional[int] = None
-    """The number of decimal places values of this type can hold."""
 
-    scale: Optional[int] = None
-    """The number of values after the decimal point."""
+class DoubleValue(FloatingValue):
 
-    scalar = "DecimalScalar"
-    column = "DecimalColumn"
+    _typename = 'double'
+    _implicit_casts = set(['decimal'])
 
-    def __init__(
-        self,
-        precision: int | None = None,
-        scale: int | None = None,
-        **kwargs: Any,
-    ) -> None:
-        if precision is not None:
-            if not isinstance(precision, numbers.Integral):
-                raise TypeError(
-                    "Decimal type precision must be an integer; "
-                    f"got {type(precision)}"
-                )
-            if precision < 0:
-                raise ValueError('Decimal type precision cannot be negative')
-            if not precision:
-                raise ValueError('Decimal type precision cannot be zero')
-        if scale is not None:
-            if not isinstance(scale, numbers.Integral):
-                raise TypeError('Decimal type scale must be an integer')
-            if scale < 0:
-                raise ValueError('Decimal type scale cannot be negative')
-            if precision is not None and precision < scale:
-                raise ValueError(
-                    'Decimal type precision must be greater than or equal to '
-                    'scale. Got precision={:d} and scale={:d}'.format(precision, scale)
-                )
-        super().__init__(precision=precision, scale=scale, **kwargs)
 
-    @property
-    def largest(self):
-        """Return the largest type of decimal."""
-        return self.__class__(
-            precision=max(self.precision, 38) if self.precision is not None else None,
-            scale=max(self.scale, 2) if self.scale is not None else None,
-        )
+class StringValue(AnyValue):
 
-    @property
-    def _pretty_piece(self) -> str:
-        precision = self.precision
-        scale = self.scale
-        if precision is None and scale is None:
-            return ""
+    _typename = 'string'
 
-        args = [str(precision) if precision is not None else "_"]
+    def _can_compare(self, other):
+        return isinstance(other, StringValue)
 
-        if scale is not None:
-            args.append(str(scale))
 
-        return f"({', '.join(args)})"
+class DecimalValue(NumericValue):
 
+    _typename = 'decimal'
+    _implicit_casts = set(['float', 'double'])
 
-@public
-class Interval(Parametric):
-    """Interval values."""
+    def __init__(self, meta):
+        self.meta = meta
+        self._precision = meta.precision
+        self._scale = meta.scale
 
-    unit: IntervalUnit
-    """The time unit of the interval."""
+    def type(self):
+        from ibis.expr.datatypes import Decimal
+        return Decimal(self._precision, self._scale)
 
-    scalar = "IntervalScalar"
-    column = "IntervalColumn"
+    def _base_type(self):
+        return 'decimal'
 
-    @property
-    def resolution(self):
-        """The interval unit's name."""
-        return self.unit.singular
+    @classmethod
+    def _make_constructor(cls, meta):
+        def constructor(arg, name=None):
+            return cls(arg, meta, name=name)
+        return constructor
 
-    @property
-    def _pretty_piece(self) -> str:
-        return f"({self.unit!r})"
 
+class TimestampValue(AnyValue):
 
-@public
-class Struct(Parametric, MapSet):
-    """Structured values."""
+    _typename = 'timestamp'
 
-    fields: FrozenDict[str, DataType]
+    def _can_implicit_cast(self, arg):
+        op = arg.op()
+        if isinstance(op, Literal):
+            try:
+                import pandas as pd
+                pd.Timestamp(op.value)
+                return True
+            except ValueError:
+                return False
+        return False
 
-    scalar = "StructScalar"
-    column = "StructColumn"
+    def _can_compare(self, other):
+        return isinstance(other, TimestampValue)
 
-    def __class_getitem__(cls, fields):
-        return cls({slice_.start: slice_.stop for slice_ in fields})
+    def _implicit_cast(self, arg):
+        # assume we've checked this is OK at this point...
+        op = arg.op()
+        return TimestampScalar(op)
 
-    @classmethod
-    def from_tuples(
-        cls, pairs: Iterable[tuple[str, str | DataType]], nullable: bool = True
-    ) -> Struct:
-        """Construct a `Struct` type from pairs.
 
-        Parameters
-        ----------
-        pairs
-            An iterable of pairs of field name and type
-        nullable
-            Whether the type is nullable
+class NumericArray(ArrayExpr, NumericValue):
+    pass
 
-        Returns
-        -------
-        Struct
-            Struct data type instance
-        """
-        return cls(dict(pairs), nullable=nullable)
 
-    @attribute.default
-    def names(self) -> tuple[str, ...]:
-        """Return the names of the struct's fields."""
-        return tuple(self.keys())
+class NullScalar(NullValue, ScalarExpr):
+    """
+    A scalar value expression representing NULL
+    """
+    pass
+
 
-    @attribute.default
-    def types(self) -> tuple[DataType, ...]:
-        """Return the types of the struct's fields."""
-        return tuple(self.values())
+class BooleanScalar(ScalarExpr, BooleanValue):
+    pass
 
-    def __len__(self) -> int:
-        return len(self.fields)
 
-    def __iter__(self) -> Iterator[str]:
-        return iter(self.fields)
+class BooleanArray(NumericArray, BooleanValue):
+    pass
 
-    def __getitem__(self, key: str) -> DataType:
-        return self.fields[key]
 
-    def __repr__(self) -> str:
-        return f"'{self.name}({list(self.items())}, nullable={self.nullable})"
+class Int8Scalar(ScalarExpr, Int8Value):
+    pass
 
-    @property
-    def _pretty_piece(self) -> str:
-        pairs = ", ".join(map("{}: {}".format, self.names, self.types))
-        return f"<{pairs}>"
 
+class Int8Array(NumericArray, Int8Value):
+    pass
 
-@public
-class Array(Variadic, Parametric):
-    """Array values."""
 
-    value_type: DataType
+class Int16Scalar(ScalarExpr, Int16Value):
+    pass
 
-    scalar = "ArrayScalar"
-    column = "ArrayColumn"
 
-    @property
-    def _pretty_piece(self) -> str:
-        return f"<{self.value_type}>"
+class Int16Array(NumericArray, Int16Value):
+    pass
+
+
+class Int32Scalar(ScalarExpr, Int32Value):
+    pass
+
 
+class Int32Array(NumericArray, Int32Value):
+    pass
 
-@public
-class Map(Variadic, Parametric):
-    """Associative array values."""
 
-    key_type: DataType
-    value_type: DataType
+class Int64Scalar(ScalarExpr, Int64Value):
+    pass
 
-    scalar = "MapScalar"
-    column = "MapColumn"
+
+class Int64Array(NumericArray, Int64Value):
+    pass
+
+
+class FloatScalar(ScalarExpr, FloatValue):
+    pass
+
+
+class FloatArray(NumericArray, FloatValue):
+    pass
+
+
+class DoubleScalar(ScalarExpr, DoubleValue):
+    pass
+
+
+class DoubleArray(NumericArray, DoubleValue):
+    pass
+
+
+class StringScalar(ScalarExpr, StringValue):
+    pass
+
+
+class StringArray(ArrayExpr, StringValue):
+    pass
+
+
+class TimestampScalar(ScalarExpr, TimestampValue):
+    pass
+
+
+class TimestampArray(ArrayExpr, TimestampValue):
+    pass
+
+
+class DecimalScalar(DecimalValue, ScalarExpr):
+
+    def __init__(self, arg, meta, name=None):
+        DecimalValue.__init__(self, meta)
+        ScalarExpr.__init__(self, arg, name=name)
 
     @property
-    def _pretty_piece(self) -> str:
-        return f"<{self.key_type}, {self.value_type}>"
+    def _factory(self):
+        def factory(arg, name=None):
+            return DecimalScalar(arg, self.meta, name=name)
+        return factory
+
 
+class DecimalArray(DecimalValue, NumericArray):
 
-@public
-class JSON(Variadic):
-    """JSON values."""
+    def __init__(self, arg, meta, name=None):
+        DecimalValue.__init__(self, meta)
+        ArrayExpr.__init__(self, arg, name=name)
 
-    scalar = "JSONScalar"
-    column = "JSONColumn"
+    @property
+    def _factory(self):
+        def factory(arg, name=None):
+            return DecimalArray(arg, self.meta, name=name)
+        return factory
 
 
-@public
-class GeoSpatial(DataType):
-    """Geospatial values."""
+class CategoryValue(AnyValue):
 
-    geotype: Optional[Literal["geography", "geometry"]] = None
-    """The specific geospatial type."""
+    """
+    Represents some ordered data categorization; tracked as an int32 value
+    until explicitly
+    """
 
-    srid: Optional[int] = None
-    """The spatial reference identifier."""
+    _typename = 'category'
+    _implicit_casts = Int16Value._implicit_casts
 
-    column = "GeoSpatialColumn"
-    scalar = "GeoSpatialScalar"
+    def __init__(self, meta):
+        self.meta = meta
+
+    def type(self):
+        return self.meta
+
+    def _base_type(self):
+        return 'category'
+
+    def _can_compare(self, other):
+        return isinstance(other, IntegerValue)
+
+
+class CategoryScalar(CategoryValue, ScalarExpr):
+
+    def __init__(self, arg, meta, name=None):
+        CategoryValue.__init__(self, meta)
+        ScalarExpr.__init__(self, arg, name=name)
 
     @property
-    def _pretty_piece(self) -> str:
-        piece = ""
-        if self.geotype is not None:
-            piece += f":{self.geotype}"
-        if self.srid is not None:
-            piece += f";{self.srid}"
-        return piece
+    def _factory(self):
+        def factory(arg, name=None):
+            return CategoryScalar(arg, self.meta, name=name)
+        return factory
+
+
+class CategoryArray(CategoryValue, ArrayExpr):
+
+    def __init__(self, arg, meta, name=None):
+        CategoryValue.__init__(self, meta)
+        ArrayExpr.__init__(self, arg, name=name)
+
+    @property
+    def _factory(self):
+        def factory(arg, name=None):
+            return CategoryArray(arg, self.meta, name=name)
+        return factory
+
+
+class UnnamedMarker(object):
+    pass
+
+
+unnamed = UnnamedMarker()
+
+
+def as_value_expr(val):
+    import pandas as pd
+    if not isinstance(val, Expr):
+        if isinstance(val, (tuple, list)):
+            val = sequence(val)
+        elif isinstance(val, pd.Series):
+            val = sequence(list(val))
+        else:
+            val = literal(val)
+
+    return val
+
+
+def literal(value):
+    """
+    Create a scalar expression from a Python value
+
+    Parameters
+    ----------
+    value : some Python basic type
+
+    Returns
+    -------
+    lit_value : value expression, type depending on input value
+    """
+    if value is None or value is null:
+        return null()
+    else:
+        return Literal(value).to_expr()
+
+
+_NULL = None
 
 
-@public
-class Point(GeoSpatial):
-    """A point described by two coordinates."""
+def null():
+    """
+    Create a NULL/NA scalar
+    """
+    global _NULL
+    if _NULL is None:
+        _NULL = NullScalar(NullLiteral())
+
+    return _NULL
 
-    scalar = "PointScalar"
-    column = "PointColumn"
 
+def sequence(values):
+    """
+    Wrap a list of Python values as an Ibis sequence type
 
-@public
-class LineString(GeoSpatial):
-    """A sequence of 2 or more points."""
+    Parameters
+    ----------
+    values : list
+      Should all be None or the same type
 
-    scalar = "LineStringScalar"
-    column = "LineStringColumn"
+    Returns
+    -------
+    seq : Sequence
+    """
+    return ValueList(values).to_expr()
 
 
-@public
-class Polygon(GeoSpatial):
-    """A set of one or more closed line strings.
+class NullLiteral(ValueNode):
 
-    The first line string represents the shape (external ring) and the
-    rest represent holes in that shape (internal rings).
     """
+    Typeless NULL literal
+    """
+
+    def __init__(self):
+        pass
+
+    @property
+    def args(self):
+        return [None]
+
+    def equals(self, other):
+        return isinstance(other, NullLiteral)
+
+    def output_type(self):
+        return NullScalar
+
+    def root_tables(self):
+        return []
 
-    scalar = "PolygonScalar"
-    column = "PolygonColumn"
 
+class ListExpr(ArrayExpr, AnyValue):
+    pass
 
-@public
-class MultiLineString(GeoSpatial):
-    """A set of one or more line strings."""
 
-    scalar = "MultiLineStringScalar"
-    column = "MultiLineStringColumn"
+class SortExpr(Expr):
+    pass
 
 
-@public
-class MultiPoint(GeoSpatial):
-    """A set of one or more points."""
+class ValueList(ValueNode):
 
-    scalar = "MultiPointScalar"
-    column = "MultiPointColumn"
+    """
+    Data structure for a list of value expressions
+    """
 
+    def __init__(self, args):
+        self.values = [as_value_expr(x) for x in args]
+        ValueNode.__init__(self, self.values)
 
-@public
-class MultiPolygon(GeoSpatial):
-    """A set of one or more polygons."""
+    def root_tables(self):
+        return distinct_roots(*self.values)
 
-    scalar = "MultiPolygonScalar"
-    column = "MultiPolygonColumn"
+    def to_expr(self):
+        return ListExpr(self)
 
 
-@public
-class UUID(DataType):
-    """A 128-bit number used to identify information in computer systems."""
+def bind_expr(table, expr):
+    if isinstance(expr, (list, tuple)):
+        return [bind_expr(table, x) for x in expr]
 
-    scalar = "UUIDScalar"
-    column = "UUIDColumn"
+    return table._ensure_expr(expr)
 
 
-@public
-class MACADDR(String):
-    """Media Access Control (MAC) address of a network interface."""
+def find_base_table(expr):
+    if isinstance(expr, TableExpr):
+        return expr
 
-    scalar = "MACADDRScalar"
-    column = "MACADDRColumn"
+    for arg in expr.op().flat_args():
+        if isinstance(arg, Expr):
+            r = find_base_table(arg)
+            if isinstance(r, TableExpr):
+                return r
 
 
-@public
-class INET(String):
-    """IP addresses."""
+def find_all_base_tables(expr, memo=None):
+    if memo is None:
+        memo = {}
 
-    scalar = "INETScalar"
-    column = "INETColumn"
+    node = expr.op()
 
+    if (isinstance(expr, TableExpr) and
+            isinstance(node, BlockingTableNode)):
+        if id(expr) not in memo:
+            memo[id(expr)] = expr
+        return memo
 
-# ---------------------------------------------------------------------
+    for arg in expr.op().flat_args():
+        if isinstance(arg, Expr):
+            find_all_base_tables(arg, memo)
 
-null = Null()
-boolean = Boolean()
-int8 = Int8()
-int16 = Int16()
-int32 = Int32()
-int64 = Int64()
-uint8 = UInt8()
-uint16 = UInt16()
-uint32 = UInt32()
-uint64 = UInt64()
-float16 = Float16()
-float32 = Float32()
-float64 = Float64()
-string = String()
-binary = Binary()
-date = Date()
-time = Time()
-timestamp = Timestamp()
-# geo spatial data type
-geometry = GeoSpatial(geotype="geometry")
-geography = GeoSpatial(geotype="geography")
-point = Point()
-linestring = LineString()
-polygon = Polygon()
-multilinestring = MultiLineString()
-multipoint = MultiPoint()
-multipolygon = MultiPolygon()
-# json
-json = JSON()
-# special string based data type
-uuid = UUID()
-macaddr = MACADDR()
-inet = INET()
-decimal = Decimal()
-unknown = Unknown()
-
-Enum = String
-
-
-public(
-    null=null,
-    boolean=boolean,
-    int8=int8,
-    int16=int16,
-    int32=int32,
-    int64=int64,
-    uint8=uint8,
-    uint16=uint16,
-    uint32=uint32,
-    uint64=uint64,
-    float16=float16,
-    float32=float32,
-    float64=float64,
-    string=string,
-    binary=binary,
-    date=date,
-    time=time,
-    timestamp=timestamp,
-    dtype=dtype,
-    geometry=geometry,
-    geography=geography,
-    point=point,
-    linestring=linestring,
-    polygon=polygon,
-    multilinestring=multilinestring,
-    multipoint=multipoint,
-    multipolygon=multipolygon,
-    json=json,
-    uuid=uuid,
-    macaddr=macaddr,
-    inet=inet,
-    decimal=decimal,
-    unknown=unknown,
-    Enum=Enum,
-    Geography=GeoSpatial,
-    Geometry=GeoSpatial,
-    Set=Array,
-)
+    return memo
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/expr/types/groupby.py` & `ibis-framework-v0.6.0/ibis/expr/window.py`

 * *Files 24% similar despite different names*

```diff
@@ -8,336 +8,229 @@
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-"""User API for grouping operations."""
-
-from __future__ import annotations
-
-import itertools
-import types
-from typing import Iterable, Sequence
-
-import ibis
-import ibis.expr.analysis as an
-import ibis.expr.operations as ops
 import ibis.expr.types as ir
-from ibis import util
-from ibis.expr.deferred import Deferred
-from ibis.selectors import Selector
-from ibis.expr.types.relations import bind_expr
-import ibis.common.exceptions as com
-
-_function_types = tuple(
-    filter(
-        None,
-        (
-            types.BuiltinFunctionType,
-            types.BuiltinMethodType,
-            types.FunctionType,
-            types.LambdaType,
-            types.MethodType,
-            getattr(types, "UnboundMethodType", None),
-        ),
-    )
-)
-
-
-def _get_group_by_key(table, value):
-    if isinstance(value, str):
-        yield table[value]
-    elif isinstance(value, _function_types):
-        yield value(table)
-    elif isinstance(value, Deferred):
-        yield value.resolve(table)
-    elif isinstance(value, Selector):
-        yield from value.expand(table)
-    elif isinstance(value, ir.Expr):
-        yield an.sub_immediate_parents(value.op(), table.op()).to_expr()
-    else:
-        yield value
-
-
-# TODO(kszucs): make a builder class for this
-class GroupedTable:
-    """An intermediate table expression to hold grouping information."""
-
-    def __init__(
-        self, table, by, having=None, order_by=None, window=None, **expressions
-    ):
-        self.table = table
-        self.by = list(
-            itertools.chain(
-                itertools.chain.from_iterable(
-                    _get_group_by_key(table, v) for v in util.promote_list(by)
-                ),
-                (
-                    expr.name(k)
-                    for k, v in expressions.items()
-                    for expr in _get_group_by_key(table, v)
-                ),
-            )
-        )
+import ibis.expr.operations as ops
+import ibis.util as util
+import ibis.common as com
 
-        if not self.by:
-            raise com.IbisInputError("The grouping keys list is empty")
 
-        self._order_by = order_by or []
-        self._having = having or []
-        self._window = window
-
-    def __getitem__(self, args):
-        # Shortcut for projection with window functions
-        return self.select(*args)
-
-    def __getattr__(self, attr):
-        if hasattr(self.table, attr):
-            return self._column_wrapper(attr)
-
-        raise AttributeError("GroupBy has no attribute %r" % attr)
-
-    def _column_wrapper(self, attr):
-        col = self.table[attr]
-        if isinstance(col, ir.NumericValue):
-            return GroupedNumbers(col, self)
+def _list_to_tuple(x):
+    if isinstance(x, list):
+        x = tuple(x)
+    return x
+
+
+class Window(object):
+
+    """
+    A generic window function clause, patterned after SQL window clauses for
+    the time being. Can be expanded to cover more use cases as they arise.
+
+    Using None for preceding or following currently indicates unbounded. Use 0
+    for current_value
+    """
+
+    def __init__(self, group_by=None, order_by=None,
+                 preceding=None, following=None):
+        if group_by is None:
+            group_by = []
+
+        if order_by is None:
+            order_by = []
+
+        self._group_by = util.promote_list(group_by)
+
+        self._order_by = []
+        for x in util.promote_list(order_by):
+            if isinstance(x, ir.SortExpr):
+                pass
+            elif isinstance(x, ir.Expr):
+                x = ops.SortKey(x).to_expr()
+            self._order_by.append(x)
+
+        self.preceding = _list_to_tuple(preceding)
+        self.following = _list_to_tuple(following)
+
+        self._validate_frame()
+
+    def _validate_frame(self):
+        p_tuple = has_p = False
+        f_tuple = has_f = False
+        if self.preceding is not None:
+            p_tuple = isinstance(self.preceding, tuple)
+            has_p = True
+
+        if self.following is not None:
+            f_tuple = isinstance(self.following, tuple)
+            has_f = True
+
+        if ((p_tuple and has_f) or (f_tuple and has_p)):
+            raise com.IbisInputError('Can only specify one window side '
+                                     ' when you want an off-center '
+                                     'window')
+        elif p_tuple:
+            start, end = self.preceding
+            if start is None:
+                assert end >= 0
+            else:
+                assert start > end
+        elif f_tuple:
+            start, end = self.following
+            if end is None:
+                assert start >= 0
+            else:
+                assert start < end
         else:
-            return GroupedArray(col, self)
-
-    def aggregate(self, metrics=None, **kwds) -> ir.Table:
-        """Compute aggregates over a group by."""
-        return self.table.aggregate(metrics, by=self.by, having=self._having, **kwds)
-
-    agg = aggregate
-
-    def having(self, expr: ir.BooleanScalar) -> GroupedTable:
-        """Add a post-aggregation result filter `expr`.
-
-        !!! warning "Expressions like `x is None` return `bool` and **will not** generate a SQL comparison to `NULL`"
-
-        Parameters
-        ----------
-        expr
-            An expression that filters based on an aggregate value.
-
-        Returns
-        -------
-        GroupedTable
-            A grouped table expression
-        """
-        return self.__class__(
-            self.table,
-            self.by,
-            having=self._having + util.promote_list(expr),
-            order_by=self._order_by,
-            window=self._window,
-        )
+            if has_p and self.preceding < 0:
+                raise com.IbisInputError('Window offset must be positive')
 
-    def order_by(self, expr: ir.Value | Iterable[ir.Value]) -> GroupedTable:
-        """Sort a grouped table expression by `expr`.
+            if has_f and self.following < 0:
+                raise com.IbisInputError('Window offset must be positive')
 
-        Notes
-        -----
-        This API call is ignored in aggregations.
-
-        Parameters
-        ----------
-        expr
-            Expressions to order the results by
-
-        Returns
-        -------
-        GroupedTable
-            A sorted grouped GroupedTable
-        """
-        return self.__class__(
-            self.table,
-            self.by,
-            having=self._having,
-            order_by=self._order_by + util.promote_list(expr),
-            window=self._window,
+    def bind(self, table):
+        # Internal API, ensure that any unresolved expr references (as strings,
+        # say) are bound to the table being windowed
+        groups = table._resolve(self._group_by)
+        sorts = [ops.to_sort_key(table, k) for k in self._order_by]
+        return self._replace(group_by=groups, order_by=sorts)
+
+    def combine(self, window):
+        kwds = dict(
+            preceding=self.preceding or window.preceding,
+            following=self.following or window.following,
+            group_by=self._group_by + window._group_by,
+            order_by=self._order_by + window._order_by
         )
+        return Window(**kwds)
 
-    def mutate(
-        self, *exprs: ir.Value | Sequence[ir.Value], **kwexprs: ir.Value
-    ) -> ir.Table:
-        """Return a table projection with window functions applied.
-
-        Any arguments can be functions.
-
-        Parameters
-        ----------
-        exprs
-            List of expressions
-        kwexprs
-            Expressions
-
-        Examples
-        --------
-        >>> import ibis
-        >>> t = ibis.table([
-        ...     ('foo', 'string'),
-        ...     ('bar', 'string'),
-        ...     ('baz', 'double'),
-        ... ], name='t')
-        >>> t
-        UnboundTable: t
-          foo string
-          bar string
-          baz float64
-        >>> expr = (t.group_by('foo')
-        ...          .order_by(ibis.desc('bar'))
-        ...          .mutate(qux=lambda x: x.baz.lag(), qux2=t.baz.lead()))
-        >>> print(expr)
-        r0 := UnboundTable: t
-          foo string
-          bar string
-          baz float64
-        Selection[r0]
-          selections:
-            r0
-            qux:  WindowFunction(...)
-            qux2: WindowFunction(...)
-
-        Returns
-        -------
-        Table
-            A table expression with window functions applied
-        """
-
-        exprs = self._selectables(*exprs, **kwexprs)
-        return self.table.mutate(exprs)
-
-    def select(self, *exprs, **kwexprs) -> ir.Table:
-        """Project new columns out of the grouped table.
-
-        See Also
-        --------
-        [`GroupedTable.mutate`][ibis.expr.types.groupby.GroupedTable.mutate]
-        """
-        exprs = self._selectables(*exprs, **kwexprs)
-        return self.table.select(exprs)
-
-    def _selectables(self, *exprs, **kwexprs):
-        """Project new columns out of the grouped table.
-
-        See Also
-        --------
-        [`GroupedTable.mutate`][ibis.expr.types.groupby.GroupedTable.mutate]
-        """
-        table = self.table
-        default_frame = self._get_window()
-        return [
-            an.windowize_function(e2, frame=default_frame)
-            for expr in exprs
-            for e1 in util.promote_list(expr)
-            for e2 in util.promote_list(table._ensure_expr(e1))
-        ] + [
-            an.windowize_function(e, frame=default_frame).name(k)
-            for k, expr in kwexprs.items()
-            for e in util.promote_list(table._ensure_expr(expr))
-        ]
-
-    projection = select
-
-    def _get_window(self):
-        if self._window is None:
-            return ops.RowsWindowFrame(
-                table=self.table,
-                group_by=self.by,
-                order_by=bind_expr(self.table, self._order_by),
-            )
-        else:
-            return self._window.copy(
-                groupy_by=bind_expr(self.table, self._window.group_by + self.by),
-                order_by=bind_expr(self.table, self._window.order_by + self._order_by),
-            )
-
-    def over(
-        self,
-        window=None,
-        *,
-        rows=None,
-        range=None,
-        group_by=None,
-        order_by=None,
-    ) -> GroupedTable:
-        """Apply a window over the input expressions.
-
-        Parameters
-        ----------
-        window
-            Window to add to the input
-        rows
-            Whether to use the `ROWS` window clause
-        range
-            Whether to use the `RANGE` window clause
-        group_by
-            Grouping key
-        order_by
-            Ordering key
-
-        Returns
-        -------
-        GroupedTable
-            A new grouped table expression
-        """
-        if window is None:
-            window = ibis.window(
-                rows=rows,
-                range=range,
-                group_by=group_by,
-                order_by=order_by,
-            )
-
-        return self.__class__(
-            self.table,
-            self.by,
-            having=self._having,
-            order_by=self._order_by,
-            window=window,
+    def group_by(self, expr):
+        new_groups = self._group_by + util.promote_list(expr)
+        return self._replace(group_by=new_groups)
+
+    def _replace(self, **kwds):
+        new_kwds = dict(
+            group_by=kwds.get('group_by', self._group_by),
+            order_by=kwds.get('order_by', self._order_by),
+            preceding=kwds.get('preceding', self.preceding),
+            following=kwds.get('following', self.following)
         )
+        return Window(**new_kwds)
+
+    def order_by(self, expr):
+        new_sorts = self._order_by + util.promote_list(expr)
+        return self._replace(order_by=new_sorts)
+
+    def equals(self, other):
+        if not isinstance(other, Window):
+            return False
+
+        if (len(self._group_by) != len(other._group_by) or
+                not ir.all_equal(self._group_by, other._group_by)):
+            return False
+
+        if (len(self._order_by) != len(other._order_by) or
+                not ir.all_equal(self._order_by, other._order_by)):
+            return False
+
+        return (self.preceding == other.preceding and
+                self.following == other.following)
+
+
+def window(preceding=None, following=None, group_by=None, order_by=None):
+    """
+    Create a window clause for use with window (analytic and aggregate)
+    functions.
+
+    All window frames / ranges are inclusive.
+
+    Parameters
+    ----------
+    preceding : int, tuple, or None, default None
+      Specify None for unbounded, 0 to include current row
+      tuple for off-center window
+    following : int, tuple, or None, default None
+      Specify None for unbounded, 0 to include current row
+      tuple for off-center window
+    group_by : expressions, default None
+      Either specify here or with TableExpr.group_by
+    order_by : expressions, default None
+      For analytic functions requiring an ordering, specify here, or let Ibis
+      determine the default ordering (for functions like rank)
+
+    Returns
+    -------
+    win : ibis Window
+    """
+    return Window(preceding=preceding, following=following,
+                  group_by=group_by, order_by=order_by)
+
+
+def cumulative_window(group_by=None, order_by=None):
+    """
+    Create a cumulative window clause for use with aggregate window functions.
+
+    All window frames / ranges are inclusive.
+
+    Parameters
+    ----------
+    group_by : expressions, default None
+      Either specify here or with TableExpr.group_by
+    order_by : expressions, default None
+      For analytic functions requiring an ordering, specify here, or let Ibis
+      determine the default ordering (for functions like rank)
+
+    Returns
+    -------
+    win : ibis Window
+    """
+    return Window(preceding=None, following=0,
+                  group_by=group_by, order_by=order_by)
+
+
+def trailing_window(periods, group_by=None, order_by=None):
+    """
+    Create a trailing window for use with aggregate window functions.
+
+    Parameters
+    ----------
+    periods : int
+      Number of trailing periods to include. 0 includes only the current period
+    group_by : expressions, default None
+      Either specify here or with TableExpr.group_by
+    order_by : expressions, default None
+      For analytic functions requiring an ordering, specify here, or let Ibis
+      determine the default ordering (for functions like rank)
+
+    Returns
+    -------
+    win : ibis Window
+    """
+    return Window(preceding=periods, following=0,
+                  group_by=group_by, order_by=order_by)
+
+
+def propagate_down_window(expr, window):
+    op = expr.op()
+
+    clean_args = []
+    unchanged = True
+    for arg in op.args:
+        if (isinstance(arg, ir.Expr) and
+                not isinstance(op, ops.WindowOp)):
+            new_arg = propagate_down_window(arg, window)
+            if isinstance(new_arg.op(), ops.AnalyticOp):
+                new_arg = ops.WindowOp(new_arg, window).to_expr()
+            if arg is not new_arg:
+                unchanged = False
+            arg = new_arg
 
-    def count(self) -> ir.Table:
-        """Computing the number of rows per group.
+        clean_args.append(arg)
 
-        Returns
-        -------
-        Table
-            The aggregated table
-        """
-        metric = self.table.count()
-        return self.table.aggregate([metric], by=self.by, having=self._having)
-
-    size = count
-
-
-def _group_agg_dispatch(name):
-    def wrapper(self, *args, **kwargs):
-        f = getattr(self.arr, name)
-        metric = f(*args, **kwargs)
-        alias = f'{name}({self.arr.get_name()})'
-        return self.parent.aggregate(metric.name(alias))
-
-    wrapper.__name__ = name
-    return wrapper
-
-
-class GroupedArray:
-    def __init__(self, arr, parent):
-        self.arr = arr
-        self.parent = parent
-
-    count = _group_agg_dispatch('count')
-    size = count
-    min = _group_agg_dispatch('min')
-    max = _group_agg_dispatch('max')
-    approx_nunique = _group_agg_dispatch('approx_nunique')
-    approx_median = _group_agg_dispatch('approx_median')
-    group_concat = _group_agg_dispatch('group_concat')
-
-
-class GroupedNumbers(GroupedArray):
-    mean = _group_agg_dispatch('mean')
-    sum = _group_agg_dispatch('sum')
+    if unchanged:
+        return expr
+    else:
+        return type(op)(*clean_args).to_expr()
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/tests/__init__.py` & `ibis-framework-v0.6.0/ibis/tests/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,9 +1,7 @@
-from __future__ import annotations
-
 # Copyright 2015 Cloudera Inc.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 # http://www.apache.org/licenses/LICENSE-2.0
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/tests/expr/test_analysis.py` & `ibis-framework-v0.6.0/ibis/expr/tests/test_analysis.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,307 +1,269 @@
-from __future__ import annotations
-
-import pytest
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import ibis
-import ibis.common.exceptions as com
-import ibis.expr.analysis as an
+
+from ibis.compat import unittest
+from ibis.expr.tests.mocks import BasicTestCase
+import ibis.expr.analysis as L
 import ibis.expr.operations as ops
+import ibis.common as com
+
 from ibis.tests.util import assert_equal
 
-# TODO: test is_reduction
-# TODO: test is_scalar_reduction
 
 # Place to collect esoteric expression analysis bugs and tests
 
 
-def test_rewrite_join_projection_without_other_ops(con):
-    # See #790, predicate pushdown in joins not supported
-
-    # Star schema with fact table
-    table = con.table('star1')
-    table2 = con.table('star2')
-    table3 = con.table('star3')
-
-    filtered = table[table['f'] > 0]
-
-    pred1 = table['foo_id'] == table2['foo_id']
-    pred2 = filtered['bar_id'] == table3['bar_id']
-
-    j1 = filtered.left_join(table2, [pred1])
-    j2 = j1.inner_join(table3, [pred2])
-
-    # Project out the desired fields
-    view = j2[[filtered, table2['value1'], table3['value2']]]
-
-    # Construct the thing we expect to obtain
-    ex_pred2 = table['bar_id'] == table3['bar_id']
-    ex_expr = table.left_join(table2, [pred1]).inner_join(table3, [ex_pred2])
-
-    rewritten_proj = an.substitute_parents(view.op())
-
-    assert not rewritten_proj.table.equals(ex_expr.op())
-
-
-def test_multiple_join_deeper_reference():
-    # Join predicates down the chain might reference one or more root
-    # tables in the hierarchy.
-    table1 = ibis.table({'key1': 'string', 'key2': 'string', 'value1': 'double'})
-    table2 = ibis.table({'key3': 'string', 'value2': 'double'})
-    table3 = ibis.table({'key4': 'string', 'value3': 'double'})
-
-    joined = table1.inner_join(table2, [table1['key1'] == table2['key3']])
-    joined2 = joined.inner_join(table3, [table1['key2'] == table3['key4']])
-
-    # it works, what more should we test here?
-    repr(joined2)
-
-
-def test_filter_on_projected_field(con):
-    # See #173. Impala and other SQL engines do not allow filtering on a
-    # just-created alias in a projection
-    region = con.table('tpch_region')
-    nation = con.table('tpch_nation')
-    customer = con.table('tpch_customer')
-    orders = con.table('tpch_orders')
-
-    fields_of_interest = [
-        customer,
-        region.r_name.name('region'),
-        orders.o_totalprice.name('amount'),
-        orders.o_orderdate.cast('timestamp').name('odate'),
-    ]
-
-    all_join = (
-        region.join(nation, region.r_regionkey == nation.n_regionkey)
-        .join(customer, customer.c_nationkey == nation.n_nationkey)
-        .join(orders, orders.o_custkey == customer.c_custkey)
-    )
-
-    tpch = all_join[fields_of_interest]
-
-    # Correlated subquery, yikes!
-    t2 = tpch.view()
-    conditional_avg = t2[(t2.region == tpch.region)].amount.mean()
-
-    # `amount` is part of the projection above as an aliased field
-    amount_filter = tpch.amount > conditional_avg
-
-    result = tpch.filter([amount_filter])
-
-    # Now then! Predicate pushdown here is inappropriate, so we check that
-    # it didn't occur.
-    assert isinstance(result.op(), ops.Selection)
-    assert result.op().table == tpch.op()
-
-
-def test_join_predicate_from_derived_raises():
-    # Join predicate references a derived table, but we can salvage and
-    # rewrite it to get the join semantics out
-    # see ibis #74
-    table = ibis.table([('c', 'int32'), ('f', 'double'), ('g', 'string')], 'foo_table')
-
-    table2 = ibis.table([('key', 'string'), ('value', 'double')], 'bar_table')
-
-    filter_pred = table['f'] > 0
-    table3 = table[filter_pred]
-
-    with pytest.raises(com.ExpressionError):
-        table.inner_join(table2, [table3['g'] == table2['key']])
-
-
-def test_bad_join_predicate_raises():
-    table = ibis.table([('c', 'int32'), ('f', 'double'), ('g', 'string')], 'foo_table')
-
-    table2 = ibis.table([('key', 'string'), ('value', 'double')], 'bar_table')
-
-    table3 = ibis.table([('key', 'string'), ('value', 'double')], 'baz_table')
-
-    with pytest.raises(com.ExpressionError):
-        table.inner_join(table2, [table['g'] == table3['key']])
-
-
-def test_filter_self_join():
-    # GH #667
-    purchases = ibis.table(
-        [
-            ('region', 'string'),
-            ('kind', 'string'),
-            ('user', 'int64'),
-            ('amount', 'double'),
-        ],
-        'purchases',
-    )
-
-    metric = purchases.amount.sum().name('total')
-    agged = purchases.group_by(['region', 'kind']).aggregate(metric)
-
-    left = agged[agged.kind == 'foo']
-    right = agged[agged.kind == 'bar']
-
-    cond = left.region == right.region
-    joined = left.join(right, cond)
-
-    metric = (left.total - right.total).name('diff')
-    what = [left.region, metric]
-    projected = joined.select(what)
-
-    proj_exprs = projected.op().selections
-
-    # proj exprs unaffected by analysis
-    assert_equal(proj_exprs[0], left.region.op())
-    assert_equal(proj_exprs[1], metric.op())
-
-
-def test_no_rewrite(con):
-    table = con.table('test1')
-    table4 = table[['c', (table['c'] * 2).name('foo')]]
-    expr = table4['c'] == table4['foo']
-    result = an.substitute_parents(expr.op()).to_expr()
-    expected = expr
-    assert result.equals(expected)
-
-
-def test_join_table_choice():
-    # GH807
-    x = ibis.table(ibis.schema([('n', 'int64')]), 'x')
-    t = x.aggregate(cnt=x.n.count())
-    predicate = t.cnt > 0
-
-    result = an.sub_for(predicate.op(), {t.op(): t.op().table})
-    assert result == predicate.op()
-
-
-def test_is_ancestor_analytic():
-    x = ibis.table(ibis.schema([('col', 'int32')]), 'x')
-    with_filter_col = x[x.columns + [ibis.null().name('filter')]]
-    filtered = with_filter_col[with_filter_col['filter'].isnull()]
-    subquery = filtered[filtered.columns]
-
-    with_analytic = subquery[subquery.columns + [subquery.count().name('analytic')]]
-
-    assert not subquery.op().equals(with_analytic.op())
-
-
-# Pr 2635
-def test_mutation_fusion_no_overwrite():
-    """Test fusion with chained mutation that doesn't overwrite existing
-    columns."""
-    t = ibis.table(ibis.schema([('col', 'int32')]), 't')
-
-    result = t
-    result = result.mutate(col1=t['col'] + 1)
-    result = result.mutate(col2=t['col'] + 2)
-    result = result.mutate(col3=t['col'] + 3)
-    result = result.op()
-
-    first_selection = result
-
-    assert len(result.selections) == 4
-
-    col1 = (t['col'] + 1).name('col1')
-    assert first_selection.selections[1] == col1.op()
-
-    col2 = (t['col'] + 2).name('col2')
-    assert first_selection.selections[2] == col2.op()
-
-    col3 = (t['col'] + 3).name('col3')
-    assert first_selection.selections[3] == col3.op()
-
-
-# Pr 2635
-def test_mutation_fusion_overwrite():
-    """Test fusion with chained mutation that overwrites existing columns."""
-    t = ibis.table(ibis.schema([('col', 'int32')]), 't')
-
-    result = t
-
-    result = result.mutate(col1=t['col'] + 1)
-    result = result.mutate(col2=t['col'] + 2)
-    result = result.mutate(col3=t['col'] + 3)
-    result = result.mutate(col=t['col'] - 1)
-    result = result.mutate(col4=t['col'] + 4)
-
-    second_selection = result.op()
-    first_selection = second_selection.table
-
-    assert len(first_selection.selections) == 4
-    col1 = (t['col'] + 1).name('col1').op()
-    assert first_selection.selections[1] == col1
-
-    col2 = (t['col'] + 2).name('col2').op()
-    assert first_selection.selections[2] == col2
-
-    col3 = (t['col'] + 3).name('col3').op()
-    assert first_selection.selections[3] == col3
-
-    # Since the second selection overwrites existing columns, it will
-    # not have the Table as the first selection
-    assert len(second_selection.selections) == 5
-
-    col = (t['col'] - 1).name('col').op()
-    assert second_selection.selections[0] == col
-
-    col1 = first_selection.to_expr()['col1'].op()
-    assert second_selection.selections[1] == col1
-
-    col2 = first_selection.to_expr()['col2'].op()
-    assert second_selection.selections[2] == col2
-
-    col3 = first_selection.to_expr()['col3'].op()
-    assert second_selection.selections[3] == col3
-
-    col4 = (t['col'] + 4).name('col4').op()
-    assert second_selection.selections[4] == col4
-
-
-# Pr 2635
-def test_select_filter_mutate_fusion():
-    """Test fusion with filter followed by mutation on the same input."""
-
-    t = ibis.table(ibis.schema([('col', 'float32')]), 't')
-
-    result = t[['col']]
-    result = result[result['col'].isnan()]
-    result = result.mutate(col=result['col'].cast('int32'))
-
-    second_selection = result.op()
-    first_selection = second_selection.table
-    assert len(second_selection.selections) == 1
-
-    col = first_selection.to_expr()['col'].cast('int32').name('col').op()
-    assert second_selection.selections[0] == col
-
-    # we don't look past the projection when a filter is encountered, so the
-    # number of selections in the first projection (`first_selection`) is 0
-    #
-    # previously we did, but this was buggy when executing against the pandas
-    # backend
-    #
-    # eventually we will bring this back, but we're trading off the ability
-    # to remove materialize for some performance in the short term
-    assert len(first_selection.selections) == 1
-    assert len(first_selection.predicates) == 1
-
-
-def test_no_filter_means_no_selection():
-    t = ibis.table(dict(a="string"))
-    proj = t.filter([])
-    assert proj.equals(t)
-
-
-def test_mutate_overwrites_existing_column():
-    t = ibis.table(dict(a="string"))
-    mut = t.mutate(a=42).select(["a"])
-    sel = mut.op().selections[0].table.selections[0].arg
-    assert isinstance(sel, ops.Literal)
-    assert sel.value == 42
-
-
-def test_agg_selection_does_not_share_roots():
-    t = ibis.table(dict(a="string"), name="t")
-    s = ibis.table(dict(b="float64"), name="s")
-    gb = t.group_by("a")
-    n = s.count()
+class TestTableExprBasics(BasicTestCase, unittest.TestCase):
 
-    with pytest.raises(com.RelationError, match="Selection expressions"):
-        gb.aggregate(n=n)
+    def test_rewrite_substitute_distinct_tables(self):
+        t = self.con.table('test1')
+        tt = self.con.table('test1')
+
+        expr = t[t.c > 0]
+        expr2 = tt[tt.c > 0]
+
+        metric = t.f.sum().name('metric')
+        expr3 = expr.aggregate(metric)
+
+        result = L.sub_for(expr3, [(expr2, t)])
+        expected = t.aggregate(metric)
+
+        assert_equal(result, expected)
+
+    def test_rewrite_join_projection_without_other_ops(self):
+        # Drop out filters and other commutative table operations. Join
+        # predicates are "lifted" to reference the base, unmodified join roots
+
+        # Star schema with fact table
+        table = self.con.table('star1')
+        table2 = self.con.table('star2')
+        table3 = self.con.table('star3')
+
+        filtered = table[table['f'] > 0]
+
+        pred1 = table['foo_id'] == table2['foo_id']
+        pred2 = filtered['bar_id'] == table3['bar_id']
+
+        j1 = filtered.left_join(table2, [pred1])
+        j2 = j1.inner_join(table3, [pred2])
+
+        # Project out the desired fields
+        view = j2[[filtered, table2['value1'], table3['value2']]]
+
+        # Construct the thing we expect to obtain
+        ex_pred2 = table['bar_id'] == table3['bar_id']
+        ex_expr = (table.left_join(table2, [pred1])
+                   .inner_join(table3, [ex_pred2]))
+
+        rewritten_proj = L.substitute_parents(view)
+        op = rewritten_proj.op()
+        assert_equal(op.table, ex_expr)
+
+        # Ensure that filtered table has been substituted with the base table
+        assert op.selections[0] is table
+
+    def test_rewrite_past_projection(self):
+        table = self.con.table('test1')
+
+        # Rewrite past a projection
+        table3 = table[['c', 'f']]
+        expr = table3['c'] == 2
+
+        result = L.substitute_parents(expr)
+        expected = table['c'] == 2
+        assert_equal(result, expected)
+
+        # Unsafe to rewrite past projection
+        table5 = table[(table.f * 2).name('c'), table.f]
+        expr = table5['c'] == 2
+        result = L.substitute_parents(expr)
+        assert result is expr
+
+    def test_rewrite_expr_with_parent(self):
+        table = self.con.table('test1')
+
+        table2 = table[table['f'] > 0]
+
+        expr = table2['c'] == 2
+
+        result = L.substitute_parents(expr)
+        expected = table['c'] == 2
+        assert_equal(result, expected)
+
+        # Substitution not fully possible if we depend on a new expr in a
+        # projection
+
+        table4 = table[['c', (table['c'] * 2).name('foo')]]
+        expr = table4['c'] == table4['foo']
+        result = L.substitute_parents(expr)
+        expected = table['c'] == table4['foo']
+        assert_equal(result, expected)
+
+    def test_rewrite_distinct_but_equal_objects(self):
+        t = self.con.table('test1')
+        t_copy = self.con.table('test1')
+
+        table2 = t[t_copy['f'] > 0]
+
+        expr = table2['c'] == 2
+
+        result = L.substitute_parents(expr)
+        expected = t['c'] == 2
+        assert_equal(result, expected)
+
+    def test_projection_with_join_pushdown_rewrite_refs(self):
+        # Observed this expression IR issue in a TopK-rewrite context
+        table1 = ibis.table([
+            ('a_key1', 'string'),
+            ('a_key2', 'string'),
+            ('a_value', 'double')
+        ], 'foo')
+
+        table2 = ibis.table([
+            ('b_key1', 'string'),
+            ('b_name', 'string'),
+            ('b_value', 'double')
+        ], 'bar')
+
+        table3 = ibis.table([
+            ('c_key2', 'string'),
+            ('c_name', 'string')
+        ], 'baz')
+
+        proj = (table1.inner_join(table2, [('a_key1', 'b_key1')])
+                .inner_join(table3, [(table1.a_key2, table3.c_key2)])
+                [table1, table2.b_name.name('b'), table3.c_name.name('c'),
+                 table2.b_value])
+
+        cases = [
+            (proj.a_value > 0, table1.a_value > 0),
+            (proj.b_value > 0, table2.b_value > 0)
+        ]
+
+        for higher_pred, lower_pred in cases:
+            result = proj.filter([higher_pred])
+            op = result.op()
+            assert isinstance(op, ops.Projection)
+            filter_op = op.table.op()
+            assert isinstance(filter_op, ops.Filter)
+            new_pred = filter_op.predicates[0]
+            assert_equal(new_pred, lower_pred)
+
+    def test_multiple_join_deeper_reference(self):
+        # Join predicates down the chain might reference one or more root
+        # tables in the hierarchy.
+        table1 = ibis.table({'key1': 'string', 'key2': 'string',
+                            'value1': 'double'})
+        table2 = ibis.table({'key3': 'string', 'value2': 'double'})
+        table3 = ibis.table({'key4': 'string', 'value3': 'double'})
+
+        joined = table1.inner_join(table2, [table1['key1'] == table2['key3']])
+        joined2 = joined.inner_join(table3, [table1['key2'] == table3['key4']])
+
+        # it works, what more should we test here?
+        materialized = joined2.materialize()
+        repr(materialized)
+
+    def test_filter_on_projected_field(self):
+        # See #173. Impala and other SQL engines do not allow filtering on a
+        # just-created alias in a projection
+        region = self.con.table('tpch_region')
+        nation = self.con.table('tpch_nation')
+        customer = self.con.table('tpch_customer')
+        orders = self.con.table('tpch_orders')
+
+        fields_of_interest = [customer,
+                              region.r_name.name('region'),
+                              orders.o_totalprice.name('amount'),
+                              orders.o_orderdate
+                              .cast('timestamp').name('odate')]
+
+        all_join = (
+            region.join(nation, region.r_regionkey == nation.n_regionkey)
+            .join(customer, customer.c_nationkey == nation.n_nationkey)
+            .join(orders, orders.o_custkey == customer.c_custkey))
+
+        tpch = all_join[fields_of_interest]
+
+        # Correlated subquery, yikes!
+        t2 = tpch.view()
+        conditional_avg = t2[(t2.region == tpch.region)].amount.mean()
+
+        # `amount` is part of the projection above as an aliased field
+        amount_filter = tpch.amount > conditional_avg
+
+        result = tpch.filter([amount_filter])
+
+        # Now then! Predicate pushdown here is inappropriate, so we check that
+        # it didn't occur.
+
+        # If filter were pushed below projection, the top-level operator type
+        # would be Projection instead.
+        assert type(result.op()) == ops.Filter
+
+    def test_bad_join_predicate_raises(self):
+        # Join predicate references a derived table, but we can salvage and
+        # rewrite it to get the join semantics out
+        # see ibis #74
+        table = ibis.table([
+            ('c', 'int32'),
+            ('f', 'double'),
+            ('g', 'string')
+        ], 'foo_table')
+
+        table2 = ibis.table([
+            ('key', 'string'),
+            ('value', 'double')
+        ], 'bar_table')
+
+        filter_pred = table['f'] > 0
+        table3 = table[filter_pred]
+
+        with self.assertRaises(com.ExpressionError):
+            table.inner_join(table2, [table3['g'] == table2['key']])
+
+        # expected = table.inner_join(table2, [table['g'] == table2['key']])
+        # assert_equal(result, expected)
+
+    def test_filter_self_join(self):
+        # GH #667
+        purchases = ibis.table([('region', 'string'),
+                                ('kind', 'string'),
+                                ('user', 'int64'),
+                                ('amount', 'double')], 'purchases')
+
+        metric = purchases.amount.sum().name('total')
+        agged = (purchases.group_by(['region', 'kind'])
+                 .aggregate(metric))
+
+        left = agged[agged.kind == 'foo']
+        right = agged[agged.kind == 'bar']
+
+        cond = left.region == right.region
+        joined = left.join(right, cond)
+
+        # unmodified by analysis
+        assert_equal(joined.op().predicates[0], cond)
+
+        metric = (left.total - right.total).name('diff')
+        what = [left.region, metric]
+        projected = joined.projection(what)
+
+        proj_exprs = projected.op().selections
+
+        # proj exprs unaffected by analysis
+        assert_equal(proj_exprs[0], left.region)
+        assert_equal(proj_exprs[1], metric)
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/tests/expr/test_interactive.py` & `ibis-framework-v0.6.0/ibis/expr/tests/test_interactive.py`

 * *Files 24% similar despite different names*

```diff
@@ -7,107 +7,87 @@
 # http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from __future__ import annotations
 
-import pytest
+from ibis.compat import unittest
+from ibis.expr.tests.mocks import MockConnection
+import ibis.config as config
 
-from ibis import config
-from ibis.tests.expr.mocks import MockBackend
 
+class TestInteractiveUse(unittest.TestCase):
 
-@pytest.fixture
-def con():
-    return MockBackend()
+    def setUp(self):
+        self.con = MockConnection()
 
+    def test_interactive_execute_on_repr(self):
+        table = self.con.table('functional_alltypes')
+        expr = table.bigint_col.sum()
+        with config.option_context('interactive', True):
+            repr(expr)
 
-def test_interactive_execute_on_repr(con):
-    table = con.table('functional_alltypes')
-    expr = table.bigint_col.sum()
-    with config.option_context('interactive', True):
-        repr(expr)
+        assert len(self.con.executed_queries) > 0
 
-    assert len(con.executed_queries) > 0
+    def test_default_limit(self):
+        table = self.con.table('functional_alltypes')
 
-
-def test_repr_png_is_none_in_interactive(con, graphviz):
-    table = con.table('functional_alltypes')
-
-    with config.option_context('interactive', True):
-        assert table._repr_png_() is None
-
-
-def test_repr_png_is_not_none_in_not_interactive(con):
-    pytest.importorskip('ibis.expr.visualize')
-
-    table = con.table('functional_alltypes')
-
-    with config.option_context('interactive', False), config.option_context(
-        'graphviz_repr', True
-    ):
-        assert table._repr_png_() is not None
-
-
-def test_default_limit(con, snapshot):
-    table = con.table('functional_alltypes').select("id", "bool_col")
-
-    with config.option_context('interactive', True):
-        repr(table)
-
-    snapshot.assert_match(con.executed_queries[0], "out.sql")
-
-
-def test_respect_set_limit(con, snapshot):
-    table = con.table('functional_alltypes').select("id", "bool_col").limit(10)
-
-    with config.option_context('interactive', True):
-        repr(table)
-
-    snapshot.assert_match(con.executed_queries[0], "out.sql")
-
-
-def test_disable_query_limit(con, snapshot):
-    table = con.table('functional_alltypes').select("id", "bool_col")
-
-    with config.option_context('interactive', True):
-        with config.option_context('sql.default_limit', None):
+        with config.option_context('interactive', True):
             repr(table)
 
-    snapshot.assert_match(con.executed_queries[0], "out.sql")
-
+        expected = """\
+SELECT *
+FROM functional_alltypes
+LIMIT {0}""".format(config.options.sql.default_limit)
 
-def test_interactive_non_compilable_repr_not_fail(con):
-    # #170
-    table = con.table('functional_alltypes')
+        assert self.con.executed_queries[0] == expected
 
-    expr = table.string_col.topk(3)
-
-    # it works!
-    with config.option_context('interactive', True):
-        repr(expr)
-
-
-def test_histogram_repr_no_query_execute(con):
-    t = con.table('functional_alltypes')
-    tier = t.double_col.histogram(10).name('bucket')
-    expr = t.group_by(tier).size()
-    with config.option_context('interactive', True):
-        expr._repr()
-    assert con.executed_queries == []
-
-
-def test_compile_no_execute(con):
-    t = con.table('functional_alltypes')
-    t.double_col.sum().compile()
-    assert con.executed_queries == []
+    def test_respect_set_limit(self):
+        table = self.con.table('functional_alltypes').limit(10)
 
+        with config.option_context('interactive', True):
+            repr(table)
 
-def test_isin_rule_supressed_exception_repr_not_fail(con):
-    with config.option_context('interactive', True):
-        t = con.table('functional_alltypes')
-        bool_clause = t['string_col'].notin(['1', '4', '7'])
-        expr = t[bool_clause]['string_col'].value_counts()
-        repr(expr)
+        expected = """\
+SELECT *
+FROM functional_alltypes
+LIMIT 10"""
+
+        assert self.con.executed_queries[0] == expected
+
+    def test_disable_query_limit(self):
+        table = self.con.table('functional_alltypes')
+
+        with config.option_context('interactive', True):
+            with config.option_context('sql.default_limit', None):
+                repr(table)
+
+        expected = """\
+SELECT *
+FROM functional_alltypes"""
+
+        assert self.con.executed_queries[0] == expected
+
+    def test_interactive_non_compilable_repr_not_fail(self):
+        # #170
+        table = self.con.table('functional_alltypes')
+
+        expr = table.string_col.topk(3)
+
+        # it works!
+        with config.option_context('interactive', True):
+            repr(expr)
+
+    def test_histogram_repr_no_query_execute(self):
+        t = self.con.table('functional_alltypes')
+        tier = t.double_col.histogram(10).name('bucket')
+        expr = t.group_by(tier).size()
+        with config.option_context('interactive', True):
+            expr._repr()
+        assert self.con.executed_queries == []
+
+    def test_compile_no_execute(self):
+        t = self.con.table('functional_alltypes')
+        t.double_col.sum().compile()
+        assert self.con.executed_queries == []
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/tests/expr/test_pipe.py` & `ibis-framework-v0.6.0/ibis/expr/tests/test_pipe.py`

 * *Files 15% similar despite different names*

```diff
@@ -7,55 +7,53 @@
 # http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from __future__ import annotations
-
-import pytest
 
+from ibis.compat import unittest
 import ibis
 
 
-@pytest.fixture
-def pipe_table():
-    return ibis.table(
-        [
+class TestPipe(unittest.TestCase):
+
+    def setUp(self):
+        self.table = ibis.table([
             ('key1', 'string'),
             ('key2', 'string'),
             ('key3', 'string'),
-            ('value', 'double'),
-        ],
-        'foo_table',
-    )
-
-
-def test_pipe_positional_args(pipe_table):
-    def my_func(data, foo, bar):
-        return data[bar] + foo
+            ('value', 'double')
+        ], 'foo_table')
 
-    result = pipe_table.pipe(my_func, 4, 'value')
-    expected = pipe_table['value'] + 4
+    def test_pipe_positional_args(self):
+        def my_func(data, foo, bar):
+            return data[bar] + foo
 
-    assert result.equals(expected)
+        result = self.table.pipe(my_func, 4, 'value')
+        expected = self.table['value'] + 4
 
+        assert result.equals(expected)
 
-def test_pipe_keyword_args(pipe_table):
-    def my_func(data, foo=None, bar=None):
-        return data[bar] + foo
+    def test_pipe_keyword_args(self):
+        def my_func(data, foo=None, bar=None):
+            return data[bar] + foo
 
-    result = pipe_table.pipe(my_func, foo=4, bar='value')
-    expected = pipe_table['value'] + 4
+        result = self.table.pipe(my_func, foo=4, bar='value')
+        expected = self.table['value'] + 4
 
-    assert result.equals(expected)
+        assert result.equals(expected)
 
+    def test_pipe_pass_to_keyword(self):
+        def my_func(x, y, data=None):
+            return data[x] + y
 
-def test_pipe_pass_to_keyword(pipe_table):
-    def my_func(x, y, data=None):
-        return data[x] + y
+        result = self.table.pipe((my_func, 'data'), 'value', 4)
+        expected = self.table['value'] + 4
 
-    result = pipe_table.pipe((my_func, 'data'), 'value', 4)
-    expected = pipe_table['value'] + 4
+        assert result.equals(expected)
 
-    assert result.equals(expected)
+    def test_call_pipe_equivalence(self):
+        result = self.table(lambda x: x['key1'].cast('double').sum())
+        expected = self.table.key1.cast('double').sum()
+        assert result.equals(expected)
```

### Comparing `ibis_framework-6.1.1.dev22/ibis/tests/expr/test_table.py` & `ibis-framework-v0.6.0/ibis/sql/tests/test_compiler.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,1842 +1,2091 @@
-from __future__ import annotations
-
-import datetime
-import pickle
-import re
-from typing import List
-
-import numpy as np
-import pandas as pd
-import pytest
-from pytest import param
+# Copyright 2014 Cloudera Inc.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
 
 import ibis
-import ibis.common.exceptions as com
-import ibis.expr.analysis as an
-import ibis.expr.datatypes as dt
-import ibis.expr.operations as ops
-import ibis.expr.schema as sch
-import ibis.expr.types as ir
-import ibis.selectors as s
-from ibis import _
-from ibis import literal as L
-from ibis.common.exceptions import RelationError
-from ibis.expr import api
-from ibis.expr.types import Column, Table
-from ibis.tests.expr.mocks import MockAlchemyBackend, MockBackend
-from ibis.tests.util import assert_equal, assert_pickle_roundtrip
-
-
-@pytest.fixture
-def set_ops_schema_top():
-    return [('key', 'string'), ('value', 'double')]
-
-
-@pytest.fixture
-def set_ops_schema_bottom():
-    return [('key', 'string'), ('key2', 'string'), ('value', 'double')]
-
-
-@pytest.fixture
-def setops_table_foo(set_ops_schema_top):
-    return ibis.table(set_ops_schema_top, 'foo')
-
-
-@pytest.fixture
-def setops_table_bar(set_ops_schema_top):
-    return ibis.table(set_ops_schema_top, 'bar')
-
-
-@pytest.fixture
-def setops_table_baz(set_ops_schema_bottom):
-    return ibis.table(set_ops_schema_bottom, 'baz')
-
-
-@pytest.fixture
-def setops_relation_error_message():
-    return 'Table schemas must be equal for set operations'
-
-
-def test_empty_schema():
-    table = api.table([], 'foo')
-    assert not table.schema()
-
-
-def test_columns(con):
-    t = con.table('alltypes')
-    result = t.columns
-    expected = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k']
-    assert result == expected
-
-
-def test_view_new_relation(table):
-    # For assisting with self-joins and other self-referential operations
-    # where we need to be able to treat instances of the same Table as
-    # semantically distinct
-    #
-    # This thing is not exactly a projection, since it has no semantic
-    # meaning when it comes to execution
-    tview = table.view()
-
-    roots = an.find_immediate_parent_tables(tview.op())
-    assert len(roots) == 1
-    assert roots[0] is tview.op()
-
-
-def test_getitem_column_select(table):
-    for k in table.columns:
-        col = table[k]
-
-        # Make sure it's the right type
-        assert isinstance(col, Column)
-
-
-def test_table_tab_completion():
-    table = ibis.table({"a": "int", "b": "int", "for": "int", "with spaces": "int"})
-    # Only valid python identifiers in getattr tab completion
-    attrs = set(dir(table))
-    assert {"a", "b"}.issubset(attrs)
-    assert {"for", "with spaces"}.isdisjoint(attrs)
-    # All columns in getitem tab completion
-    items = set(table._ipython_key_completions_())
-    assert items.issuperset(table.columns)
-
-
-def test_getitem_attribute(table):
-    result = table.a
-    assert_equal(result, table['a'])
-
-    # Project and add a name that conflicts with a Table built-in
-    # attribute
-    view = table[[table, table['a'].name('schema')]]
-    assert not isinstance(view.schema, Column)
-
-
-def test_getitem_missing_column(table):
-    with pytest.raises(com.IbisTypeError, match="oops"):
-        table["oops"]
-
-
-def test_getattr_missing_column(table):
-    with pytest.raises(AttributeError, match="oops"):
-        table.oops  # noqa: B018
-
-
-def test_typo_method_name_recommendation(table):
-    with pytest.raises(AttributeError, match="order_by"):
-        table.sort("a")
-
-    # Existing columns take precedence over raising an error
-    # for a common method typo
-    table2 = table.relabel({"a": "sort"})
-    assert isinstance(table2.sort, Column)
-
-
-def test_projection(table):
-    cols = ['f', 'a', 'h']
-
-    proj = table[cols]
-    assert isinstance(proj, Table)
-    assert isinstance(proj.op(), ops.Selection)
-
-    assert proj.schema().names == tuple(cols)
-    for c in cols:
-        expr = proj[c]
-        assert isinstance(expr, type(table[c]))
-
-
-def test_projection_no_list(table):
-    expr = (table.f * 2).name('bar')
-    result = table.select(expr)
-    expected = table.select([expr])
-    assert_equal(result, expected)
-
-
-def test_projection_with_exprs(table):
-    # unnamed expr to test
-    mean_diff = (table['a'] - table['c']).mean()
-
-    col_exprs = [table['b'].log().name('log_b'), mean_diff.name('mean_diff')]
-
-    proj = table[col_exprs + ['g']]
-    schema = proj.schema()
-    assert schema.names == ('log_b', 'mean_diff', 'g')
-    assert schema.types == (dt.double, dt.double, dt.string)
-
-    # Test with unnamed expr
-    proj = table.select(['g', table['a'] - table['c']])
-    schema = proj.schema()
-    assert schema.names == ('g', 'Subtract(a, c)')
-    assert schema.types == (dt.string, dt.int64)
-
-
-def test_projection_duplicate_names(table):
-    with pytest.raises(com.IntegrityError):
-        table.select([table.c, table.c])
-
-
-def test_projection_invalid_root(table):
-    schema1 = {'foo': 'double', 'bar': 'int32'}
-
-    left = api.table(schema1, name='foo')
-    right = api.table(schema1, name='bar')
-
-    exprs = [right['foo'], right['bar']]
-    with pytest.raises(RelationError):
-        left.select(exprs)
-
-
-def test_projection_with_star_expr(table):
-    new_expr = (table['a'] * 5).name('bigger_a')
-
-    t = table
-
-    # it lives!
-    proj = t[t, new_expr]
-    repr(proj)
-
-    ex_names = table.schema().names + ('bigger_a',)
-    assert proj.schema().names == ex_names
-
-    # cannot pass an invalid table expression
-    t2 = t.aggregate([t['a'].sum().name('sum(a)')], by=['g'])
-    with pytest.raises(RelationError):
-        t[[t2]]
-    # TODO: there may be some ways this can be invalid
-
-
-def test_projection_convenient_syntax(table):
-    proj = table[table, table['a'].name('foo')]
-    proj2 = table[[table, table['a'].name('foo')]]
-    assert_equal(proj, proj2)
-
-
-def test_projection_mutate_analysis_bug(con):
-    # GH #549
-
-    t = con.table('airlines')
-
-    filtered = t[t.depdelay.notnull()]
-    leg = ibis.literal('-').join([t.origin, t.dest])
-    mutated = filtered.mutate(leg=leg)
 
-    # it works!
-    mutated['year', 'month', 'day', 'depdelay', 'leg']
+from ibis.impala.compiler import build_ast, to_sql
 
+from ibis import impala
 
-def test_projection_self(table):
-    result = table[table]
-    expected = table.select(table)
+from ibis.expr.tests.mocks import MockConnection
+from ibis.compat import unittest
+import ibis.common as com
 
-    assert_equal(result, expected)
-
-
-def test_projection_array_expr(table):
-    result = table[table.a]
-    expected = table[[table.a]]
-    assert_equal(result, expected)
+import ibis.expr.api as api
+import ibis.expr.operations as ops
 
 
-@pytest.mark.parametrize("empty", [list(), dict()])
-def test_projection_no_expr(table, empty):
-    with pytest.raises(com.IbisTypeError, match="must select at least one"):
-        table.select(empty)
+class TestASTBuilder(unittest.TestCase):
 
+    def setUp(self):
+        self.con = MockConnection()
 
-def test_mutate(table):
-    expr = table.mutate(
-        [
-            (table.a + 1).name("x1"),
-            table.b.sum().name("x2"),
-            (_.a + 2).name("x3"),
-            lambda _: (_.a + 3).name("x4"),
-            4,
-            "five",
+    def test_ast_with_projection_join_filter(self):
+        table = self.con.table('test1')
+        table2 = self.con.table('test2')
+
+        filter_pred = table['f'] > 0
+
+        table3 = table[filter_pred]
+
+        join_pred = table3['g'] == table2['key']
+
+        joined = table2.inner_join(table3, [join_pred])
+        result = joined[[table3, table2['value']]]
+
+        ast = build_ast(result)
+        stmt = ast.queries[0]
+
+        def foo():
+            table3 = table[filter_pred]
+            joined = table2.inner_join(table3, [join_pred])
+            result = joined[[table3, table2['value']]]
+            return result
+
+        assert len(stmt.select_set) == 2
+        assert len(stmt.where) == 1
+        assert stmt.where[0] is filter_pred
+
+        # Check that the join has been rebuilt to only include the root tables
+        tbl = stmt.table_set
+        tbl_node = tbl.op()
+        assert isinstance(tbl_node, ops.InnerJoin)
+        assert tbl_node.left is table2
+        assert tbl_node.right is table
+
+        # table expression substitution has been made in the predicate
+        assert tbl_node.predicates[0].equals(table['g'] == table2['key'])
+
+    def test_ast_with_aggregation_join_filter(self):
+        table = self.con.table('test1')
+        table2 = self.con.table('test2')
+
+        filter_pred = table['f'] > 0
+        table3 = table[filter_pred]
+        join_pred = table3['g'] == table2['key']
+
+        joined = table2.inner_join(table3, [join_pred])
+
+        met1 = (table3['f'] - table2['value']).mean().name('foo')
+        result = joined.aggregate([met1, table3['f'].sum().name('bar')],
+                                  by=[table3['g'], table2['key']])
+
+        ast = build_ast(result)
+        stmt = ast.queries[0]
+
+        # hoisted metrics
+        ex_metrics = [(table['f'] - table2['value']).mean().name('foo'),
+                      table['f'].sum().name('bar')]
+        ex_by = [table['g'], table2['key']]
+
+        # hoisted join and aggregate
+        expected_table_set = \
+            table2.inner_join(table, [table['g'] == table2['key']])
+        assert stmt.table_set.equals(expected_table_set)
+
+        # Check various exprs
+        for res, ex in zip(stmt.select_set, ex_by + ex_metrics):
+            assert res.equals(ex)
+
+        for res, ex in zip(stmt.group_by, ex_by):
+            assert stmt.select_set[res].equals(ex)
+
+        # Check we got the filter
+        assert len(stmt.where) == 1
+        assert stmt.where[0].equals(filter_pred)
+
+
+class TestNonTabularResults(unittest.TestCase):
+
+    """
+
+    """
+
+    def setUp(self):
+        self.con = MockConnection()
+        self.table = self.con.table('alltypes')
+
+    def test_simple_scalar_aggregates(self):
+        from pandas import DataFrame
+
+        # Things like table.column.{sum, mean, ...}()
+        table = self.con.table('alltypes')
+
+        expr = table[table.c > 0].f.sum()
+
+        ast = build_ast(expr)
+        query = ast.queries[0]
+
+        sql_query = query.compile()
+        expected = """SELECT sum(`f`) AS `sum`
+FROM alltypes
+WHERE `c` > 0"""
+
+        assert sql_query == expected
+
+        # Maybe the result handler should act on the cursor. Not sure.
+        handler = query.result_handler
+        output = DataFrame({'sum': [5]})
+        assert handler(output) == 5
+
+    def test_table_column_unbox(self):
+        from pandas import DataFrame
+
+        table = self.table
+        m = table.f.sum().name('total')
+        agged = table[table.c > 0].group_by('g').aggregate([m])
+        expr = agged.g
+
+        ast = build_ast(expr)
+        query = ast.queries[0]
+
+        sql_query = query.compile()
+        expected = """\
+SELECT `g`
+FROM (
+  SELECT `g`, sum(`f`) AS `total`
+  FROM alltypes
+  WHERE `c` > 0
+  GROUP BY 1
+) t0"""
+
+        assert sql_query == expected
+
+        # Maybe the result handler should act on the cursor. Not sure.
+        handler = query.result_handler
+        output = DataFrame({'g': ['foo', 'bar', 'baz']})
+        assert (handler(output) == output['g']).all()
+
+    def test_complex_array_expr_projection(self):
+        # May require finding the base table and forming a projection.
+        expr = (self.table.group_by('g')
+                .aggregate([self.table.count().name('count')]))
+        expr2 = expr.g.cast('double')
+
+        query = impala.compile(expr2)
+        expected = """SELECT CAST(`g` AS double) AS `tmp`
+FROM (
+  SELECT `g`, count(*) AS `count`
+  FROM alltypes
+  GROUP BY 1
+) t0"""
+        assert query == expected
+
+    def test_scalar_exprs_no_table_refs(self):
+        expr1 = ibis.now()
+        expected1 = """\
+SELECT now() AS `tmp`"""
+
+        expr2 = ibis.literal(1) + ibis.literal(2)
+        expected2 = """\
+SELECT 1 + 2 AS `tmp`"""
+
+        cases = [
+            (expr1, expected1),
+            (expr2, expected2)
+        ]
+
+        for expr, expected in cases:
+            result = impala.compile(expr)
+            assert result == expected
+
+    def test_expr_list_no_table_refs(self):
+        exlist = ibis.api.expr_list([ibis.literal(1).name('a'),
+                                     ibis.now().name('b'),
+                                     ibis.literal(2).log().name('c')])
+        result = impala.compile(exlist)
+        expected = """\
+SELECT 1 AS `a`, now() AS `b`, ln(2) AS `c`"""
+        assert result == expected
+
+    def test_isnull_case_expr_rewrite_failure(self):
+        # #172, case expression that was not being properly converted into an
+        # aggregation
+        reduction = self.table.g.isnull().ifelse(1, 0).sum()
+
+        result = impala.compile(reduction)
+        expected = """\
+SELECT sum(CASE WHEN `g` IS NULL THEN 1 ELSE 0 END) AS `sum`
+FROM alltypes"""
+        assert result == expected
+
+
+def _get_query(expr):
+    ast = build_ast(expr)
+    return ast.queries[0]
+
+nation = api.table([
+    ('n_regionkey', 'int32'),
+    ('n_nationkey', 'int32'),
+    ('n_name', 'string')
+], 'nation')
+
+region = api.table([
+    ('r_regionkey', 'int32'),
+    ('r_name', 'string')
+], 'region')
+
+customer = api.table([
+    ('c_nationkey', 'int32'),
+    ('c_name', 'string'),
+    ('c_acctbal', 'double')
+], 'customer')
+
+
+def _table_wrapper(name, tname=None):
+    @property
+    def f(self):
+        return self._table_from_schema(name, tname)
+    return f
+
+
+class ExprTestCases(object):
+
+    _schemas = {
+        'foo': [
+            ('job', 'string'),
+            ('dept_id', 'string'),
+            ('year', 'int32'),
+            ('y', 'double')
         ],
-        kw1=(table.a + 6),
-        kw2=table.b.sum(),
-        kw3=(_.a + 7),
-        kw4=lambda _: (_.a + 8),
-        kw5=9,
-        kw6="ten",
-    )
-    expected = table[
-        table,
-        (table.a + 1).name("x1"),
-        table.b.sum().name("x2"),
-        (table.a + 2).name("x3"),
-        (table.a + 3).name("x4"),
-        ibis.literal(4).name("4"),
-        ibis.literal("five").name("'five'"),
-        (table.a + 6).name("kw1"),
-        table.b.sum().name("kw2"),
-        (table.a + 7).name("kw3"),
-        (table.a + 8).name("kw4"),
-        ibis.literal(9).name("kw5"),
-        ibis.literal("ten").name("kw6"),
-    ]
-    assert_equal(expr, expected)
-
-
-def test_mutate_alter_existing_columns(table):
-    new_f = table.f * 2
-    foo = table.d * 2
-    expr = table.mutate(f=new_f, foo=foo)
-
-    expected = table[
-        'a',
-        'b',
-        'c',
-        'd',
-        'e',
-        new_f.name('f'),
-        'g',
-        'h',
-        'i',
-        'j',
-        'k',
-        foo.name('foo'),
-    ]
-
-    assert_equal(expr, expected)
-
-
-def test_replace_column():
-    tb = api.table([('a', 'int32'), ('b', 'double'), ('c', 'string')])
-
-    expr = tb.b.cast('int32')
-    tb2 = tb.mutate(b=expr)
-    expected = tb[tb.a, expr.name('b'), tb.c]
-
-    assert_equal(tb2, expected)
-
-
-def test_filter_no_list(table):
-    pred = table.a > 5
-
-    result = table.filter(pred)
-    expected = table[pred]
-    assert_equal(result, expected)
-
-
-def test_add_predicate(table):
-    pred = table['a'] > 5
-    result = table[pred]
-    assert isinstance(result.op(), ops.Selection)
-
-
-def test_invalid_predicate(table, schema):
-    # a lookalike
-    table2 = api.table(schema, name='bar')
-    predicate = table2.a > 5
-    with pytest.raises(RelationError):
-        table.filter(predicate)
-
-
-def test_add_predicate_coalesce(table):
-    # Successive predicates get combined into one rather than nesting. This
-    # is mainly to enhance readability since we could handle this during
-    # expression evaluation anyway.
-    pred1 = table['a'] > 5
-    pred2 = table['b'] > 0
-
-    result = table[pred1][pred2]
-    expected = table.filter([pred1, pred2])
-    assert_equal(result, expected)
-
-    # 59, if we are not careful, we can obtain broken refs
-    subset = table[pred1]
-    result = subset.filter([subset['b'] > 0])
-    assert_equal(result, expected)
-
-
-def test_repr_same_but_distinct_objects(con):
-    t = con.table('test1')
-    t_copy = con.table('test1')
-    table2 = t[t_copy['f'] > 0]
-
-    result = repr(table2)
-    assert result.count('DatabaseTable') == 1
-
-
-def test_filter_fusion_distinct_table_objects(con):
-    t = con.table('test1')
-    tt = con.table('test1')
-
-    expr = t[t.f > 0][t.c > 0]
-    expr2 = t[t.f > 0][tt.c > 0]
-    expr3 = t[tt.f > 0][tt.c > 0]
-    expr4 = t[tt.f > 0][t.c > 0]
-
-    assert_equal(expr, expr2)
-    assert repr(expr) == repr(expr2)
-    assert_equal(expr, expr3)
-    assert_equal(expr, expr4)
-
-
-def test_column_relabel():
-    table = api.table({"x": "int32", "y": "string", "z": "double"})
-    sol = sch.schema({"x_1": "int32", "y_1": "string", "z": "double"})
-
-    # Using a mapping
-    res = table.relabel({"x": "x_1", "y": "y_1"}).schema()
-    assert_equal(res, sol)
-
-    # Using a function
-    res = table.relabel(lambda x: None if x == "z" else f"{x}_1").schema()
-    assert_equal(res, sol)
-
-    # Mapping with unknown columns errors
-    with pytest.raises(KeyError, match="is not an existing column"):
-        table.relabel({"missing": "oops"})
-
-
-def test_relabel_format_string():
-    t = ibis.table({"x": "int", "y": "int", "z": "int"})
-
-    res = t.relabel("_{name}_")
-    sol = t.relabel({"x": "_x_", "y": "_y_", "z": "_z_"})
-    assert_equal(res, sol)
-
-    with pytest.raises(ValueError, match="Format strings must"):
-        t.relabel("no format string parameter")
-
-    with pytest.raises(ValueError, match="Format strings must"):
-        t.relabel("{unknown} format string parameter")
-
-
-def test_relabel_snake_case():
-    cases = [
-        ("cola", "cola"),
-        ("ColB", "col_b"),
-        ("colC", "col_c"),
-        ("col-d", "col_d"),
-        ("col_e", "col_e"),
-        (" Column F ", "column_f"),
-        ("Column G-with-hyphens", "column_g_with_hyphens"),
-        ("Col H notCamelCase", "col_h_notcamelcase"),
-    ]
-    t = ibis.table({c: "int" for c, _ in cases})
-    res = t.relabel("snake_case")
-    sol = t.relabel(dict(cases))
-    assert_equal(res, sol)
-
-
-def test_relabel_all_caps():
-    cases = [
-        ("cola", "COLA"),
-        ("ColB", "COL_B"),
-        ("colC", "COL_C"),
-        ("col-d", "COL_D"),
-        ("col_e", "COL_E"),
-        (" Column F ", "COLUMN_F"),
-        ("Column G-with-hyphens", "COLUMN_G_WITH_HYPHENS"),
-        ("Col H notCamelCase", "COL_H_NOTCAMELCASE"),
-    ]
-    t = ibis.table({c: "int" for c, _ in cases})
-    res = t.relabel("ALL_CAPS")
-    sol = t.relabel(dict(cases))
-    assert_equal(res, sol)
-
-
-def test_limit(table):
-    limited = table.limit(10, offset=5)
-    assert limited.op().n == 10
-    assert limited.op().offset == 5
-
-
-def test_order_by(table):
-    result = table.order_by(['f']).op()
-
-    sort_key = result.sort_keys[0]
-
-    assert_equal(sort_key.expr, table.f.op())
-    assert sort_key.ascending
-
-    # non-list input. per #150
-    result2 = table.order_by('f').op()
-    assert_equal(result, result2)
-
-    with pytest.warns(FutureWarning):
-        result2 = table.order_by([('f', False)])
-    with pytest.warns(FutureWarning):
-        result3 = table.order_by([('f', 'descending')])
-    with pytest.warns(FutureWarning):
-        result4 = table.order_by([('f', 0)])
-
-    key2 = result2.op().sort_keys[0]
-    key3 = result3.op().sort_keys[0]
-    key4 = result4.op().sort_keys[0]
-
-    assert key2.descending
-    assert key3.descending
-    assert key4.descending
-    assert_equal(result2, result3)
-
-
-def test_order_by_desc_deferred_sort_key(table):
-    result = table.group_by('g').size().order_by(ibis._[1].desc())
-
-    tmp = table.group_by('g').size()
-    expected = tmp.order_by(ibis.desc(tmp[1]))
-
-    assert_equal(result, expected)
-
-
-def test_order_by_asc_deferred_sort_key(table):
-    result = table.group_by('g').size().order_by(ibis._[1])
-
-    tmp = table.group_by('g').size()
-    expected = tmp.order_by(tmp[1])
-    expected2 = tmp.order_by(ibis.asc(tmp[1]))
-
-    assert_equal(result, expected)
-    assert_equal(result, expected2)
-
-
-@pytest.mark.parametrize(
-    ("key", "expected"),
-    [
-        param(ibis.NA, ibis.NA.op(), id="na"),
-        param(ibis.random(), ibis.random().op(), id="random"),
-        param(1.0, L(1.0).op(), id="float"),
-        param(L("a"), L("a").op(), id="string"),
-        param(L([1, 2, 3]), L([1, 2, 3]).op(), id="array"),
-    ],
-)
-def test_order_by_scalar(table, key, expected):
-    result = table.order_by(key)
-    assert result.op().sort_keys == (ops.SortKey(expected),)
-
-
-@pytest.mark.parametrize(
-    ("key", "exc_type"),
-    [
-        ("bogus", com.IbisTypeError),
-        (("bogus", False), com.IbisTypeError),
-        (ibis.desc("bogus"), com.IbisTypeError),
-        (1000, IndexError),
-        ((1000, False), IndexError),
-        (_.bogus, AttributeError),
-        (_.bogus.desc(), AttributeError),
-    ],
-)
-@pytest.mark.parametrize(
-    "expr_func",
-    [
-        param(lambda t: t, id="table"),
-        param(lambda t: t.select("a", "b"), id="selection"),
-        param(lambda t: t.group_by("a").agg(new=_.b.sum()), id="aggregation"),
-    ],
-)
-def test_order_by_nonexistent_column_errors(table, expr_func, key, exc_type):
-    # `order_by` is implemented on a few different operations, we check them
-    # all in turn here.
-    expr = expr_func(table)
-    with pytest.raises(exc_type):
-        expr.order_by(key)
-
-
-def test_slice(table):
-    expr = table[:5]
-    expr2 = table[:5:1]
-    assert_equal(expr, table.limit(5))
-    assert_equal(expr, expr2)
-
-    expr = table[2:7]
-    expr2 = table[2:7:1]
-    assert_equal(expr, table.limit(5, offset=2))
-    assert_equal(expr, expr2)
-
-    with pytest.raises(ValueError):
-        table[2:15:2]
-
-    with pytest.raises(ValueError):
-        table[5:]
-
-    with pytest.raises(ValueError):
-        table[:-5]
-
-    with pytest.raises(ValueError):
-        table[-10:-5]
-
-
-def test_table_count(table):
-    result = table.count()
-    assert isinstance(result, ir.IntegerScalar)
-    assert isinstance(result.op(), ops.CountStar)
-
-
-def test_len_raises_expression_error(table):
-    with pytest.raises(com.ExpressionError):
-        len(table)
-
-
-def test_sum_expr_basics(table, int_col):
-    # Impala gives bigint for all integer types
-    result = table[int_col].sum()
-    assert isinstance(result, ir.IntegerScalar)
-    assert isinstance(result.op(), ops.Sum)
-
-
-def test_sum_expr_basics_floats(table, float_col):
-    # Impala gives double for all floating point types
-    result = table[float_col].sum()
-    assert isinstance(result, ir.FloatingScalar)
-    assert isinstance(result.op(), ops.Sum)
-
-
-def test_mean_expr_basics(table, numeric_col):
-    result = table[numeric_col].mean()
-    assert isinstance(result, ir.FloatingScalar)
-    assert isinstance(result.op(), ops.Mean)
-
-
-def test_aggregate_no_keys(table):
-    metrics = [
-        table['a'].sum().name('sum(a)'),
-        table['c'].mean().name('mean(c)'),
-    ]
-
-    # A Table, which in SQL at least will yield a table with a single
-    # row
-    result = table.aggregate(metrics)
-    assert isinstance(result, Table)
-
-
-def test_aggregate_keys_basic(table):
-    metrics = [
-        table['a'].sum().name('sum(a)'),
-        table['c'].mean().name('mean(c)'),
-    ]
-
-    # A Table, which in SQL at least will yield a table with a single
-    # row
-    result = table.aggregate(metrics, by=['g'])
-    assert isinstance(result, Table)
-
-    # it works!
-    repr(result)
-
-
-def test_aggregate_non_list_inputs(table):
-    # per #150
-    metric = table.f.sum().name('total')
-    by = 'g'
-    having = table.c.sum() > 10
-
-    result = table.aggregate(metric, by=by, having=having)
-    expected = table.aggregate([metric], by=[by], having=[having])
-    assert_equal(result, expected)
-
-
-def test_aggregate_keywords(table):
-    t = table
-
-    expr = t.aggregate(foo=t.f.sum(), bar=lambda x: x.f.mean(), by='g')
-    expr2 = t.group_by('g').aggregate(foo=t.f.sum(), bar=lambda x: x.f.mean())
-    expected = t.aggregate([t.f.sum().name('foo'), t.f.mean().name('bar')], by='g')
-
-    assert_equal(expr, expected)
-    assert_equal(expr2, expected)
+        'bar': [
+            ('x', 'double'),
+            ('job', 'string')
+        ],
+        't1': [
+            ('key1', 'string'),
+            ('key2', 'string'),
+            ('value1', 'double')
+        ],
+        't2': [
+            ('key1', 'string'),
+            ('key2', 'string')
+        ]
+    }
+
+    def _table_from_schema(self, name, tname=None):
+        tname = tname or name
+        return api.table(self._schemas[name], tname)
+
+    def _case_multiple_joins(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+        t3 = self.con.table('star3')
+
+        predA = t1['foo_id'] == t2['foo_id']
+        predB = t1['bar_id'] == t3['bar_id']
+
+        what = (t1.left_join(t2, [predA])
+                .inner_join(t3, [predB])
+                .projection([t1, t2['value1'], t3['value2']]))
+        return what
 
+    def _case_join_between_joins(self):
+        t1 = api.table([
+            ('key1', 'string'),
+            ('key2', 'string'),
+            ('value1', 'double'),
+        ], 'first')
 
-def test_filter_aggregate_pushdown_predicate(table):
-    # In the case where we want to add a predicate to an aggregate
-    # expression after the fact, rather than having to backpedal and add it
-    # before calling aggregate.
-    #
-    # TODO (design decision): This could happen automatically when adding a
-    # predicate originating from the same root table; if an expression is
-    # created from field references from the aggregated table then it
-    # becomes a filter predicate applied on top of a view
+        t2 = api.table([
+            ('key1', 'string'),
+            ('value2', 'double'),
+        ], 'second')
 
-    pred = table.f > 0
-    metrics = [table.a.sum().name('total')]
-    agged = table.aggregate(metrics, by=['g'])
-    filtered = agged.filter([pred])
-    expected = table[pred].aggregate(metrics, by=['g'])
-    assert_equal(filtered, expected)
+        t3 = api.table([
+            ('key2', 'string'),
+            ('key3', 'string'),
+            ('value3', 'double'),
+        ], 'third')
 
+        t4 = api.table([
+            ('key3', 'string'),
+            ('value4', 'double')
+        ], 'fourth')
 
-def test_filter_on_literal_then_aggregate(table):
-    # Mostly just a smoketest, this used to error on construction
-    expr = table.filter(ibis.literal(True)).agg(lambda t: t.a.sum().name("total"))
-    assert expr.columns == ["total"]
+        left = t1.inner_join(t2, [('key1', 'key1')])[t1, t2.value2]
+        right = t3.inner_join(t4, [('key3', 'key3')])[t3, t4.value4]
 
+        joined = left.inner_join(right, [('key2', 'key2')])
 
-@pytest.mark.parametrize(
-    "case_fn",
-    [
-        param(lambda t: t.f.sum(), id="non_boolean"),
-        param(lambda t: t.f > 2, id="non_scalar"),
-    ],
+        # At one point, the expression simplification was resulting in bad refs
+        # here (right.value3 referencing the table inside the right join)
+        exprs = [left, right.value3, right.value4]
+        projected = joined.projection(exprs)
+
+        return projected
+
+    def _case_join_just_materialized(self):
+        t1 = self.con.table('tpch_nation')
+        t2 = self.con.table('tpch_region')
+        t3 = self.con.table('tpch_customer')
+
+        # GH #491
+        return (t1.inner_join(t2, t1.n_regionkey == t2.r_regionkey)
+                .inner_join(t3, t1.n_nationkey == t3.c_nationkey))
+
+    def _case_semi_anti_joins(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        sj = t1.semi_join(t2, [t1.foo_id == t2.foo_id])[[t1]]
+        aj = t1.anti_join(t2, [t1.foo_id == t2.foo_id])[[t1]]
+
+        return sj, aj
+
+    def _case_self_reference_simple(self):
+        t1 = self.con.table('star1')
+        return t1.view()
+
+    def _case_self_reference_join(self):
+        t1 = self.con.table('star1')
+        t2 = t1.view()
+        return t1.inner_join(t2, [t1.foo_id == t2.bar_id])[[t1]]
+
+    def _case_join_projection_subquery_bug(self):
+        # From an observed bug, derived from tpch tables
+        geo = (nation.inner_join(region, [('n_regionkey', 'r_regionkey')])
+               [nation.n_nationkey,
+                nation.n_name.name('nation'),
+                region.r_name.name('region')])
+
+        expr = (geo.inner_join(customer, [('n_nationkey', 'c_nationkey')])
+                [customer, geo])
+
+        return expr
+
+    def _case_where_simple_comparisons(self):
+        t1 = self.con.table('star1')
+
+        what = t1.filter([t1.f > 0, t1.c < t1.f * 2])
+
+        return what
+
+    def _case_where_with_join(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        # This also tests some cases of predicate pushdown
+        e1 = (t1.inner_join(t2, [t1.foo_id == t2.foo_id])
+              .projection([t1, t2.value1, t2.value3])
+              .filter([t1.f > 0, t2.value3 < 1000]))
+
+        e2 = (t1.inner_join(t2, [t1.foo_id == t2.foo_id])
+              .filter([t1.f > 0, t2.value3 < 1000])
+              .projection([t1, t2.value1, t2.value3]))
+
+        return e1, e2
+
+    def _case_subquery_used_for_self_join(self):
+        # There could be cases that should look in SQL like
+        # WITH t0 as (some subquery)
+        # select ...
+        # from t0 t1
+        #   join t0 t2
+        #     on t1.kind = t2.subkind
+        # ...
+        # However, the Ibis code will simply have an expression (projection or
+        # aggregation, say) built on top of the subquery expression, so we need
+        # to extract the subquery unit (we see that it appears multiple times
+        # in the tree).
+        t = self.con.table('alltypes')
+
+        agged = t.aggregate([t.f.sum().name('total')], by=['g', 'a', 'b'])
+        view = agged.view()
+        metrics = [(agged.total - view.total).max().name('metric')]
+        expr = (agged.inner_join(view, [agged.a == view.b])
+                .aggregate(metrics, by=[agged.g]))
+
+        return expr
+
+    def _case_subquery_factor_correlated_subquery(self):
+        region = self.con.table('tpch_region')
+        nation = self.con.table('tpch_nation')
+        customer = self.con.table('tpch_customer')
+        orders = self.con.table('tpch_orders')
+
+        fields_of_interest = [customer,
+                              region.r_name.name('region'),
+                              orders.o_totalprice.name('amount'),
+                              orders.o_orderdate
+                              .cast('timestamp').name('odate')]
+
+        tpch = (region.join(nation, region.r_regionkey == nation.n_regionkey)
+                .join(customer, customer.c_nationkey == nation.n_nationkey)
+                .join(orders, orders.o_custkey == customer.c_custkey)
+                [fields_of_interest])
+
+        # Self-reference + correlated subquery complicates things
+        t2 = tpch.view()
+        conditional_avg = t2[t2.region == tpch.region].amount.mean()
+        amount_filter = tpch.amount > conditional_avg
+
+        return tpch[amount_filter].limit(10)
+
+    def _case_self_join_subquery_distinct_equal(self):
+        region = self.con.table('tpch_region')
+        nation = self.con.table('tpch_nation')
+
+        j1 = (region.join(nation, region.r_regionkey == nation.n_regionkey)
+              [region, nation])
+
+        j2 = (region.join(nation, region.r_regionkey == nation.n_regionkey)
+              [region, nation].view())
+
+        expr = (j1.join(j2, j1.r_regionkey == j2.r_regionkey)
+                [j1.r_name, j2.n_name])
+
+        return expr
+
+    def _case_cte_factor_distinct_but_equal(self):
+        t = self.con.table('alltypes')
+        tt = self.con.table('alltypes')
+
+        expr1 = t.group_by('g').aggregate(t.f.sum().name('metric'))
+        expr2 = tt.group_by('g').aggregate(tt.f.sum().name('metric')).view()
+
+        expr = expr1.join(expr2, expr1.g == expr2.g)[[expr1]]
+
+        return expr
+
+    def _case_tpch_self_join_failure(self):
+        # duplicating the integration test here
+
+        region = self.con.table('tpch_region')
+        nation = self.con.table('tpch_nation')
+        customer = self.con.table('tpch_customer')
+        orders = self.con.table('tpch_orders')
+
+        fields_of_interest = [
+            region.r_name.name('region'),
+            nation.n_name.name('nation'),
+            orders.o_totalprice.name('amount'),
+            orders.o_orderdate.cast('timestamp').name('odate')]
+
+        joined_all = (
+            region.join(nation, region.r_regionkey == nation.n_regionkey)
+            .join(customer, customer.c_nationkey == nation.n_nationkey)
+            .join(orders, orders.o_custkey == customer.c_custkey)
+            [fields_of_interest])
+
+        year = joined_all.odate.year().name('year')
+        total = joined_all.amount.sum().cast('double').name('total')
+        annual_amounts = (joined_all
+                          .group_by(['region', year])
+                          .aggregate(total))
+
+        current = annual_amounts
+        prior = annual_amounts.view()
+
+        yoy_change = (current.total - prior.total).name('yoy_change')
+        yoy = (current.join(prior, current.year == (prior.year - 1))
+               [current.region, current.year, yoy_change])
+        return yoy
+
+    def _case_subquery_in_filter_predicate(self):
+        # E.g. comparing against some scalar aggregate value. See Ibis #43
+        t1 = self.con.table('star1')
+
+        pred = t1.f > t1.f.mean()
+        expr = t1[pred]
+
+        # This brought out another expression rewriting bug, since the filtered
+        # table isn't found elsewhere in the expression.
+        pred2 = t1.f > t1[t1.foo_id == 'foo'].f.mean()
+        expr2 = t1[pred2]
+
+        return expr, expr2
+
+    def _case_filter_subquery_derived_reduction(self):
+        t1 = self.con.table('star1')
+
+        # Reduction can be nested inside some scalar expression
+        pred3 = t1.f > t1[t1.foo_id == 'foo'].f.mean().log()
+        pred4 = t1.f > (t1[t1.foo_id == 'foo'].f.mean().log() + 1)
+
+        expr3 = t1[pred3]
+        expr4 = t1[pred4]
+
+        return expr3, expr4
+
+    def _case_topk_operation(self):
+        # TODO: top K with filter in place
+
+        table = api.table([
+            ('foo', 'string'),
+            ('bar', 'string'),
+            ('city', 'string'),
+            ('v1', 'double'),
+            ('v2', 'double'),
+        ], 'tbl')
+
+        what = table.city.topk(10, by=table.v2.mean())
+        e1 = table[what]
+
+        # Test the default metric (count)
+        what = table.city.topk(10)
+        e2 = table[what]
+
+        return e1, e2
+
+    def _case_simple_aggregate_query(self):
+        t1 = self.con.table('star1')
+        cases = [
+            t1.aggregate([t1['f'].sum().name('total')],
+                         [t1['foo_id']]),
+            t1.aggregate([t1['f'].sum().name('total')],
+                         ['foo_id', 'bar_id'])
+        ]
+
+        return cases
+
+    def _case_aggregate_having(self):
+        # Filtering post-aggregation predicate
+        t1 = self.con.table('star1')
+
+        total = t1.f.sum().name('total')
+        metrics = [total]
+
+        e1 = t1.aggregate(metrics, by=['foo_id'], having=[total > 10])
+        e2 = t1.aggregate(metrics, by=['foo_id'], having=[t1.count() > 100])
+
+        return e1, e2
+
+    def _case_aggregate_count_joined(self):
+        # count on more complicated table
+        region = self.con.table('tpch_region')
+        nation = self.con.table('tpch_nation')
+        join_expr = region.r_regionkey == nation.n_regionkey
+        joined = region.inner_join(nation, join_expr)
+        table_ref = joined[nation, region.r_name.name('region')]
+
+        return table_ref.count()
+
+    def _case_sort_by(self):
+        table = self.con.table('star1')
+
+        return [
+            table.sort_by('f'),
+            table.sort_by(('f', 0)),
+            table.sort_by(['c', ('f', 0)])
+        ]
+
+    def _case_limit(self):
+        star1 = self.con.table('star1')
+
+        cases = [
+            star1.limit(10),
+            star1.limit(10, offset=5),
+            star1[star1.f > 0].limit(10),
+
+            # Semantically, this should produce a subquery
+            star1.limit(10)[lambda x: x.f > 0]
+        ]
+
+        return cases
+
+    foo = _table_wrapper('foo')
+    bar = _table_wrapper('bar')
+    t1 = _table_wrapper('t1', 'foo')
+    t2 = _table_wrapper('t2', 'bar')
+
+    def _case_where_uncorrelated_subquery(self):
+        return self.foo[self.foo.job.isin(self.bar.job)]
+
+    def _case_where_correlated_subquery(self):
+        t1 = self.foo
+        t2 = t1.view()
+
+        stat = t2[t1.dept_id == t2.dept_id].y.mean()
+        return t1[t1.y > stat]
+
+    def _case_exists(self):
+        t1, t2 = self.t1, self.t2
+
+        cond = (t1.key1 == t2.key1).any()
+        expr = t1[cond]
+
+        cond2 = ((t1.key1 == t2.key1) & (t2.key2 == 'foo')).any()
+        expr2 = t1[cond2]
+
+        return expr, expr2
+
+    def _case_not_exists(self):
+        t1, t2 = self.t1, self.t2
+
+        cond = (t1.key1 == t2.key1).any()
+        return t1[-cond]
+
+    def _case_join_with_limited_table(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        limited = t1.limit(100)
+        joined = (limited.inner_join(t2, [limited.foo_id == t2.foo_id])
+                  [[limited]])
+        return joined
+
+    def _case_union(self, distinct=False):
+        table = self.con.table('functional_alltypes')
+
+        t1 = (table[table.int_col > 0]
+              [table.string_col.name('key'),
+               table.float_col.cast('double').name('value')])
+        t2 = (table[table.int_col <= 0]
+                   [table.string_col.name('key'),
+                    table.double_col.name('value')])
+
+        expr = t1.union(t2, distinct=distinct)
+
+        return expr
+
+    def _case_simple_case(self):
+        t = self.con.table('alltypes')
+        return (t.g.case()
+                .when('foo', 'bar')
+                .when('baz', 'qux')
+                .else_('default')
+                .end())
+
+    def _case_search_case(self):
+        t = self.con.table('alltypes')
+        return (ibis.case()
+                .when(t.f > 0, t.d * 2)
+                .when(t.c < 0, t.a * 2)
+                .end())
+
+    def _case_self_reference_in_exists(self):
+        t = self.con.table('functional_alltypes')
+        t2 = t.view()
+
+        cond = (t.string_col == t2.string_col).any()
+        semi = t[cond]
+        anti = t[-cond]
+
+        return semi, anti
+
+    def _case_self_reference_limit_exists(self):
+        alltypes = self.con.table('functional_alltypes')
+        t = alltypes.limit(100)
+        t2 = t.view()
+        return t[-(t.string_col == t2.string_col).any()]
+
+    def _case_limit_cte_extract(self):
+        alltypes = self.con.table('functional_alltypes')
+        t = alltypes.limit(100)
+        t2 = t.view()
+        return t.join(t2).projection(t)
+
+    def _case_subquery_aliased(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        agged = t1.aggregate([t1.f.sum().name('total')], by=['foo_id'])
+        what = (agged.inner_join(t2, [agged.foo_id == t2.foo_id])
+                [agged, t2.value1])
+
+        return what
+
+    def _case_filter_self_join_analysis_bug(self):
+        purchases = ibis.table([('region', 'string'),
+                                ('kind', 'string'),
+                                ('user', 'int64'),
+                                ('amount', 'double')], 'purchases')
+
+        metric = purchases.amount.sum().name('total')
+        agged = (purchases.group_by(['region', 'kind'])
+                 .aggregate(metric))
+
+        left = agged[agged.kind == 'foo']
+        right = agged[agged.kind == 'bar']
+
+        joined = left.join(right, left.region == right.region)
+        result = joined[left.region,
+                        (left.total - right.total).name('diff')]
+
+        return result, purchases
+
+    def _case_projection_fuse_filter(self):
+        # Probably test this during the evaluation phase. In SQL, "fusable"
+        # table operations will be combined together into a single select
+        # statement
+        #
+        # see ibis #71 for more on this
+
+        t = ibis.table([
+            ('a', 'int8'),
+            ('b', 'int16'),
+            ('c', 'int32'),
+            ('d', 'int64'),
+            ('e', 'float'),
+            ('f', 'double'),
+            ('g', 'string'),
+            ('h', 'boolean')
+        ], 'foo')
+
+        proj = t['a', 'b', 'c']
+
+        # Rewrite a little more aggressively here
+        expr1 = proj[t.a > 0]
+
+        # at one point these yielded different results
+        filtered = t[t.a > 0]
+
+        expr2 = filtered[t.a, t.b, t.c]
+        expr3 = filtered.projection(['a', 'b', 'c'])
+
+        return expr1, expr2, expr3
+
+
+class TestSelectSQL(unittest.TestCase, ExprTestCases):
+
+    @classmethod
+    def setUpClass(cls):
+        cls.con = MockConnection()
+
+    def _compare_sql(self, expr, expected):
+        result = to_sql(expr)
+        assert result == expected
+
+    def test_nameless_table(self):
+        # Ensure that user gets some kind of sensible error
+        nameless = api.table([('key', 'string')])
+        self.assertRaises(com.RelationError, to_sql, nameless)
+
+        with_name = api.table([('key', 'string')], name='baz')
+        result = to_sql(with_name)
+        assert result == 'SELECT *\nFROM baz'
+
+    def test_physical_table_reference_translate(self):
+        # If an expression's table leaves all reference database tables, verify
+        # we translate correctly
+        table = self.con.table('alltypes')
+
+        query = _get_query(table)
+        sql_string = query.compile()
+        expected = "SELECT *\nFROM alltypes"
+        assert sql_string == expected
+
+    def test_simple_joins(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        pred = t1['foo_id'] == t2['foo_id']
+        pred2 = t1['bar_id'] == t2['foo_id']
+        cases = [
+            (t1.inner_join(t2, [pred])[[t1]],
+             """SELECT t0.*
+FROM star1 t0
+  INNER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`"""),
+            (t1.left_join(t2, [pred])[[t1]],
+             """SELECT t0.*
+FROM star1 t0
+  LEFT OUTER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`"""),
+            (t1.outer_join(t2, [pred])[[t1]],
+             """SELECT t0.*
+FROM star1 t0
+  FULL OUTER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`"""),
+            # multiple predicates
+            (t1.inner_join(t2, [pred, pred2])[[t1]],
+             """SELECT t0.*
+FROM star1 t0
+  INNER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id` AND
+       t0.`bar_id` = t1.`foo_id`"""),
+        ]
+
+        for expr, expected_sql in cases:
+            result_sql = to_sql(expr)
+            assert result_sql == expected_sql
+
+    def test_multiple_joins(self):
+        what = self._case_multiple_joins()
+
+        result_sql = to_sql(what)
+        expected_sql = """SELECT t0.*, t1.`value1`, t2.`value2`
+FROM star1 t0
+  LEFT OUTER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`
+  INNER JOIN star3 t2
+    ON t0.`bar_id` = t2.`bar_id`"""
+        assert result_sql == expected_sql
+
+    def test_join_between_joins(self):
+        projected = self._case_join_between_joins()
+
+        result = to_sql(projected)
+        expected = """SELECT t0.*, t1.`value3`, t1.`value4`
+FROM (
+  SELECT t2.*, t3.`value2`
+  FROM `first` t2
+    INNER JOIN second t3
+      ON t2.`key1` = t3.`key1`
+) t0
+  INNER JOIN (
+    SELECT t2.*, t3.`value4`
+    FROM third t2
+      INNER JOIN fourth t3
+        ON t2.`key3` = t3.`key3`
+  ) t1
+    ON t0.`key2` = t1.`key2`"""
+        assert result == expected
+
+    def test_join_just_materialized(self):
+        joined = self._case_join_just_materialized()
+        result = to_sql(joined)
+        expected = """SELECT *
+FROM tpch_nation t0
+  INNER JOIN tpch_region t1
+    ON t0.`n_regionkey` = t1.`r_regionkey`
+  INNER JOIN tpch_customer t2
+    ON t0.`n_nationkey` = t2.`c_nationkey`"""
+        assert result == expected
+
+        result = to_sql(joined.materialize())
+        assert result == expected
+
+    def test_join_no_predicates_for_impala(self):
+        # Impala requires that joins without predicates be written explicitly
+        # as CROSS JOIN, since result sets can accidentally get too large if a
+        # query is executed before predicates are written
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        joined2 = t1.cross_join(t2)[[t1]]
+
+        expected = """SELECT t0.*
+FROM star1 t0
+  CROSS JOIN star2 t1"""
+        result2 = to_sql(joined2)
+        assert result2 == expected
+
+        for jtype in ['inner_join', 'left_join', 'outer_join']:
+            joined = getattr(t1, jtype)(t2)[[t1]]
+
+            result = to_sql(joined)
+            assert result == expected
+
+    def test_semi_anti_joins(self):
+        sj, aj = self._case_semi_anti_joins()
+
+        result = to_sql(sj)
+        expected = """SELECT t0.*
+FROM star1 t0
+  LEFT SEMI JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`"""
+        assert result == expected
+
+        result = to_sql(aj)
+        expected = """SELECT t0.*
+FROM star1 t0
+  LEFT ANTI JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`"""
+        assert result == expected
+
+    def test_self_reference_simple(self):
+        expr = self._case_self_reference_simple()
+
+        result_sql = to_sql(expr)
+        expected_sql = "SELECT *\nFROM star1"
+        assert result_sql == expected_sql
+
+    def test_join_self_reference(self):
+        result = self._case_self_reference_join()
+
+        result_sql = to_sql(result)
+        expected_sql = """SELECT t0.*
+FROM star1 t0
+  INNER JOIN star1 t1
+    ON t0.`foo_id` = t1.`bar_id`"""
+        assert result_sql == expected_sql
+
+    def test_join_projection_subquery_broken_alias(self):
+        expr = self._case_join_projection_subquery_bug()
+
+        result = to_sql(expr)
+        expected = """SELECT t1.*, t0.*
+FROM (
+  SELECT t2.`n_nationkey`, t2.`n_name` AS `nation`, t3.`r_name` AS `region`
+  FROM nation t2
+    INNER JOIN region t3
+      ON t2.`n_regionkey` = t3.`r_regionkey`
+) t0
+  INNER JOIN customer t1
+    ON t0.`n_nationkey` = t1.`c_nationkey`"""
+        assert result == expected
+
+    def test_where_simple_comparisons(self):
+        what = self._case_where_simple_comparisons()
+        result = to_sql(what)
+        expected = """SELECT *
+FROM star1
+WHERE `f` > 0 AND
+      `c` < (`f` * 2)"""
+        assert result == expected
+
+    def test_where_in_array_literal(self):
+        # e.g.
+        # where string_col in (v1, v2, v3)
+        raise unittest.SkipTest
+
+    def test_where_with_join(self):
+        e1, e2 = self._case_where_with_join()
+
+        expected_sql = """SELECT t0.*, t1.`value1`, t1.`value3`
+FROM star1 t0
+  INNER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`
+WHERE t0.`f` > 0 AND
+      t1.`value3` < 1000"""
+
+        result_sql = to_sql(e1)
+        assert result_sql == expected_sql
+
+        result2_sql = to_sql(e2)
+        assert result2_sql == expected_sql
+
+    def test_where_no_pushdown_possible(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        joined = (t1.inner_join(t2, [t1.foo_id == t2.foo_id])
+                  [t1, (t1.f - t2.value1).name('diff')])
+
+        filtered = joined[joined.diff > 1]
+
+        # TODO: I'm not sure if this is exactly what we want
+        expected_sql = """SELECT *
+FROM (
+  SELECT t0.*, t0.`f` - t1.`value1` AS `diff`
+  FROM star1 t0
+    INNER JOIN star2 t1
+      ON t0.`foo_id` = t1.`foo_id`
+  WHERE t0.`f` > 0 AND
+        t1.`value3` < 1000
 )
-def test_aggregate_post_predicate(table, case_fn):
-    # Test invalid having clause
-    metrics = [table.f.sum().name('total')]
-    by = ['g']
-    having = [case_fn(table)]
-
-    with pytest.raises(com.IbisTypeError):
-        table.aggregate(metrics, by=by, having=having)
-
-
-def test_group_by_having_api(table):
-    # #154, add a HAVING post-predicate in a composable way
-    metric = table.f.sum().name('foo')
-    postp = table.d.mean() > 1
-
-    expr = table.group_by('g').having(postp).aggregate(metric)
-
-    expected = table.aggregate(metric, by='g', having=postp)
-    assert_equal(expr, expected)
-
-
-def test_group_by_kwargs(table):
-    t = table
-    expr = t.group_by(['f', t.h], z='g', z2=t.d).aggregate(t.d.mean().name('foo'))
-    expected = t.group_by(['f', t.h, t.g.name('z'), t.d.name('z2')]).aggregate(
-        t.d.mean().name('foo')
-    )
-    assert_equal(expr, expected)
+WHERE `diff` > 1"""
 
+        raise unittest.SkipTest
 
-def test_compound_aggregate_expr(table):
-    # See ibis #24
-    compound_expr = (table['a'].sum() / table['a'].mean()).name('foo')
-    assert an.is_reduction(compound_expr.op())
+        result_sql = to_sql(filtered)
+        assert result_sql == expected_sql
 
-    # Validates internally
-    table.aggregate([compound_expr])
+    def test_where_with_between(self):
+        t = self.con.table('alltypes')
+
+        what = t.filter([t.a > 0, t.f.between(0, 1)])
+        result = to_sql(what)
+        expected = """SELECT *
+FROM alltypes
+WHERE `a` > 0 AND
+      `f` BETWEEN 0 AND 1"""
+        assert result == expected
+
+    def test_where_analyze_scalar_op(self):
+        # root cause of #310
+
+        table = self.con.table('functional_alltypes')
+
+        expr = (table.filter([table.timestamp_col <
+                             (ibis.timestamp('2010-01-01') + ibis.month(3)),
+                             table.timestamp_col < (ibis.now() +
+                                                    ibis.day(10))])
+                .count())
+
+        result = to_sql(expr)
+        expected = """\
+SELECT count(*) AS `count`
+FROM functional_alltypes
+WHERE `timestamp_col` < months_add('2010-01-01 00:00:00', 3) AND
+      `timestamp_col` < days_add(now(), 10)"""
+        assert result == expected
+
+    def test_bug_duplicated_where(self):
+        # GH #539
+        table = self.con.table('airlines')
+
+        t = table['arrdelay', 'dest']
+        expr = (t.group_by('dest')
+                .mutate(dest_avg=t.arrdelay.mean(),
+                        dev=t.arrdelay - t.arrdelay.mean()))
+
+        worst = expr[expr.dev.notnull()].sort_by(ibis.desc('dev')).limit(10)
+        result = to_sql(worst)
+        expected = """\
+SELECT *
+FROM (
+  SELECT `arrdelay`, `dest`,
+         avg(`arrdelay`) OVER (PARTITION BY `dest`) AS `dest_avg`,
+         `arrdelay` - avg(`arrdelay`) OVER (PARTITION BY `dest`) AS `dev`
+  FROM airlines
+) t0
+WHERE `dev` IS NOT NULL
+ORDER BY `dev` DESC
+LIMIT 10"""
+        assert result == expected
+
+    def test_simple_aggregate_query(self):
+        expected = [
+            """SELECT `foo_id`, sum(`f`) AS `total`
+FROM star1
+GROUP BY 1""",
+            """SELECT `foo_id`, `bar_id`, sum(`f`) AS `total`
+FROM star1
+GROUP BY 1, 2"""
+        ]
+
+        cases = self._case_simple_aggregate_query()
+        for expr, expected_sql in zip(cases, expected):
+            result_sql = to_sql(expr)
+            assert result_sql == expected_sql
+
+    def test_aggregate_having(self):
+        e1, e2 = self._case_aggregate_having()
+
+        result = to_sql(e1)
+        expected = """SELECT `foo_id`, sum(`f`) AS `total`
+FROM star1
+GROUP BY 1
+HAVING sum(`f`) > 10"""
+        assert result == expected
+
+        result = to_sql(e2)
+        expected = """SELECT `foo_id`, sum(`f`) AS `total`
+FROM star1
+GROUP BY 1
+HAVING count(*) > 100"""
+        assert result == expected
+
+    def test_aggregate_table_count_metric(self):
+        expr = self.con.table('star1').count()
+
+        result = to_sql(expr)
+        expected = """SELECT count(*) AS `count`
+FROM star1"""
+        assert result == expected
+
+    def test_aggregate_count_joined(self):
+        expr = self._case_aggregate_count_joined()
+
+        result = to_sql(expr)
+        expected = """SELECT count(*) AS `count`
+FROM (
+  SELECT t2.*, t1.`r_name` AS `region`
+  FROM tpch_region t1
+    INNER JOIN tpch_nation t2
+      ON t1.`r_regionkey` = t2.`n_regionkey`
+) t0"""
+        assert result == expected
+
+    def test_expr_template_field_name_binding(self):
+        # Given an expression with no concrete links to actual database tables,
+        # indicate a mapping between the distinct unbound table leaves of the
+        # expression and some database tables with compatible schemas but
+        # potentially different column names
+        pass
 
+    def test_no_aliases_needed(self):
+        table = api.table([
+            ('key1', 'string'),
+            ('key2', 'string'),
+            ('value', 'double')
+        ])
 
-def test_groupby_convenience(table):
-    metrics = [table.f.sum().name('total')]
+        expr = table.aggregate([table['value'].sum().name('total')],
+                               by=['key1', 'key2'])
 
-    expr = table.group_by('g').aggregate(metrics)
-    expected = table.aggregate(metrics, by=['g'])
-    assert_equal(expr, expected)
+        query = _get_query(expr)
+        context = query.context
+        assert not context.need_aliases()
 
-    group_expr = table.g.cast('double').name('g')
-    expr = table.group_by(group_expr).aggregate(metrics)
-    expected = table.aggregate(metrics, by=[group_expr])
-    assert_equal(expr, expected)
+    def test_table_names_overlap_default_aliases(self):
+        # see discussion in #104; this actually is not needed for query
+        # correctness, and only makes the generated SQL nicer
+        raise unittest.SkipTest
 
+        t0 = api.table([
+            ('key', 'string'),
+            ('v1', 'double')
+        ], 't1')
 
-def test_group_by_count_size(table):
-    # #148, convenience for interactive use, and so forth
-    result1 = table.group_by('g').size()
-    result2 = table.group_by('g').count()
+        t1 = api.table([
+            ('key', 'string'),
+            ('v2', 'double')
+        ], 't0')
 
-    expected = table.group_by('g').aggregate(table.count())
+        expr = t0.join(t1, t0.key == t1.key)[t0.key, t0.v1, t1.v2]
 
-    assert_equal(result1, expected)
-    assert_equal(result2, expected)
+        result = to_sql(expr)
+        expected = """\
+SELECT t2.`key`, t2.`v1`, t3.`v2`
+FROM t0 t2
+  INNER JOIN t1 t3
+    ON t2.`key` = t3.`key`"""
 
+        assert result == expected
 
-def test_group_by_column_select_api(table):
-    grouped = table.group_by('g')
+    def test_context_aliases_multiple_join(self):
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+        t3 = self.con.table('star3')
 
-    result = grouped.f.sum()
-    expected = grouped.aggregate(table.f.sum().name('sum(f)'))
-    assert_equal(result, expected)
+        expr = (t1.left_join(t2, [t1['foo_id'] == t2['foo_id']])
+                .inner_join(t3, [t1['bar_id'] == t3['bar_id']])
+                [[t1, t2['value1'], t3['value2']]])
 
-    supported_functions = ['sum', 'mean', 'count', 'size', 'max', 'min']
+        query = _get_query(expr)
+        context = query.context
 
-    # make sure they all work
-    for fn in supported_functions:
-        getattr(grouped.f, fn)()
+        assert context.get_ref(t1) == 't0'
+        assert context.get_ref(t2) == 't1'
+        assert context.get_ref(t3) == 't2'
 
+    def test_fuse_projections(self):
+        table = api.table([
+            ('foo', 'int32'),
+            ('bar', 'int64'),
+            ('value', 'double')
+        ], name='tbl')
 
-def test_value_counts_convenience(table):
-    # #152
-    result = table.g.value_counts()
-    expected = table.select('g').group_by('g').aggregate(g_count=lambda t: t.count())
+        # Cases where we project in both cases using the base table reference
+        f1 = (table['foo'] + table['bar']).name('baz')
+        pred = table['value'] > 0
 
-    assert_equal(result, expected)
+        table2 = table[table, f1]
+        table2_filtered = table2[pred]
 
+        f2 = (table2['foo'] * 2).name('qux')
+        f3 = (table['foo'] * 2).name('qux')
 
-def test_isin_value_counts(table):
-    # #157, this code path was untested before
-    bool_clause = table.g.notin(['1', '4', '7'])
-    # it works!
-    bool_clause.name('notin').value_counts()
+        table3 = table2.projection([table2, f2])
 
+        # fusion works even if there's a filter
+        table3_filtered = table2_filtered.projection([table2, f2])
 
-def test_value_counts_unnamed_expr(con):
-    nation = con.table('tpch_nation')
+        expected = table[table, f1, f3]
+        expected2 = table[pred][table, f1, f3]
 
-    expr = nation.n_name.lower().value_counts()
-    expected = nation.n_name.lower().name('Lowercase(n_name)').value_counts()
-    assert_equal(expr, expected)
+        assert table3.equals(expected)
+        assert table3_filtered.equals(expected2)
 
+        ex_sql = """SELECT *, `foo` + `bar` AS `baz`, `foo` * 2 AS `qux`
+FROM tbl"""
 
-def test_aggregate_unnamed_expr(con):
-    nation = con.table('tpch_nation')
-    expr = nation.n_name.lower().left(1)
+        ex_sql2 = """SELECT *, `foo` + `bar` AS `baz`, `foo` * 2 AS `qux`
+FROM tbl
+WHERE `value` > 0"""
 
-    agg = nation.group_by(expr).aggregate(nation.count().name('metric'))
-    schema = agg.schema()
-    assert schema.names == ('Substring(Lowercase(n_name), 0, 1)', 'metric')
-    assert schema.types == (dt.string, dt.int64)
+        table3_sql = to_sql(table3)
+        table3_filt_sql = to_sql(table3_filtered)
 
+        assert table3_sql == ex_sql
+        assert table3_filt_sql == ex_sql2
 
-def test_join_no_predicate_list(con):
-    region = con.table('tpch_region')
-    nation = con.table('tpch_nation')
+        # Use the intermediate table refs
+        table3 = table2.projection([table2, f2])
 
-    pred = region.r_regionkey == nation.n_regionkey
-    joined = region.inner_join(nation, pred)
-    expected = region.inner_join(nation, [pred])
-    assert_equal(joined, expected)
+        # fusion works even if there's a filter
+        table3_filtered = table2_filtered.projection([table2, f2])
 
+        expected = table[table, f1, f3]
+        expected2 = table[pred][table, f1, f3]
 
-def test_join_deferred(con):
-    region = con.table("tpch_region")
-    nation = con.table("tpch_nation")
-    res = region.join(nation, _.r_regionkey == nation.n_regionkey)
-    exp = region.join(nation, region.r_regionkey == nation.n_regionkey)
-    assert_equal(res, exp)
+        assert table3.equals(expected)
+        assert table3_filtered.equals(expected2)
 
+    def test_projection_filter_fuse(self):
+        expr1, expr2, expr3 = self._case_projection_fuse_filter()
 
-def test_asof_join():
-    left = ibis.table([('time', 'int32'), ('value', 'double')])
-    right = ibis.table([('time', 'int32'), ('value2', 'double')])
-    joined = api.asof_join(left, right, 'time')
+        sql1 = to_sql(expr1)
+        sql2 = to_sql(expr2)
+        sql3 = to_sql(expr3)
 
-    assert joined.columns == [
-        "time",
-        "value",
-        "time_right",
-        "value2",
-    ]
-    pred = joined.op().table.predicates[0]
-    assert pred.left.name == pred.right.name == 'time'
+        assert sql1 == sql2
+        assert sql1 == sql3
 
+    def test_bug_project_multiple_times(self):
+        # 108
+        customer = self.con.table('tpch_customer')
+        nation = self.con.table('tpch_nation')
+        region = self.con.table('tpch_region')
 
-def test_asof_join_with_by():
-    left = ibis.table([('time', 'int32'), ('key', 'int32'), ('value', 'double')])
-    right = ibis.table([('time', 'int32'), ('key', 'int32'), ('value2', 'double')])
-    joined = api.asof_join(left, right, 'time', by='key')
-    assert joined.columns == [
-        "time",
-        "key",
-        "value",
-        "time_right",
-        "key_right",
-        "value2",
-    ]
-    by = joined.op().table.by[0]
-    assert by.left.name == by.right.name == 'key'
+        joined = (
+            customer.inner_join(nation,
+                                [customer.c_nationkey == nation.n_nationkey])
+            .inner_join(region,
+                        [nation.n_regionkey == region.r_regionkey])
+        )
+        proj1 = [customer, nation.n_name, region.r_name]
+        step1 = joined[proj1]
 
+        topk_by = step1.c_acctbal.cast('double').sum()
+        pred = step1.n_name.topk(10, by=topk_by)
 
-@pytest.mark.parametrize(
-    ('ibis_interval', 'timedelta_interval'),
-    [
-        [ibis.interval(days=2), pd.Timedelta('2 days')],
-        [ibis.interval(days=2), datetime.timedelta(days=2)],
-        [ibis.interval(hours=5), pd.Timedelta('5 hours')],
-        [ibis.interval(hours=5), datetime.timedelta(hours=5)],
-        [ibis.interval(minutes=7), pd.Timedelta('7 minutes')],
-        [ibis.interval(minutes=7), datetime.timedelta(minutes=7)],
-        [ibis.interval(seconds=9), pd.Timedelta('9 seconds')],
-        [ibis.interval(seconds=9), datetime.timedelta(seconds=9)],
-        [ibis.interval(milliseconds=11), pd.Timedelta('11 milliseconds')],
-        [ibis.interval(milliseconds=11), datetime.timedelta(milliseconds=11)],
-        [ibis.interval(microseconds=15), pd.Timedelta('15 microseconds')],
-        [ibis.interval(microseconds=15), datetime.timedelta(microseconds=15)],
-        [ibis.interval(nanoseconds=17), pd.Timedelta('17 nanoseconds')],
-    ],
-)
-def test_asof_join_with_tolerance(ibis_interval, timedelta_interval):
-    left = ibis.table([('time', 'int32'), ('key', 'int32'), ('value', 'double')])
-    right = ibis.table([('time', 'int32'), ('key', 'int32'), ('value2', 'double')])
-
-    joined = api.asof_join(left, right, 'time', tolerance=ibis_interval).op()
-    tolerance = joined.table.tolerance
-    assert_equal(tolerance, ibis_interval.op())
-
-    joined = api.asof_join(left, right, 'time', tolerance=timedelta_interval).op()
-    tolerance = joined.table.tolerance
-    assert isinstance(tolerance.to_expr(), ir.IntervalScalar)
-    assert isinstance(tolerance, ops.Literal)
-
-
-def test_equijoin_schema_merge():
-    table1 = ibis.table([('key1', 'string'), ('value1', 'double')])
-    table2 = ibis.table([('key2', 'string'), ('stuff', 'int32')])
-
-    pred = table1['key1'] == table2['key2']
-    join_types = ['inner_join', 'left_join', 'outer_join']
-
-    ex_schema = ibis.schema(
-        names=['key1', 'value1', 'key2', 'stuff'],
-        types=['string', 'double', 'string', 'int32'],
-    )
-
-    for fname in join_types:
-        f = getattr(table1, fname)
-        joined = f(table2, [pred])
-        assert_equal(joined.schema(), ex_schema)
-
-
-def test_join_combo_with_projection(table):
-    # Test a case where there is column name overlap, but the projection
-    # passed makes it a non-issue. Highly relevant with self-joins
-    #
-    # For example, where left/right have some field names in common:
-    # SELECT left.*, right.a, right.b
-    # FROM left join right on left.key = right.key
-    t = table
-    t2 = t.mutate(foo=t.f * 2, bar=t.f * 4)
-
-    # this works
-    joined = t.left_join(t2, [t['g'] == t2['g']])
-    proj = joined.select([t, t2['foo'], t2['bar']])
-    repr(proj)
-
-
-def test_join_getitem_projection(con):
-    region = con.table('tpch_region')
-    nation = con.table('tpch_nation')
-
-    pred = region.r_regionkey == nation.n_regionkey
-    joined = region.inner_join(nation, pred)
-
-    result = joined[nation]
-    expected = joined.select(nation)
-    assert_equal(result, expected)
-
-
-def test_self_join(table):
-    # Self-joins are problematic with this design because column
-    # expressions may reference either the left or right  For example:
-    #
-    # SELECT left.key, sum(left.value - right.value) as total_deltas
-    # FROM table left
-    #  INNER JOIN table right
-    #    ON left.current_period = right.previous_period + 1
-    # GROUP BY 1
-    #
-    # One way around the self-join issue is to force the user to add
-    # prefixes to the joined fields, then project using those. Not that
-    # satisfying, though.
-    left = table
-    right = table.view()
-    metric = (left['a'] - right['b']).mean().name('metric')
-
-    joined = left.inner_join(right, [right['g'] == left['g']])
-
-    # Project out left table schema
-    proj = joined[[left]]
-    assert_equal(proj.schema(), left.schema())
-
-    # Try aggregating on top of joined
-    aggregated = joined.aggregate([metric], by=[left['g']])
-    ex_schema = api.Schema({'g': 'string', 'metric': 'double'})
-    assert_equal(aggregated.schema(), ex_schema)
-
-
-def test_self_join_no_view_convenience(table):
-    # #165, self joins ought to be possible when the user specifies the
-    # column names to join on rather than referentially-valid expressions
-
-    result = table.join(table, [('g', 'g')])
-    expected_cols = list(table.columns)
-    expected_cols.extend(f"{c}_right" for c in table.columns if c != 'g')
-    assert result.columns == expected_cols
-
-
-def test_join_reference_bug(con):
-    # GH#403
-    orders = con.table('tpch_orders')
-    customer = con.table('tpch_customer')
-    lineitem = con.table('tpch_lineitem')
-
-    items = orders.join(lineitem, orders.o_orderkey == lineitem.l_orderkey)[
-        lineitem, orders.o_custkey, orders.o_orderpriority
-    ].join(customer, [('o_custkey', 'c_custkey')])
-    items['o_orderpriority'].value_counts()
-
-
-def test_join_project_after(table):
-    # e.g.
-    #
-    # SELECT L.foo, L.bar, R.baz, R.qux
-    # FROM table1 L
-    #   INNER JOIN table2 R
-    #     ON L.key = R.key
-    #
-    # or
-    #
-    # SELECT L.*, R.baz
-    # ...
-    #
-    # The default for a join is selecting all fields if possible
-    table1 = ibis.table([('key1', 'string'), ('value1', 'double')])
-    table2 = ibis.table([('key2', 'string'), ('stuff', 'int32')])
-
-    pred = table1['key1'] == table2['key2']
-
-    joined = table1.left_join(table2, [pred])
-    projected = joined.select([table1, table2['stuff']])
-    assert projected.schema().names == ('key1', 'value1', 'stuff')
-
-    projected = joined.select([table2, table1['key1']])
-    assert projected.schema().names == ('key2', 'stuff', 'key1')
-
-
-def test_semi_join_schema(table):
-    # A left semi join discards the schema of the right table
-    table1 = ibis.table([('key1', 'string'), ('value1', 'double')])
-    table2 = ibis.table([('key2', 'string'), ('stuff', 'double')])
-
-    pred = table1['key1'] == table2['key2']
-    semi_joined = table1.semi_join(table2, [pred])
-
-    result_schema = semi_joined.schema()
-    assert_equal(result_schema, table1.schema())
-
-
-def test_cross_join(table):
-    metrics = [
-        table['a'].sum().name('sum_a'),
-        table['b'].mean().name('mean_b'),
-    ]
-    scalar_aggs = table.aggregate(metrics)
-
-    joined = table.cross_join(scalar_aggs)
-    agg_schema = api.Schema({'sum_a': 'int64', 'mean_b': 'double'})
-    ex_schema = table.schema() | agg_schema
-    assert_equal(joined.schema(), ex_schema)
-
-
-def test_cross_join_multiple(table):
-    a = table['a', 'b', 'c']
-    b = table['d', 'e']
-    c = table['f', 'h']
-
-    joined = ibis.cross_join(a, b, c)
-    expected = a.cross_join(b.cross_join(c))
-    assert joined.equals(expected)
-
-
-def test_filter_join():
-    table1 = ibis.table({'key1': 'string', 'key2': 'string', 'value1': 'double'})
-    table2 = ibis.table({'key3': 'string', 'value2': 'double'})
-
-    # It works!
-    joined = table1.inner_join(table2, [table1['key1'] == table2['key3']])
-    filtered = joined.filter([table1.value1 > 0])
-    repr(filtered)
-
-
-def test_inner_join_overlapping_column_names():
-    t1 = ibis.table([('foo', 'string'), ('bar', 'string'), ('value1', 'double')])
-    t2 = ibis.table([('foo', 'string'), ('bar', 'string'), ('value2', 'double')])
-
-    joined = t1.join(t2, 'foo')
-    expected = t1.join(t2, t1.foo == t2.foo)
-    assert_equal(joined, expected)
-    assert joined.columns == ["foo", "bar", "value1", "bar_right", "value2"]
-
-    joined = t1.join(t2, ['foo', 'bar'])
-    expected = t1.join(t2, [t1.foo == t2.foo, t1.bar == t2.bar])
-    assert_equal(joined, expected)
-    assert joined.columns == ["foo", "bar", "value1", "value2"]
-
-    # Equality predicates don't have same name, need to rename
-    joined = t1.join(t2, t1.foo == t2.bar)
-    assert joined.columns == [
-        "foo",
-        "bar",
-        "value1",
-        "foo_right",
-        "bar_right",
-        "value2",
-    ]
-
-    # Not all predicates are equality, still need to rename
-    joined = t1.join(t2, ["foo", t1.value1 < t2.value2])
-    assert joined.columns == [
-        "foo",
-        "bar",
-        "value1",
-        "foo_right",
-        "bar_right",
-        "value2",
-    ]
-
-
-def test_join_key_alternatives(con):
-    t1 = con.table('star1')
-    t2 = con.table('star2')
-
-    # Join with tuples
-    joined = t1.inner_join(t2, [('foo_id', 'foo_id')])
-    joined2 = t1.inner_join(t2, [(t1.foo_id, t2.foo_id)])
-
-    # Join with single expr
-    joined3 = t1.inner_join(t2, t1.foo_id == t2.foo_id)
-
-    expected = t1.inner_join(t2, [t1.foo_id == t2.foo_id])
-
-    assert_equal(joined, expected)
-    assert_equal(joined2, expected)
-    assert_equal(joined3, expected)
-
-    with pytest.raises(com.ExpressionError):
-        t1.inner_join(t2, [('foo_id', 'foo_id', 'foo_id')])
-
-
-def test_join_invalid_refs(con):
-    t1 = con.table('star1')
-    t2 = con.table('star2')
-    t3 = con.table('star3')
-
-    predicate = t1.bar_id == t3.bar_id
-    with pytest.raises(com.RelationError):
-        t1.inner_join(t2, [predicate])
-
-
-def test_join_invalid_expr_type(con):
-    left = con.table('star1')
-    invalid_right = left.foo_id
-    join_key = ['bar_id']
-
-    with pytest.raises(com.IbisTypeError, match="Argument is not a table"):
-        left.inner_join(invalid_right, join_key)
-
-
-def test_join_non_boolean_expr(con):
-    t1 = con.table('star1')
-    t2 = con.table('star2')
-
-    # oops
-    predicate = t1.f * t2.value1
-    with pytest.raises(com.ExpressionError):
-        t1.inner_join(t2, [predicate])
-
-
-def test_unravel_compound_equijoin(table):
-    t1 = ibis.table(
-        [
+        proj_exprs = [step1.c_name, step1.r_name, step1.n_name]
+        step2 = step1[pred]
+        expr = step2.projection(proj_exprs)
+
+        # it works!
+        result = to_sql(expr)
+        expected = """\
+SELECT `c_name`, `r_name`, `n_name`
+FROM (
+  SELECT t1.*, t2.`n_name`, t3.`r_name`
+  FROM tpch_customer t1
+    INNER JOIN tpch_nation t2
+      ON t1.`c_nationkey` = t2.`n_nationkey`
+    INNER JOIN tpch_region t3
+      ON t2.`n_regionkey` = t3.`r_regionkey`
+    LEFT SEMI JOIN (
+      SELECT t2.`n_name`, sum(CAST(t1.`c_acctbal` AS double)) AS `sum`
+      FROM tpch_customer t1
+        INNER JOIN tpch_nation t2
+          ON t1.`c_nationkey` = t2.`n_nationkey`
+        INNER JOIN tpch_region t3
+          ON t2.`n_regionkey` = t3.`r_regionkey`
+      GROUP BY 1
+      ORDER BY `sum` DESC
+      LIMIT 10
+    ) t4
+      ON t2.`n_name` = t4.`n_name`
+) t0"""
+        assert result == expected
+
+    def test_aggregate_projection_subquery(self):
+        t = self.con.table('alltypes')
+
+        proj = t[t.f > 0][t, (t.a + t.b).name('foo')]
+
+        result = to_sql(proj)
+        expected = """SELECT *, `a` + `b` AS `foo`
+FROM alltypes
+WHERE `f` > 0"""
+        assert result == expected
+
+        def agg(x):
+            return x.aggregate([x.foo.sum().name('foo total')], by=['g'])
+
+        # predicate gets pushed down
+        filtered = proj[proj.g == 'bar']
+
+        result = to_sql(filtered)
+        expected = """SELECT *, `a` + `b` AS `foo`
+FROM alltypes
+WHERE `f` > 0 AND
+      `g` = 'bar'"""
+        assert result == expected
+
+        agged = agg(filtered)
+        result = to_sql(agged)
+        expected = """SELECT `g`, sum(`foo`) AS `foo total`
+FROM (
+  SELECT *, `a` + `b` AS `foo`
+  FROM alltypes
+  WHERE `f` > 0 AND
+        `g` = 'bar'
+) t0
+GROUP BY 1"""
+        assert result == expected
+
+        # Pushdown is not possible (in Impala, Postgres, others)
+        agged2 = agg(proj[proj.foo < 10])
+
+        result = to_sql(agged2)
+        expected = """SELECT t0.`g`, sum(t0.`foo`) AS `foo total`
+FROM (
+  SELECT *, `a` + `b` AS `foo`
+  FROM alltypes
+  WHERE `f` > 0
+) t0
+WHERE t0.`foo` < 10
+GROUP BY 1"""
+        assert result == expected
+
+    def test_subquery_aliased(self):
+        case = self._case_subquery_aliased()
+
+        expected = """SELECT t0.*, t1.`value1`
+FROM (
+  SELECT `foo_id`, sum(`f`) AS `total`
+  FROM star1
+  GROUP BY 1
+) t0
+  INNER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`"""
+        self._compare_sql(case, expected)
+
+    def test_double_nested_subquery_no_aliases(self):
+        # We don't require any table aliasing anywhere
+        t = api.table([
             ('key1', 'string'),
             ('key2', 'string'),
             ('key3', 'string'),
-            ('value1', 'double'),
-        ],
-        'foo_table',
-    )
+            ('value', 'double')
+        ], 'foo_table')
 
-    t2 = ibis.table(
-        [
-            ('key1', 'string'),
-            ('key2', 'string'),
-            ('key3', 'string'),
-            ('value2', 'double'),
-        ],
-        'bar_table',
-    )
-
-    p1 = t1.key1 == t2.key1
-    p2 = t1.key2 == t2.key2
-    p3 = t1.key3 == t2.key3
-
-    joined = t1.inner_join(t2, [p1 & p2 & p3])
-    expected = t1.inner_join(t2, [p1, p2, p3])
-    assert_equal(joined, expected)
-
-
-def test_union(
-    setops_table_foo,
-    setops_table_bar,
-    setops_table_baz,
-    setops_relation_error_message,
-):
-    result = setops_table_foo.union(setops_table_bar)
-    assert isinstance(result.op().table, ops.Union)
-    assert not result.op().table.distinct
-
-    result = setops_table_foo.union(setops_table_bar, distinct=True)
-    assert result.op().table.distinct
-
-    with pytest.raises(RelationError, match=setops_relation_error_message):
-        setops_table_foo.union(setops_table_baz)
-
-
-def test_intersection(
-    setops_table_foo,
-    setops_table_bar,
-    setops_table_baz,
-    setops_relation_error_message,
-):
-    result = setops_table_foo.intersect(setops_table_bar)
-    assert isinstance(result.op().table, ops.Intersection)
-
-    with pytest.raises(RelationError, match=setops_relation_error_message):
-        setops_table_foo.intersect(setops_table_baz)
-
-
-def test_difference(
-    setops_table_foo,
-    setops_table_bar,
-    setops_table_baz,
-    setops_relation_error_message,
-):
-    result = setops_table_foo.difference(setops_table_bar)
-    assert isinstance(result.op().table, ops.Difference)
-
-    with pytest.raises(RelationError, match=setops_relation_error_message):
-        setops_table_foo.difference(setops_table_baz)
-
-
-def test_column_ref_on_projection_rename(con):
-    region = con.table('tpch_region')
-    nation = con.table('tpch_nation')
-    customer = con.table('tpch_customer')
-
-    joined = region.inner_join(
-        nation, [region.r_regionkey == nation.n_regionkey]
-    ).inner_join(customer, [customer.c_nationkey == nation.n_nationkey])
-
-    proj_exprs = [
-        customer,
-        nation.n_name.name('nation'),
-        region.r_name.name('region'),
-    ]
-    joined = joined.select(proj_exprs)
-
-    metrics = [joined.c_acctbal.sum().name('metric')]
-
-    # it works!
-    joined.aggregate(metrics, by=['region'])
-
-
-@pytest.fixture
-def t1():
-    return ibis.table(
-        [('key1', 'string'), ('key2', 'string'), ('value1', 'double')], 'foo'
-    )
-
-
-@pytest.fixture
-def t2():
-    return ibis.table([('key1', 'string'), ('key2', 'string')], 'bar')
-
-
-@pytest.mark.parametrize(
-    ("func", "expected_type"),
-    [
-        param(
-            lambda t1, t2: (t1.key1 == t2.key1).any(),
-            ops.UnresolvedExistsSubquery,
-            id="exists",
-        ),
-        param(
-            lambda t1, t2: -(t1.key1 == t2.key1).any(),
-            ops.UnresolvedNotExistsSubquery,
-            id="not_exists",
-        ),
-        param(
-            lambda t1, t2: -(-(t1.key1 == t2.key1).any()),
-            ops.UnresolvedExistsSubquery,
-            id="not_not_exists",
-        ),
-    ],
+        agg1 = t.aggregate([t.value.sum().name('total')],
+                           by=['key1', 'key2', 'key3'])
+        agg2 = agg1.aggregate([agg1.total.sum().name('total')],
+                              by=['key1', 'key2'])
+        agg3 = agg2.aggregate([agg2.total.sum().name('total')],
+                              by=['key1'])
+
+        result = to_sql(agg3)
+        expected = """SELECT `key1`, sum(`total`) AS `total`
+FROM (
+  SELECT `key1`, `key2`, sum(`total`) AS `total`
+  FROM (
+    SELECT `key1`, `key2`, `key3`, sum(`value`) AS `total`
+    FROM foo_table
+    GROUP BY 1, 2, 3
+  ) t1
+  GROUP BY 1, 2
+) t0
+GROUP BY 1"""
+        assert result == expected
+
+    def test_aggregate_projection_alias_bug(self):
+        # Observed in use
+        t1 = self.con.table('star1')
+        t2 = self.con.table('star2')
+
+        what = (t1.inner_join(t2, [t1.foo_id == t2.foo_id])
+                [[t1, t2.value1]])
+
+        what = what.aggregate([what.value1.sum().name('total')],
+                              by=[what.foo_id])
+
+        # TODO: Not fusing the aggregation with the projection yet
+        result = to_sql(what)
+        expected = """SELECT `foo_id`, sum(`value1`) AS `total`
+FROM (
+  SELECT t1.*, t2.`value1`
+  FROM star1 t1
+    INNER JOIN star2 t2
+      ON t1.`foo_id` = t2.`foo_id`
+) t0
+GROUP BY 1"""
+        assert result == expected
+
+    def test_aggregate_fuse_with_projection(self):
+        # see above test case
+        pass
+
+    def test_subquery_used_for_self_join(self):
+        expr = self._case_subquery_used_for_self_join()
+
+        result = to_sql(expr)
+        expected = """WITH t0 AS (
+  SELECT `g`, `a`, `b`, sum(`f`) AS `total`
+  FROM alltypes
+  GROUP BY 1, 2, 3
 )
-def test_unresolved_existence_predicate(t1, t2, func, expected_type):
-    expr = func(t1, t2)
-    assert isinstance(expr, ir.BooleanColumn)
-
-    op = expr.op()
-    assert isinstance(op, expected_type)
-
-
-@pytest.mark.parametrize(
-    ("func", "expected_type", "expected_negated_type"),
-    [
-        param(
-            lambda t1, t2: t1[(t1.key1 == t2.key1).any()],
-            ops.ExistsSubquery,
-            ops.NotExistsSubquery,
-            id="exists",
-        ),
-        param(
-            lambda t1, t2: t1[-(t1.key1 == t2.key1).any()],
-            ops.NotExistsSubquery,
-            ops.ExistsSubquery,
-            id="not_exists",
-        ),
-        param(
-            lambda t1, t2: t1[-(-(t1.key1 == t2.key1).any())],
-            ops.ExistsSubquery,
-            ops.NotExistsSubquery,
-            id="not_not_exists",
-        ),
-    ],
+SELECT t0.`g`, max(t0.`total` - t1.`total`) AS `metric`
+FROM t0
+  INNER JOIN t0 t1
+    ON t0.`a` = t1.`b`
+GROUP BY 1"""
+        assert result == expected
+
+    def test_subquery_factor_correlated_subquery(self):
+        # #173, #183 and other issues
+
+        expr = self._case_subquery_factor_correlated_subquery()
+
+        result = to_sql(expr)
+        expected = """\
+WITH t0 AS (
+  SELECT t6.*, t1.`r_name` AS `region`, t3.`o_totalprice` AS `amount`,
+         CAST(t3.`o_orderdate` AS timestamp) AS `odate`
+  FROM tpch_region t1
+    INNER JOIN tpch_nation t2
+      ON t1.`r_regionkey` = t2.`n_regionkey`
+    INNER JOIN tpch_customer t6
+      ON t6.`c_nationkey` = t2.`n_nationkey`
+    INNER JOIN tpch_orders t3
+      ON t3.`o_custkey` = t6.`c_custkey`
 )
-def test_resolve_existence_predicate(
-    t1,
-    t2,
-    func,
-    expected_type,
-    expected_negated_type,
-):
-    expr = func(t1, t2)
-    op = expr.op()
-    assert isinstance(op, ops.Selection)
-
-    pred = op.predicates[0].to_expr()
-    assert isinstance(pred.op(), expected_type)
-    assert isinstance((-pred).op(), expected_negated_type)
-
-
-def test_aggregate_metrics(table):
-    functions = [
-        lambda x: x.e.sum().name('esum'),
-        lambda x: x.f.sum().name('fsum'),
-    ]
-    exprs = [table.e.sum().name('esum'), table.f.sum().name('fsum')]
-
-    result = table.aggregate(functions[0])
-    expected = table.aggregate(exprs[0])
-    assert_equal(result, expected)
-
-    result = table.aggregate(functions)
-    expected = table.aggregate(exprs)
-    assert_equal(result, expected)
-
-
-def test_group_by_keys(table):
-    m = table.mutate(foo=table.f * 2, bar=table.e / 2)
-
-    expr = m.group_by(lambda x: x.foo).size()
-    expected = m.group_by('foo').size()
-    assert_equal(expr, expected)
-
-    expr = m.group_by([lambda x: x.foo, lambda x: x.bar]).size()
-    expected = m.group_by(['foo', 'bar']).size()
-    assert_equal(expr, expected)
-
-
-def test_having(table):
-    m = table.mutate(foo=table.f * 2, bar=table.e / 2)
-
-    expr = m.group_by('foo').having(lambda x: x.foo.sum() > 10).size()
-    expected = m.group_by('foo').having(m.foo.sum() > 10).size()
-
-    assert_equal(expr, expected)
-
-
-def test_filter(table):
-    m = table.mutate(foo=table.f * 2, bar=table.e / 2)
-
-    result = m.filter(lambda x: x.foo > 10)
-    result2 = m[lambda x: x.foo > 10]
-    expected = m[m.foo > 10]
-
-    assert_equal(result, expected)
-    assert_equal(result2, expected)
-
-    result = m.filter([lambda x: x.foo > 10, lambda x: x.bar < 0])
-    expected = m.filter([m.foo > 10, m.bar < 0])
-    assert_equal(result, expected)
-
-
-def test_order_by2(table):
-    m = table.mutate(foo=table.e + table.f)
-
-    result = m.order_by(lambda x: -x.foo)
-    expected = m.order_by(-m.foo)
-    assert_equal(result, expected)
-
-    result = m.order_by(lambda x: ibis.desc(x.foo))
-    expected = m.order_by(ibis.desc('foo'))
-    assert_equal(result, expected)
-
-    result = m.order_by(ibis.desc(lambda x: x.foo))
-    expected = m.order_by(ibis.desc('foo'))
-    assert_equal(result, expected)
-
-    result = m.order_by(ibis.asc(lambda x: x.foo))
-    expected = m.order_by('foo')
-    assert_equal(result, expected)
-
-
-def test_projection2(table):
-    m = table.mutate(foo=table.f * 2)
-
-    def f(x):
-        return (x.foo * 2).name('bar')
-
-    result = m.select([f, 'f'])
-    result2 = m[f, 'f']
-    expected = m.select([f(m), 'f'])
-    assert_equal(result, expected)
-    assert_equal(result2, expected)
-
-
-def test_mutate2(table):
-    m = table.mutate(foo=table.f * 2)
-
-    def g(x):
-        return x.foo * 2
-
-    def h(x):
-        return x.bar * 2
-
-    result = m.mutate(bar=g).mutate(baz=h)
-
-    m2 = m.mutate(bar=g(m))
-    expected = m2.mutate(baz=h(m2))
-
-    assert_equal(result, expected)
-
-
-def test_groupby_mutate(table):
-    t = table
-
-    g = t.group_by('g').order_by('f')
-    expr = g.mutate(foo=lambda x: x.f.lag(), bar=lambda x: x.f.rank())
-    expected = g.mutate(foo=t.f.lag(), bar=t.f.rank())
-
-    assert_equal(expr, expected)
-
-
-def test_groupby_projection(table):
-    t = table
-
-    g = t.group_by('g').order_by('f')
-    expr = g.select([lambda x: x.f.lag().name('foo'), lambda x: x.f.rank().name('bar')])
-    expected = g.select([t.f.lag().name('foo'), t.f.rank().name('bar')])
-
-    assert_equal(expr, expected)
-
-
-def test_pickle_table_expr():
-    schema = [('time', 'timestamp'), ('key', 'string'), ('value', 'double')]
-    t0 = ibis.table(schema, name='t0')
-    raw = pickle.dumps(t0, protocol=2)
-    t1 = pickle.loads(raw)
-    assert t1.equals(t0)
-
-
-def test_pickle_table_node(table):
-    n0 = table.op()
-    assert_pickle_roundtrip(n0)
-
-
-def test_pickle_projection_node(table):
-    m = table.mutate(foo=table.f * 2)
-
-    def f(x):
-        return (x.foo * 2).name('bar')
-
-    node = m.select([f, 'f']).op()
-
-    assert_pickle_roundtrip(node)
-
-
-def test_pickle_group_by(table):
-    m = table.mutate(foo=table.f * 2, bar=table.e / 2)
-    expr = m.group_by(lambda x: x.foo).size()
-    node = expr.op()
-
-    assert_pickle_roundtrip(node)
-
-
-def test_pickle_asof_join():
-    left = ibis.table([('time', 'int32'), ('value', 'double')])
-    right = ibis.table([('time', 'int32'), ('value2', 'double')])
-    joined = api.asof_join(left, right, 'time')
-    node = joined.op()
-
-    assert_pickle_roundtrip(node)
-
-
-def test_group_by_key_function():
-    t = ibis.table([('a', 'timestamp'), ('b', 'string'), ('c', 'double')])
-    expr = t.group_by(new_key=lambda t: t.b.length()).aggregate(foo=t.c.mean())
-    assert expr.columns == ['new_key', 'foo']
-
-
-def test_group_by_no_keys():
-    t = ibis.table([('a', 'timestamp'), ('b', 'string'), ('c', 'double')])
-
-    with pytest.raises(com.IbisInputError):
-        t.group_by(s.startswith("x")).aggregate(foo=t.c.mean())
-
-
-def test_unbound_table_name():
-    t = ibis.table([('a', 'timestamp')])
-    name = t.op().name
-    match = re.match(r'^unbound_table_\d+$', name)
-    assert match is not None
-
-
-class MyTable:
-    a: int
-    b: str
-    c: List[float]
-
-
-def test_unbound_table_using_class_definition():
-    expected_schema = ibis.schema({'a': 'int64', 'b': 'string', 'c': 'array<double>'})
-
-    t1 = ibis.table(MyTable)
-    t2 = ibis.table(MyTable, name="MyNamedTable")
-
-    cases = {t1: "MyTable", t2: "MyNamedTable"}
-    for t, name in cases.items():
-        assert isinstance(t, ir.TableExpr)
-        assert isinstance(t.op(), ops.UnboundTable)
-        assert t.schema() == expected_schema
-        assert t.get_name() == name
-
-
-def test_mutate_chain():
-    one = ibis.table([('a', 'string'), ('b', 'string')], name='t')
-    two = one.mutate(b=lambda t: t.b.fillna('Short Term'))
-    three = two.mutate(a=lambda t: t.a.fillna('Short Term'))
-    a, b = three.op().selections
-
-    # we can't fuse these correctly yet
-    assert isinstance(a, ops.Alias)
-    assert isinstance(a.arg, ops.IfNull)
-    assert isinstance(b, ops.TableColumn)
-
-    expr = b.table.selections[1]
-    assert isinstance(expr, ops.Alias)
-    assert isinstance(expr.arg, ops.IfNull)
-
-
-# TODO(kszucs): move this test case to ibis/tests/sql since it requires the
-# sql backend to be executed
-def test_multiple_dbcon():
-    """Expr from multiple connections to same DB should be compatible."""
-    con1 = MockBackend()
-    con2 = MockBackend()
-
-    con1.table('alltypes').union(con2.table('alltypes')).execute()
-
-
-def test_multiple_db_different_backends():
-    con1 = MockBackend()
-    con2 = MockAlchemyBackend()
-
-    backend1_table = con1.table('alltypes')
-    backend2_table = con2.table('alltypes')
-
-    expr = backend1_table.union(backend2_table)
-    with pytest.raises(com.IbisError, match="Multiple backends"):
-        expr.compile()
-
-
-def test_merge_as_of_allows_overlapping_columns():
-    # GH3295
-    table = ibis.table(
-        [
-            ("field", "string"),
-            ("value", "float64"),
-            ("timestamp_received", "timestamp"),
-        ],
-        name="t",
-    )
-
-    signal_one = table[
-        table['field'].contains('signal_one') & table['field'].contains('current')
-    ]
-    signal_one = signal_one[
-        'value', 'timestamp_received', 'field'
-    ]  # select columns we care about
-    signal_one = signal_one.relabel({'value': 'current', 'field': 'signal_one'})
-
-    signal_two = table[
-        table['field'].contains('signal_two') & table['field'].contains('voltage')
-    ]
-    signal_two = signal_two[
-        'value', 'timestamp_received', 'field'
-    ]  # select columns we care about
-    signal_two = signal_two.relabel({'value': 'voltage', 'field': 'signal_two'})
-
-    merged = ibis.api.asof_join(signal_one, signal_two, 'timestamp_received')
-    assert merged.columns == [
-        'current',
-        'timestamp_received',
-        'signal_one',
-        'voltage',
-        'timestamp_received_right',
-        'signal_two',
-    ]
-
-
-def test_select_from_unambiguous_join_with_strings():
-    # GH1387
-    t = ibis.table([('a', 'int64'), ('b', 'string')])
-    s = ibis.table([('b', 'int64'), ('c', 'string')])
-    joined = t.left_join(s, [t.b == s.c])
-    expr = joined[t, 'c']
-    assert expr.columns == ["a", "b", "c"]
-
-
-def test_filter_applied_to_join():
-    # GH2437
-    countries = ibis.table([("iso_alpha3", "string")])
-    gdp = ibis.table([("country_code", "string"), ("year", "int64")])
-
-    expr = countries.inner_join(
-        gdp,
-        predicates=[countries["iso_alpha3"] == gdp["country_code"]],
-    ).filter(gdp["year"] == 2017)
-    assert expr.columns == ["iso_alpha3", "country_code", "year"]
-
-
-@pytest.mark.parametrize("how", ["inner", "left", "outer", "right"])
-def test_join_lname_rname(how):
-    left = ibis.table([("id", "int64"), ("first_name", "string")])
-    right = ibis.table([("id", "int64"), ("last_name", "string")])
-    method = getattr(left, f"{how}_join")
-
-    expr = method(right)
-    assert expr.columns == ["id", "first_name", "id_right", "last_name"]
-
-    expr = method(right, rname="right_{name}")
-    assert expr.columns == ["id", "first_name", "right_id", "last_name"]
-
-    expr = method(right, lname="left_{name}", rname="")
-    assert expr.columns == ["left_id", "first_name", "id", "last_name"]
-
-    expr = method(right, rname="right_{name}", lname="left_{name}")
-    assert expr.columns == ["left_id", "first_name", "right_id", "last_name"]
-
-
-def test_join_lname_rname_still_collide():
-    t1 = ibis.table({"id": "int64", "col1": "int64", "col2": "int64"})
-    t2 = ibis.table({"id": "int64", "col1": "int64", "col2": "int64"})
-    t3 = ibis.table({"id": "int64", "col1": "int64", "col2": "int64"})
-
-    with pytest.raises(com.IntegrityError) as rec:
-        t1.left_join(t2, "id").left_join(t3, "id")
-
-    assert "`['col1_right', 'col2_right', 'id_right']`" in str(rec.value)
-    assert "`lname='', rname='{name}_right'`" in str(rec.value)
-
-
-def test_drop():
-    t = ibis.table(dict.fromkeys("abcd", "int"))
-
-    assert t.drop() is t
-
-    res = t.drop("a")
-    assert res.equals(t.select("b", "c", "d"))
-
-    res = t.drop("a", "b")
-    assert res.equals(t.select("c", "d"))
-
-    assert res.equals(t.select("c", "d"))
-
-    assert res.equals(t.drop(s.matches("a|b")))
-
-    with pytest.raises(KeyError):
-        t.drop("e")
-
-
-def test_python_table_ambiguous():
-    with pytest.raises(NotImplementedError):
-        ibis.memtable(
-            [(1,)],
-            schema=ibis.schema(dict(a="int8")),
-            columns=["a"],
-        )
-
-
-def test_memtable_filter():
-    # Mostly just a smoketest, this used to error on construction
-    t = ibis.memtable([(1, 2), (3, 4), (5, 6)], columns=["x", "y"])
-    expr = t.filter(t.x > 1)
-    assert expr.columns == ["x", "y"]
-
-
-def test_default_backend_with_unbound_table():
-    t = ibis.table(dict(a="int"), name="t")
-    expr = t.a.sum()
-
-    with pytest.raises(
-        com.IbisError,
-        match="Expression contains unbound tables",
-    ):
-        assert expr.execute()
-
-
-def test_numpy_ufuncs_dont_cast_tables():
-    t = ibis.table(dict.fromkeys("abcd", "int"))
-    for arg in [np.int64(1), np.array([1, 2, 3])]:
-        for left, right in [(t, arg), (arg, t)]:
-            with pytest.raises(TypeError):
-                left + right
-
-
-def test_array_string_compare():
-    t = ibis.table(schema=dict(by="string", words="array<string>"), name="t")
-    expr = t[t.by == "foo"].mutate(words=_.words.unnest()).filter(_.words == "the")
-    assert expr is not None
-
-
-@pytest.mark.parametrize("value", [True, False])
-@pytest.mark.parametrize(
-    "api",
-    [
-        param(lambda t, value: t[value], id="getitem"),
-        param(lambda t, value: t.filter(value), id="filter"),
-    ],
+SELECT t0.*
+FROM t0
+WHERE t0.`amount` > (
+  SELECT avg(t4.`amount`) AS `mean`
+  FROM t0 t4
+  WHERE t4.`region` = t0.`region`
 )
-def test_filter_with_literal(value, api):
-    t = ibis.table(dict(a="string"))
-    filt = api(t, ibis.literal(value))
-    assert filt is not None
-
-    # ints are invalid predicates
-    int_val = ibis.literal(int(value))
-    with pytest.raises((NotImplementedError, com.IbisTypeError)):
-        api(t, int_val)
-
-
-def test_cast():
-    t = ibis.table(dict(a="int", b="string", c="float"), name="t")
-
-    assert t.cast({"a": "string"}).equals(t.mutate(a=t.a.cast("string")))
-
-    with pytest.raises(
-        com.IbisError, match="fields that are not in the table: .+'d'.+"
-    ):
-        t.cast({"d": "array<int>"}).equals(t.select())
-
-    assert t.cast(ibis.schema({"a": "string", "b": "int"})).equals(
-        t.mutate(a=t.a.cast("string"), b=t.b.cast("int"))
-    )
-    assert t.cast([("a", "string"), ("b", "float")]).equals(
-        t.mutate(a=t.a.cast("string"), b=t.b.cast("float"))
-    )
-
-
-def test_pivot_longer():
-    diamonds = ibis.table(
-        {
-            'carat': 'float64',
-            'cut': 'string',
-            'color': 'string',
-            'clarity': 'string',
-            'depth': 'float64',
-            'table': 'float64',
-            'price': 'int64',
-            'x': 'float64',
-            'y': 'float64',
-            'z': 'float64',
-        },
-        name="diamonds",
-    )
-    res = diamonds.pivot_longer(s.c("x", "y", "z"), names_to="pos", values_to="xyz")
-    assert res.schema().names == (
-        "carat",
-        "cut",
-        "color",
-        "clarity",
-        "depth",
-        "table",
-        "price",
-        "pos",
-        "xyz",
-    )
-
-
-def test_pivot_longer_strip_prefix():
-    t = ibis.table(
-        dict(artist="string", track="string", wk1="int", wk2="int", wk3="int")
-    )
-    expr = t.pivot_longer(
-        s.startswith("wk"),
-        names_to="week",
-        names_pattern=r"wk(.+)",
-        names_transform=int,
-        values_to="rank",
-        values_transform=_.cast("int"),
-    )
-    schema = ibis.schema(dict(artist="string", track="string", week="int8", rank="int"))
-    assert expr.schema() == schema
-
-
-def test_pivot_longer_pluck_regex():
-    t = ibis.table(
-        dict(artist="string", track="string", x_wk1="int", x_wk2="int", x_wk3="int")
-    )
-    expr = t.pivot_longer(
-        s.matches("^.+wk.$"),
-        names_to=["other_var", "week"],
-        names_pattern=r"(.)_wk(\d)",
-        names_transform=dict(other_var=str.upper, week=int),
-        values_to="rank",
-        values_transform=_.cast("int"),
-    )
-    schema = ibis.schema(
-        dict(
-            artist="string", track="string", other_var="string", week="int8", rank="int"
-        )
-    )
-    assert expr.schema() == schema
+LIMIT 10"""
+        assert result == expected
 
+    def test_self_join_subquery_distinct_equal(self):
+        expr = self._case_self_join_subquery_distinct_equal()
 
-def test_pivot_longer_no_match():
-    t = ibis.table(
-        dict(artist="string", track="string", x_wk1="int", x_wk2="int", x_wk3="int")
-    )
-    with pytest.raises(
-        com.IbisInputError, match="Selector returned no columns to pivot on"
-    ):
-        t.pivot_longer(
-            s.matches("foo"),
-            names_to=["other_var", "week"],
-            names_pattern=r"(.)_wk(\d)",
-            names_transform=dict(other_var=str.upper, week=int),
-            values_to="rank",
-            values_transform=_.cast("int"),
-        )
-
-
-def test_pivot_wider():
-    fish = ibis.table({"fish": "int", "station": "string", "seen": "int"}, name="fish")
-    res = fish.pivot_wider(
-        names=["Release", "Lisbon"], names_from="station", values_from="seen"
-    )
-    assert res.schema().names == ("fish", "Release", "Lisbon")
-    with pytest.raises(
-        com.IbisInputError, match="No matching names columns in `names_from`"
-    ):
-        fish.pivot_wider(names=["Release", "Lisbon"], values_from="seen")
-
-
-def test_invalid_deferred():
-    t = ibis.table(dict(value="int", lagged_value="int"), name="t")
-
-    with pytest.raises(com.IbisTypeError, match="Deferred input is not allowed"):
-        ibis.greatest(t.value, ibis._.lagged_value)
-
-
-@pytest.mark.parametrize("keep", ["last", None])
-def test_invalid_distinct(keep):
-    t = ibis.table(dict(a="int"), name="t")
-    with pytest.raises(com.IbisError, match="Only keep='first'"):
-        t.distinct(keep=keep)
-
-
-def test_invalid_keep_distinct():
-    t = ibis.table(dict(a="int", b="string"), name="t")
-    with pytest.raises(com.IbisError, match="Invalid value for `keep`:"):
-        t.distinct(on="a", keep="invalid")
-
-
-def test_invalid_distinct_empty_key():
-    t = ibis.table(dict(a="int", b="string"), name="t")
-    with pytest.raises(com.IbisInputError):
-        t.distinct(on="c", keep="first")
+        result = to_sql(expr)
+        expected = """\
+WITH t0 AS (
+  SELECT t2.*, t3.*
+  FROM tpch_region t2
+    INNER JOIN tpch_nation t3
+      ON t2.`r_regionkey` = t3.`n_regionkey`
+)
+SELECT t0.`r_name`, t1.`n_name`
+FROM t0
+  INNER JOIN t0 t1
+    ON t0.`r_regionkey` = t1.`r_regionkey`"""
+
+        assert result == expected
+
+    def test_limit_with_self_join(self):
+        t = self.con.table('functional_alltypes')
+        t2 = t.view()
+
+        expr = t.join(t2, t.tinyint_col < t2.timestamp_col.minute()).count()
+
+        # it works
+        result = to_sql(expr)
+        expected = """\
+SELECT count(*) AS `count`
+FROM functional_alltypes t0
+  INNER JOIN functional_alltypes t1
+    ON t0.`tinyint_col` < extract(t1.`timestamp_col`, 'minute')"""
+        assert result == expected
+
+    def test_cte_factor_distinct_but_equal(self):
+        expr = self._case_cte_factor_distinct_but_equal()
+
+        result = to_sql(expr)
+        expected = """\
+WITH t0 AS (
+  SELECT `g`, sum(`f`) AS `metric`
+  FROM alltypes
+  GROUP BY 1
+)
+SELECT t0.*
+FROM t0
+  INNER JOIN t0 t1
+    ON t0.`g` = t1.`g`"""
+
+        assert result == expected
+
+    def test_tpch_self_join_failure(self):
+        yoy = self._case_tpch_self_join_failure()
+        to_sql(yoy)
+
+    def test_extract_subquery_nested_lower(self):
+        # We may have a join between two tables requiring subqueries, and
+        # buried inside these there may be a common subquery. Let's test that
+        # we find it and pull it out to the top level to avoid repeating
+        # ourselves.
+        pass
+
+    def test_subquery_in_filter_predicate(self):
+        expr, expr2 = self._case_subquery_in_filter_predicate()
+
+        result = to_sql(expr)
+        expected = """SELECT *
+FROM star1
+WHERE `f` > (
+  SELECT avg(`f`) AS `mean`
+  FROM star1
+)"""
+        assert result == expected
+
+        result = to_sql(expr2)
+        expected = """SELECT *
+FROM star1
+WHERE `f` > (
+  SELECT avg(`f`) AS `mean`
+  FROM star1
+  WHERE `foo_id` = 'foo'
+)"""
+        assert result == expected
+
+    def test_filter_subquery_derived_reduction(self):
+        expr3, expr4 = self._case_filter_subquery_derived_reduction()
+
+        result = to_sql(expr3)
+        expected = """SELECT *
+FROM star1
+WHERE `f` > (
+  SELECT ln(avg(`f`)) AS `tmp`
+  FROM star1
+  WHERE `foo_id` = 'foo'
+)"""
+        assert result == expected
+
+        result = to_sql(expr4)
+        expected = """SELECT *
+FROM star1
+WHERE `f` > (
+  SELECT ln(avg(`f`)) + 1 AS `tmp`
+  FROM star1
+  WHERE `foo_id` = 'foo'
+)"""
+        assert result == expected
+
+    def test_topk_operation(self):
+        filtered, filtered2 = self._case_topk_operation()
+
+        query = to_sql(filtered)
+        expected = """SELECT t0.*
+FROM tbl t0
+  LEFT SEMI JOIN (
+    SELECT `city`, avg(`v2`) AS `mean`
+    FROM tbl
+    GROUP BY 1
+    ORDER BY `mean` DESC
+    LIMIT 10
+  ) t1
+    ON t0.`city` = t1.`city`"""
+        assert query == expected
+
+        query = to_sql(filtered2)
+        expected = """SELECT t0.*
+FROM tbl t0
+  LEFT SEMI JOIN (
+    SELECT `city`, count(`city`) AS `count`
+    FROM tbl
+    GROUP BY 1
+    ORDER BY `count` DESC
+    LIMIT 10
+  ) t1
+    ON t0.`city` = t1.`city`"""
+        assert query == expected
+
+    def test_topk_predicate_pushdown_bug(self):
+        # Observed on TPCH data
+        cplusgeo = (
+            customer.inner_join(nation, [customer.c_nationkey ==
+                                         nation.n_nationkey])
+                    .inner_join(region, [nation.n_regionkey ==
+                                         region.r_regionkey])
+            [customer, nation.n_name, region.r_name])
+
+        pred = cplusgeo.n_name.topk(10, by=cplusgeo.c_acctbal.sum())
+        expr = cplusgeo.filter([pred])
+
+        result = to_sql(expr)
+        expected = """\
+SELECT t0.*, t1.`n_name`, t2.`r_name`
+FROM customer t0
+  INNER JOIN nation t1
+    ON t0.`c_nationkey` = t1.`n_nationkey`
+  INNER JOIN region t2
+    ON t1.`n_regionkey` = t2.`r_regionkey`
+  LEFT SEMI JOIN (
+    SELECT t1.`n_name`, sum(t0.`c_acctbal`) AS `sum`
+    FROM customer t0
+      INNER JOIN nation t1
+        ON t0.`c_nationkey` = t1.`n_nationkey`
+      INNER JOIN region t2
+        ON t1.`n_regionkey` = t2.`r_regionkey`
+    GROUP BY 1
+    ORDER BY `sum` DESC
+    LIMIT 10
+  ) t3
+    ON t1.`n_name` = t3.`n_name`"""
+        assert result == expected
+
+    def test_topk_analysis_bug(self):
+        # GH #398
+        airlines = ibis.table([('dest', 'string'),
+                               ('origin', 'string'),
+                               ('arrdelay', 'int32')], 'airlines')
+
+        dests = ['ORD', 'JFK', 'SFO']
+        t = airlines[airlines.dest.isin(dests)]
+        delay_filter = t.dest.topk(10, by=t.arrdelay.mean())
+        expr = t[delay_filter].group_by('origin').size()
+
+        result = to_sql(expr)
+        expected = """\
+SELECT t0.`origin`, count(*) AS `count`
+FROM airlines t0
+  LEFT SEMI JOIN (
+    SELECT `dest`, avg(`arrdelay`) AS `mean`
+    FROM airlines
+    WHERE `dest` IN ('ORD', 'JFK', 'SFO')
+    GROUP BY 1
+    ORDER BY `mean` DESC
+    LIMIT 10
+  ) t1
+    ON t0.`dest` = t1.`dest`
+WHERE t0.`dest` IN ('ORD', 'JFK', 'SFO')
+GROUP BY 1"""
+
+        assert result == expected
+
+    def test_topk_to_aggregate(self):
+        t = ibis.table([('dest', 'string'),
+                        ('origin', 'string'),
+                        ('arrdelay', 'int32')], 'airlines')
+
+        top = t.dest.topk(10, by=t.arrdelay.mean())
+
+        result = to_sql(top)
+        expected = to_sql(top.to_aggregation())
+        assert result == expected
+
+    def test_bottomk(self):
+        pass
+
+    def test_topk_antijoin(self):
+        # Get the "other" category somehow
+        pass
+
+    def test_case_in_projection(self):
+        t = self.con.table('alltypes')
+
+        expr = (t.g.case()
+                .when('foo', 'bar')
+                .when('baz', 'qux')
+                .else_('default').end())
+
+        expr2 = (api.case()
+                 .when(t.g == 'foo', 'bar')
+                 .when(t.g == 'baz', t.g)
+                 .end())
+
+        proj = t[expr.name('col1'), expr2.name('col2'), t]
+
+        result = to_sql(proj)
+        expected = """SELECT
+  CASE `g`
+    WHEN 'foo' THEN 'bar'
+    WHEN 'baz' THEN 'qux'
+    ELSE 'default'
+  END AS `col1`,
+  CASE
+    WHEN `g` = 'foo' THEN 'bar'
+    WHEN `g` = 'baz' THEN `g`
+    ELSE NULL
+  END AS `col2`, *
+FROM alltypes"""
+        assert result == expected
+
+    def test_identifier_quoting(self):
+        data = api.table([
+            ('date', 'int32'),
+            ('explain', 'string')
+        ], 'table')
+
+        expr = data[data.date.name('else'), data.explain.name('join')]
+
+        result = to_sql(expr)
+        expected = """SELECT `date` AS `else`, `explain` AS `join`
+FROM `table`"""
+        assert result == expected
+
+    def test_scalar_subquery_different_table(self):
+        t1, t2 = self.foo, self.bar
+        expr = t1[t1.y > t2.x.max()]
+
+        result = to_sql(expr)
+        expected = """SELECT *
+FROM foo
+WHERE `y` > (
+  SELECT max(`x`) AS `max`
+  FROM bar
+)"""
+        assert result == expected
+
+    def test_where_uncorrelated_subquery(self):
+        expr = self._case_where_uncorrelated_subquery()
+
+        result = to_sql(expr)
+        expected = """SELECT *
+FROM foo
+WHERE `job` IN (
+  SELECT `job`
+  FROM bar
+)"""
+        assert result == expected
+
+    def test_where_correlated_subquery(self):
+        expr = self._case_where_correlated_subquery()
+        result = to_sql(expr)
+        expected = """SELECT t0.*
+FROM foo t0
+WHERE t0.`y` > (
+  SELECT avg(t1.`y`) AS `mean`
+  FROM foo t1
+  WHERE t0.`dept_id` = t1.`dept_id`
+)"""
+        assert result == expected
+
+    def test_where_array_correlated(self):
+        # Test membership in some record-dependent values, if this is supported
+        pass
+
+    def test_exists(self):
+        e1, e2 = self._case_exists()
+
+        result = to_sql(e1)
+        expected = """SELECT t0.*
+FROM foo t0
+WHERE EXISTS (
+  SELECT 1
+  FROM bar t1
+  WHERE t0.`key1` = t1.`key1`
+)"""
+        assert result == expected
+
+        result = to_sql(e2)
+        expected = """SELECT t0.*
+FROM foo t0
+WHERE EXISTS (
+  SELECT 1
+  FROM bar t1
+  WHERE t0.`key1` = t1.`key1` AND
+        t1.`key2` = 'foo'
+)"""
+        assert result == expected
+
+    def test_exists_subquery_repr(self):
+        # GH #660
+        t1, t2 = self.t1, self.t2
+
+        cond = t1.key1 == t2.key1
+        expr = t1[cond.any()]
+        stmt = build_ast(expr).queries[0]
+
+        repr(stmt.where[0])
+
+    def test_not_exists(self):
+        expr = self._case_not_exists()
+        result = to_sql(expr)
+        expected = """SELECT t0.*
+FROM foo t0
+WHERE NOT EXISTS (
+  SELECT 1
+  FROM bar t1
+  WHERE t0.`key1` = t1.`key1`
+)"""
+        assert result == expected
+
+    def test_filter_inside_exists(self):
+        events = ibis.table([('session_id', 'int64'),
+                             ('user_id', 'int64'),
+                             ('event_type', 'int32'),
+                             ('ts', 'timestamp')], 'events')
+
+        purchases = ibis.table([('item_id', 'int64'),
+                                ('user_id', 'int64'),
+                                ('price', 'double'),
+                                ('ts', 'timestamp')], 'purchases')
+        filt = purchases.ts > '2015-08-15'
+        cond = (events.user_id == purchases[filt].user_id).any()
+        expr = events[cond]
+
+        result = to_sql(expr)
+        expected = """\
+SELECT t0.*
+FROM events t0
+WHERE EXISTS (
+  SELECT 1
+  FROM purchases t1
+  WHERE t1.`ts` > '2015-08-15' AND
+        t0.`user_id` = t1.`user_id`
+)"""
+
+        assert result == expected
+
+    def test_self_reference_in_exists(self):
+        semi, anti = self._case_self_reference_in_exists()
+
+        result = to_sql(semi)
+        expected = """\
+SELECT t0.*
+FROM functional_alltypes t0
+WHERE EXISTS (
+  SELECT 1
+  FROM functional_alltypes t1
+  WHERE t0.`string_col` = t1.`string_col`
+)"""
+        assert result == expected
+
+        result = to_sql(anti)
+        expected = """\
+SELECT t0.*
+FROM functional_alltypes t0
+WHERE NOT EXISTS (
+  SELECT 1
+  FROM functional_alltypes t1
+  WHERE t0.`string_col` = t1.`string_col`
+)"""
+        assert result == expected
+
+    def test_self_reference_limit_exists(self):
+        case = self._case_self_reference_limit_exists()
+
+        expected = """\
+WITH t0 AS (
+  SELECT *
+  FROM functional_alltypes
+  LIMIT 100
+)
+SELECT *
+FROM t0
+WHERE NOT EXISTS (
+  SELECT 1
+  FROM t0 t1
+  WHERE t0.`string_col` = t1.`string_col`
+)"""
+        self._compare_sql(case, expected)
+
+    def test_limit_cte_extract(self):
+        case = self._case_limit_cte_extract()
+
+        expected = """\
+WITH t0 AS (
+  SELECT *
+  FROM functional_alltypes
+  LIMIT 100
+)
+SELECT t0.*
+FROM t0
+  CROSS JOIN t0 t1"""
+
+        self._compare_sql(case, expected)
+
+    def test_sort_by(self):
+        cases = self._case_sort_by()
+
+        expected = [
+            """SELECT *
+FROM star1
+ORDER BY `f`""",
+            """SELECT *
+FROM star1
+ORDER BY `f` DESC""",
+            """SELECT *
+FROM star1
+ORDER BY `c`, `f` DESC"""
+        ]
+
+        for case, ex in zip(cases, expected):
+            result = to_sql(case)
+            assert result == ex
+
+    def test_limit(self):
+        cases = self._case_limit()
+
+        expected = [
+            """SELECT *
+FROM star1
+LIMIT 10""",
+            """SELECT *
+FROM star1
+LIMIT 10 OFFSET 5""",
+            """SELECT *
+FROM star1
+WHERE `f` > 0
+LIMIT 10""",
+            """SELECT *
+FROM (
+  SELECT *
+  FROM star1
+  LIMIT 10
+) t0
+WHERE `f` > 0"""
+        ]
+
+        for case, ex in zip(cases, expected):
+            result = to_sql(case)
+            assert result == ex
+
+    def test_join_with_limited_table(self):
+        joined = self._case_join_with_limited_table()
+
+        result = to_sql(joined)
+        expected = """SELECT t0.*
+FROM (
+  SELECT *
+  FROM star1
+  LIMIT 100
+) t0
+  INNER JOIN star2 t1
+    ON t0.`foo_id` = t1.`foo_id`"""
+
+        assert result == expected
+
+    def test_sort_by_on_limit_yield_subquery(self):
+        # x.limit(...).sort_by(...)
+        #   is semantically different from
+        # x.sort_by(...).limit(...)
+        #   and will often yield different results
+        t = self.con.table('functional_alltypes')
+        expr = (t.group_by('string_col')
+                .aggregate([t.count().name('nrows')])
+                .limit(5)
+                .sort_by('string_col'))
+
+        result = to_sql(expr)
+        expected = """SELECT *
+FROM (
+  SELECT `string_col`, count(*) AS `nrows`
+  FROM functional_alltypes
+  GROUP BY 1
+  LIMIT 5
+) t0
+ORDER BY `string_col`"""
+        assert result == expected
+
+    def test_multiple_limits(self):
+        t = self.con.table('functional_alltypes')
+
+        expr = t.limit(20).limit(10)
+        stmt = build_ast(expr).queries[0]
+
+        assert stmt.limit['n'] == 10
+
+    def test_top_convenience(self):
+        # x.top(10, by=field)
+        # x.top(10, by=[field1, field2])
+        pass
+
+    def test_self_aggregate_in_predicate(self):
+        # Per ibis #43
+        pass
+
+    def test_self_join_filter_analysis_bug(self):
+        expr, _ = self._case_filter_self_join_analysis_bug()
+
+        expected = """\
+WITH t0 AS (
+  SELECT `region`, `kind`, sum(`amount`) AS `total`
+  FROM purchases
+  GROUP BY 1, 2
+)
+SELECT t1.`region`, t1.`total` - t2.`total` AS `diff`
+FROM (
+  SELECT *
+  FROM t0
+  WHERE `kind` = 'foo'
+) t1
+  INNER JOIN (
+    SELECT *
+    FROM t0
+    WHERE `kind` = 'bar'
+  ) t2
+    ON t1.`region` = t2.`region`"""
+        self._compare_sql(expr, expected)
+
+
+class TestUnions(unittest.TestCase, ExprTestCases):
+
+    def setUp(self):
+        self.con = MockConnection()
+
+    def test_union(self):
+        union1 = self._case_union()
+
+        result = to_sql(union1)
+        expected = """\
+SELECT `string_col` AS `key`, CAST(`float_col` AS double) AS `value`
+FROM functional_alltypes
+WHERE `int_col` > 0
+UNION ALL
+SELECT `string_col` AS `key`, `double_col` AS `value`
+FROM functional_alltypes
+WHERE `int_col` <= 0"""
+        assert result == expected
+
+    def test_union_distinct(self):
+        union = self._case_union(distinct=True)
+        result = to_sql(union)
+        expected = """\
+SELECT `string_col` AS `key`, CAST(`float_col` AS double) AS `value`
+FROM functional_alltypes
+WHERE `int_col` > 0
+UNION
+SELECT `string_col` AS `key`, `double_col` AS `value`
+FROM functional_alltypes
+WHERE `int_col` <= 0"""
+        assert result == expected
+
+    def test_union_project_column(self):
+        # select a column, get a subquery
+        union1 = self._case_union()
+        expr = union1[[union1.key]]
+        result = to_sql(expr)
+        expected = """SELECT `key`
+FROM (
+  SELECT `string_col` AS `key`, CAST(`float_col` AS double) AS `value`
+  FROM functional_alltypes
+  WHERE `int_col` > 0
+  UNION ALL
+  SELECT `string_col` AS `key`, `double_col` AS `value`
+  FROM functional_alltypes
+  WHERE `int_col` <= 0
+) t0"""
+        assert result == expected
+
+
+class TestDistinct(unittest.TestCase):
+
+    def setUp(self):
+        self.con = MockConnection()
+
+    def test_table_distinct(self):
+        t = self.con.table('functional_alltypes')
+
+        expr = t[t.string_col, t.int_col].distinct()
+
+        result = to_sql(expr)
+        expected = """SELECT DISTINCT `string_col`, `int_col`
+FROM functional_alltypes"""
+        assert result == expected
+
+    def test_array_distinct(self):
+        t = self.con.table('functional_alltypes')
+        expr = t.string_col.distinct()
+
+        result = to_sql(expr)
+        expected = """SELECT DISTINCT `string_col`
+FROM functional_alltypes"""
+        assert result == expected
+
+    def test_count_distinct(self):
+        t = self.con.table('functional_alltypes')
+
+        metric = t.int_col.nunique().name('nunique')
+        expr = t[t.bigint_col > 0].group_by('string_col').aggregate([metric])
+
+        result = to_sql(expr)
+        expected = """\
+SELECT `string_col`, COUNT(DISTINCT `int_col`) AS `nunique`
+FROM functional_alltypes
+WHERE `bigint_col` > 0
+GROUP BY 1"""
+        assert result == expected
+
+    def test_multiple_count_distinct(self):
+        # Impala and some other databases will not execute multiple
+        # count-distincts in a single aggregation query. This error reporting
+        # will be left to the database itself, for now.
+        t = self.con.table('functional_alltypes')
+        metrics = [t.int_col.nunique().name('int_card'),
+                   t.smallint_col.nunique().name('smallint_card')]
+
+        expr = t.group_by('string_col').aggregate(metrics)
+
+        result = to_sql(expr)
+        expected = """\
+SELECT `string_col`, COUNT(DISTINCT `int_col`) AS `int_card`,
+       COUNT(DISTINCT `smallint_col`) AS `smallint_card`
+FROM functional_alltypes
+GROUP BY 1"""
+        assert result == expected
```

